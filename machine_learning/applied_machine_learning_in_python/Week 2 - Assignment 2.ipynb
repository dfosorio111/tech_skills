{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fdf86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 2: Assignment 2  Supervised Learning Part I\n",
    "# Part1: Regression\n",
    "# Part2: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9d0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***NOTE: If you have the line %matplotlib notebook or you are using import matplotlib.pyplot as plt in your solution,\n",
    "# you MUST REMOVE IT or comment it out before submitting for grading.\n",
    "# Detected Jupyter notebook submission.\n",
    "\n",
    "\n",
    "# Plot functions \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 15\n",
    "\n",
    "# create source data X data and y target\n",
    "x = np.linspace(0,10,n) + np.random.randn(n)/5\n",
    "y = np.sin(x)+x/6 + np.random.randn(n)/10\n",
    "\n",
    "# divide source data into train-set and test-set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "# You can use this function to help you visualize the dataset by\n",
    "# plotting a scatterplot of the data points\n",
    "# in the training and test sets.\n",
    "\n",
    "\n",
    "\n",
    "# Plot train-set and test-set\n",
    "def part1_scatter():\n",
    "    # import matplotlib.pyplot as plt  plot library \n",
    "    #import matplotlib.pyplot as plt\n",
    "    #%matplotlib notebook\n",
    "    # create figure plot \n",
    "    plt.figure()\n",
    "    # plt.scatter(X_data, y_data, label='val'),  plot scatter function. parameters: X_data,y_data points, label=val   \n",
    "    plt.scatter(X_train, y_train, label='training data')\n",
    "    plt.scatter(X_test, y_test, label='test data')\n",
    "    plt.legend(loc=4);\n",
    "    \n",
    "#part1_scatter()\n",
    "\n",
    "# Plot scores vs. parameters sensibility(model complexity)\n",
    "def plot_score_modelcomplex(scores,param):\n",
    "    # import matplotlib.pyplot as plt  plot library \n",
    "    # import matplotlib.pyplot as plt  plot library \n",
    "    #import matplotlib.pyplot as plt\n",
    "    #%matplotlib notebook\n",
    "    # create figure plot \n",
    "    plt.figure()\n",
    "    # plt.scatter(X_data, y_data, label='val'),  plot scatter function. parameters: X_data,y_data points, label=val   \n",
    "    plt.scatter(param, scores, label='Score/Accuracy vs. Parameter sensitivity')\n",
    "    plt.legend(loc=4);\n",
    "    \n",
    "    \n",
    "def plot_one(predicted_vals, x_vals):\n",
    "    # import matplotlib.pyplot as plt\n",
    "    #%matplotlib notebook\n",
    "    # create figure \n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    # plot train-set: y_train vs X_train\n",
    "    plt.plot(X_train, y_train, 'o', label='training data', markersize=10)\n",
    "    \n",
    "    # plot test-set: y_test vs X_test\n",
    "    plt.plot(X_test, y_test, 'o', label='test data', markersize=10)\n",
    "    \n",
    "    # iterate through parameters\n",
    "    for i,degree in enumerate([1,3,6,9]):\n",
    "        plt.plot(np.linspace(0,10,100), predicted_vals[i], alpha=0.8, lw=2, label='degree={}'.format(degree))\n",
    "        \n",
    "    plt.ylim(-1,2.5)\n",
    "    plt.legend(loc=4)\n",
    "    \n",
    "# NOTE: Uncomment the function below to visualize the data, but be sure \n",
    "# to **re-comment it before submitting this assignment to the autograder**.   \n",
    "#part1_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e88fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.53040195e-01,  2.69201547e-01,  2.85362899e-01,\n",
       "         3.01524251e-01,  3.17685603e-01,  3.33846955e-01,\n",
       "         3.50008306e-01,  3.66169658e-01,  3.82331010e-01,\n",
       "         3.98492362e-01,  4.14653714e-01,  4.30815066e-01,\n",
       "         4.46976417e-01,  4.63137769e-01,  4.79299121e-01,\n",
       "         4.95460473e-01,  5.11621825e-01,  5.27783177e-01,\n",
       "         5.43944529e-01,  5.60105880e-01,  5.76267232e-01,\n",
       "         5.92428584e-01,  6.08589936e-01,  6.24751288e-01,\n",
       "         6.40912640e-01,  6.57073992e-01,  6.73235343e-01,\n",
       "         6.89396695e-01,  7.05558047e-01,  7.21719399e-01,\n",
       "         7.37880751e-01,  7.54042103e-01,  7.70203454e-01,\n",
       "         7.86364806e-01,  8.02526158e-01,  8.18687510e-01,\n",
       "         8.34848862e-01,  8.51010214e-01,  8.67171566e-01,\n",
       "         8.83332917e-01,  8.99494269e-01,  9.15655621e-01,\n",
       "         9.31816973e-01,  9.47978325e-01,  9.64139677e-01,\n",
       "         9.80301028e-01,  9.96462380e-01,  1.01262373e+00,\n",
       "         1.02878508e+00,  1.04494644e+00,  1.06110779e+00,\n",
       "         1.07726914e+00,  1.09343049e+00,  1.10959184e+00,\n",
       "         1.12575320e+00,  1.14191455e+00,  1.15807590e+00,\n",
       "         1.17423725e+00,  1.19039860e+00,  1.20655995e+00,\n",
       "         1.22272131e+00,  1.23888266e+00,  1.25504401e+00,\n",
       "         1.27120536e+00,  1.28736671e+00,  1.30352807e+00,\n",
       "         1.31968942e+00,  1.33585077e+00,  1.35201212e+00,\n",
       "         1.36817347e+00,  1.38433482e+00,  1.40049618e+00,\n",
       "         1.41665753e+00,  1.43281888e+00,  1.44898023e+00,\n",
       "         1.46514158e+00,  1.48130294e+00,  1.49746429e+00,\n",
       "         1.51362564e+00,  1.52978699e+00,  1.54594834e+00,\n",
       "         1.56210969e+00,  1.57827105e+00,  1.59443240e+00,\n",
       "         1.61059375e+00,  1.62675510e+00,  1.64291645e+00,\n",
       "         1.65907781e+00,  1.67523916e+00,  1.69140051e+00,\n",
       "         1.70756186e+00,  1.72372321e+00,  1.73988457e+00,\n",
       "         1.75604592e+00,  1.77220727e+00,  1.78836862e+00,\n",
       "         1.80452997e+00,  1.82069132e+00,  1.83685268e+00,\n",
       "         1.85301403e+00],\n",
       "       [ 1.22989539e+00,  1.15143628e+00,  1.07722393e+00,\n",
       "         1.00717881e+00,  9.41221419e-01,  8.79272234e-01,\n",
       "         8.21251741e-01,  7.67080426e-01,  7.16678772e-01,\n",
       "         6.69967266e-01,  6.26866391e-01,  5.87296632e-01,\n",
       "         5.51178474e-01,  5.18432402e-01,  4.88978901e-01,\n",
       "         4.62738455e-01,  4.39631549e-01,  4.19578668e-01,\n",
       "         4.02500297e-01,  3.88316920e-01,  3.76949022e-01,\n",
       "         3.68317088e-01,  3.62341603e-01,  3.58943051e-01,\n",
       "         3.58041918e-01,  3.59558687e-01,  3.63413845e-01,\n",
       "         3.69527874e-01,  3.77821261e-01,  3.88214491e-01,\n",
       "         4.00628046e-01,  4.14982414e-01,  4.31198078e-01,\n",
       "         4.49195522e-01,  4.68895233e-01,  4.90217694e-01,\n",
       "         5.13083391e-01,  5.37412808e-01,  5.63126429e-01,\n",
       "         5.90144741e-01,  6.18388226e-01,  6.47777371e-01,\n",
       "         6.78232660e-01,  7.09674578e-01,  7.42023609e-01,\n",
       "         7.75200238e-01,  8.09124950e-01,  8.43718230e-01,\n",
       "         8.78900563e-01,  9.14592432e-01,  9.50714324e-01,\n",
       "         9.87186723e-01,  1.02393011e+00,  1.06086498e+00,\n",
       "         1.09791181e+00,  1.13499108e+00,  1.17202328e+00,\n",
       "         1.20892890e+00,  1.24562842e+00,  1.28204233e+00,\n",
       "         1.31809110e+00,  1.35369523e+00,  1.38877520e+00,\n",
       "         1.42325149e+00,  1.45704459e+00,  1.49007498e+00,\n",
       "         1.52226316e+00,  1.55352959e+00,  1.58379478e+00,\n",
       "         1.61297919e+00,  1.64100332e+00,  1.66778766e+00,\n",
       "         1.69325268e+00,  1.71731887e+00,  1.73990672e+00,\n",
       "         1.76093671e+00,  1.78032933e+00,  1.79800506e+00,\n",
       "         1.81388438e+00,  1.82788778e+00,  1.83993575e+00,\n",
       "         1.84994877e+00,  1.85784732e+00,  1.86355189e+00,\n",
       "         1.86698296e+00,  1.86806103e+00,  1.86670656e+00,\n",
       "         1.86284006e+00,  1.85638200e+00,  1.84725286e+00,\n",
       "         1.83537314e+00,  1.82066332e+00,  1.80304388e+00,\n",
       "         1.78243530e+00,  1.75875808e+00,  1.73193269e+00,\n",
       "         1.70187963e+00,  1.66851936e+00,  1.63177240e+00,\n",
       "         1.59155920e+00],\n",
       "       [-1.99554310e-01, -3.95192725e-03,  1.79851752e-01,\n",
       "         3.51005136e-01,  5.08831706e-01,  6.52819233e-01,\n",
       "         7.82609240e-01,  8.97986721e-01,  9.98870117e-01,\n",
       "         1.08530155e+00,  1.15743729e+00,  1.21553852e+00,\n",
       "         1.25996233e+00,  1.29115292e+00,  1.30963316e+00,\n",
       "         1.31599632e+00,  1.31089811e+00,  1.29504889e+00,\n",
       "         1.26920626e+00,  1.23416782e+00,  1.19076415e+00,\n",
       "         1.13985218e+00,  1.08230867e+00,  1.01902405e+00,\n",
       "         9.50896441e-01,  8.78825970e-01,  8.03709344e-01,\n",
       "         7.26434655e-01,  6.47876457e-01,  5.68891088e-01,\n",
       "         4.90312256e-01,  4.12946874e-01,  3.37571147e-01,\n",
       "         2.64926923e-01,  1.95718291e-01,  1.30608438e-01,\n",
       "         7.02167560e-02,  1.51162118e-02, -3.41690366e-02,\n",
       "        -7.71657636e-02, -1.13453547e-01, -1.42666382e-01,\n",
       "        -1.64494044e-01, -1.78683194e-01, -1.85038228e-01,\n",
       "        -1.83421873e-01, -1.73755533e-01, -1.56019368e-01,\n",
       "        -1.30252132e-01, -9.65507462e-02, -5.50696232e-02,\n",
       "        -6.01973199e-03,  5.03325883e-02,  1.13667071e-01,\n",
       "         1.83611221e-01,  2.59742264e-01,  3.41589357e-01,\n",
       "         4.28636046e-01,  5.20322987e-01,  6.16050916e-01,\n",
       "         7.15183874e-01,  8.17052690e-01,  9.20958717e-01,\n",
       "         1.02617782e+00,  1.13196463e+00,  1.23755703e+00,\n",
       "         1.34218093e+00,  1.44505526e+00,  1.54539723e+00,\n",
       "         1.64242789e+00,  1.73537785e+00,  1.82349336e+00,\n",
       "         1.90604254e+00,  1.98232198e+00,  2.05166348e+00,\n",
       "         2.11344114e+00,  2.16707864e+00,  2.21205680e+00,\n",
       "         2.24792141e+00,  2.27429129e+00,  2.29086658e+00,\n",
       "         2.29743739e+00,  2.29389257e+00,  2.28022881e+00,\n",
       "         2.25656001e+00,  2.22312684e+00,  2.18030664e+00,\n",
       "         2.12862347e+00,  2.06875850e+00,  2.00156065e+00,\n",
       "         1.92805743e+00,  1.84946605e+00,  1.76720485e+00,\n",
       "         1.68290491e+00,  1.59842194e+00,  1.51584842e+00,\n",
       "         1.43752602e+00,  1.36605824e+00,  1.30432333e+00,\n",
       "         1.25548743e+00],\n",
       "       [ 6.79502355e+00,  4.14320005e+00,  2.23123355e+00,\n",
       "         9.10495758e-01,  5.49804881e-02, -4.41344343e-01,\n",
       "        -6.66950350e-01, -6.94942800e-01, -5.85049523e-01,\n",
       "        -3.85418317e-01, -1.34235951e-01,  1.38818688e-01,\n",
       "         4.11275346e-01,  6.66715600e-01,  8.93747630e-01,\n",
       "         1.08510220e+00,  1.23683998e+00,  1.34766088e+00,\n",
       "         1.41830652e+00,  1.45104744e+00,  1.44924714e+00,\n",
       "         1.41699554e+00,  1.35880463e+00,  1.27936005e+00,\n",
       "         1.18332201e+00,  1.07517014e+00,  9.59086597e-01,\n",
       "         8.38872642e-01,  7.17893842e-01,  5.99049780e-01,\n",
       "         4.84764235e-01,  3.76992248e-01,  2.77240786e-01,\n",
       "         1.86600009e-01,  1.05782462e-01,  3.51677676e-02,\n",
       "        -2.51492923e-02, -7.53092056e-02, -1.15638286e-01,\n",
       "        -1.46600758e-01, -1.68753543e-01, -1.82704708e-01,\n",
       "        -1.89076340e-01, -1.88472434e-01, -1.81452187e-01,\n",
       "        -1.68508942e-01, -1.50054886e-01, -1.26411445e-01,\n",
       "        -9.78052026e-02, -6.43690749e-02, -2.61483329e-02,\n",
       "         1.68889851e-02,  6.48378336e-02,  1.17838707e-01,\n",
       "         1.76057646e-01,  2.39664416e-01,  3.08809594e-01,\n",
       "         3.83601332e-01,  4.64082549e-01,  5.50209309e-01,\n",
       "         6.41831127e-01,  7.38673902e-01,  8.40326138e-01,\n",
       "         9.46229054e-01,  1.05567113e+00,  1.16778754e+00,\n",
       "         1.28156484e+00,  1.39585113e+00,  1.50937196e+00,\n",
       "         1.62075178e+00,  1.72854110e+00,  1.83124875e+00,\n",
       "         1.92737910e+00,  2.01547343e+00,  2.09415470e+00,\n",
       "         2.16217476e+00,  2.21846267e+00,  2.26217282e+00,\n",
       "         2.29273102e+00,  2.30987675e+00,  2.31369931e+00,\n",
       "         2.30466542e+00,  2.28363552e+00,  2.25186567e+00,\n",
       "         2.21099182e+00,  2.16299257e+00,  2.11012661e+00,\n",
       "         2.05484028e+00,  1.99964072e+00,  1.94692936e+00,\n",
       "         1.89879035e+00,  1.85672807e+00,  1.82134741e+00,\n",
       "         1.79197012e+00,  1.76618016e+00,  1.73929044e+00,\n",
       "         1.70372288e+00,  1.64829346e+00,  1.55739306e+00,\n",
       "         1.41005483e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    # Your code here\n",
    "    predicted_values = np.zeros(shape=(4,100) )\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    \n",
    "    np.random.seed(0)\n",
    "    n = 15\n",
    "    # create  source data X data and y target\n",
    "    x = np.linspace(0,10,n) + np.random.randn(n)/5\n",
    "    y = np.sin(x)+x/6 + np.random.randn(n)/10\n",
    "    \n",
    "    # divide source data into train-set and test-set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "    \n",
    "    # parameter degree of PolynomialFeatures model\n",
    "    degrees = [1,3,6,9]\n",
    "    \n",
    "    # y_test values to be predicted by model\n",
    "    y_predict = np.linspace(0,10,100).reshape(100,1)\n",
    "    \n",
    "    \n",
    "    # reshape X_data\n",
    "    X_train_shape = X_train.reshape(X_train.size, 1)\n",
    "    X_test_shape = X_test.reshape(X_test.size, 1)\n",
    "        \n",
    "    # create PolynomialFeatures model, parameters: degree=deg\n",
    "    # transform X_data from 1D to N=degD D space\n",
    "    poly1 = PolynomialFeatures(degree=degrees[0])\n",
    "        \n",
    "    # transform 1D X data into N-degree data points\n",
    "    X_train_poly = poly1.fit_transform(X_train_shape)\n",
    "    X_test_poly = poly1.fit_transform(X_test_shape)\n",
    "        \n",
    "    # transform 1D y_predict_test data into N-degree space\n",
    "    y_predict_test_poly1 = poly1.fit_transform(y_predict)\n",
    "        \n",
    "    # create  LinearRegression, fit/train with X_transformed data and y_train data\n",
    "    linreg1 = LinearRegression().fit(X_train_poly, y_train)\n",
    "        \n",
    "    # predict y target data,  for y_test transformed data \n",
    "    vals1 = linreg1.predict(y_predict_test_poly1)\n",
    "        \n",
    "    # insert predicted values into list(array)\n",
    "    predicted_values[0] = vals1\n",
    "    #print(\"predicted values added\")\n",
    "\n",
    "    # append score/performance of model to scores list \n",
    "    scores.append(linreg1.score(X_test_poly, y_test))\n",
    "    \n",
    "    #print('(poly deg ) linear model coeff (w):\\n{}'.format(linreg1.coef_))\n",
    "    #print('(poly deg ) linear model intercept (b): {:.3f}'.format(linreg1.intercept_))\n",
    "    #print('(poly deg ) R-squared score (test): {:.3f}\\n'.format(linreg1.score(X_train_poly, y_train)))\n",
    "    #print('(poly deg ) R-squared score (training): {:.3f}'.format(linreg1.score(X_test_poly, y_test)))\n",
    "\n",
    "    \n",
    "    poly3 = PolynomialFeatures(degree=degrees[1])\n",
    "    X_train_poly = poly3.fit_transform(X_train_shape)\n",
    "    X_test_poly = poly3.fit_transform(X_test_shape)\n",
    "    y_predict_test_poly3 = poly3.fit_transform(y_predict)\n",
    "    linreg3 = LinearRegression().fit(X_train_poly, y_train)\n",
    "    vals3 = linreg3.predict(y_predict_test_poly3)\n",
    "    predicted_values[1] = vals3\n",
    "    scores.append(linreg3.score(X_test_poly, y_test))\n",
    "    \n",
    "    #print('(poly deg ) linear model coeff (w):\\n{}'.format(linreg3.coef_))\n",
    "    #print('(poly deg ) linear model intercept (b): {:.3f}'.format(linreg3.intercept_))\n",
    "    #print('(poly deg ) R-squared score (test): {:.3f}\\n'.format(linreg3.score(X_train_poly, y_train)))\n",
    "    #print('(poly deg ) R-squared score (training): {:.3f}'.format(linreg3.score(X_test_poly, y_test)))\n",
    "    \n",
    "        \n",
    "    poly6 = PolynomialFeatures(degree=degrees[2])\n",
    "    X_train_poly = poly6.fit_transform(X_train_shape)\n",
    "    X_test_poly = poly6.fit_transform(X_test_shape)\n",
    "    y_predict_test_poly6 = poly6.fit_transform(y_predict)\n",
    "    linreg6 = LinearRegression().fit(X_train_poly, y_train)\n",
    "    vals6 = linreg6.predict(y_predict_test_poly6)\n",
    "    predicted_values[2] = vals6\n",
    "    scores.append(linreg6.score(X_test_poly, y_test))\n",
    "    \n",
    "    #print('(poly deg ) linear model coeff (w):\\n{}'.format(linreg6.coef_))\n",
    "    #print('(poly deg ) linear model intercept (b): {:.3f}'.format(linreg6.intercept_))\n",
    "    #print('(poly deg ) R-squared score (test): {:.3f}\\n'.format(linreg6.score(X_train_poly, y_train)))\n",
    "    #print('(poly deg ) R-squared score (training): {:.3f}'.format(linreg6.score(X_test_poly, y_test)))\n",
    "    \n",
    "    poly9 = PolynomialFeatures(degree=degrees[3])\n",
    "    X_train_poly = poly9.fit_transform(X_train_shape)\n",
    "    X_test_poly = poly9.fit_transform(X_test_shape)\n",
    "    y_predict_test_poly9 = poly9.fit_transform(y_predict)\n",
    "    linreg9 = LinearRegression().fit(X_train_poly, y_train)\n",
    "    vals9 = linreg9.predict(y_predict_test_poly9)\n",
    "    predicted_values[3] = vals9\n",
    "    scores.append(linreg9.score(X_test_poly, y_test))\n",
    "    \n",
    "    #print('(poly deg ) linear model coeff (w):\\n{}'.format(linreg9.coef_))\n",
    "    #print('(poly deg ) linear model intercept (b): {:.3f}'.format(linreg9.intercept_))\n",
    "    #print('(poly deg ) R-squared score (test): {:.3f}\\n'.format(linreg9.score(X_train_poly, y_train)))\n",
    "    #print('(poly deg ) R-squared score (training): {:.3f}'.format(linreg9.score(X_test_poly, y_test)))\n",
    "    \n",
    "    # Plot score/performance and y target polynomial function  parameters    \n",
    "    \n",
    "    predicted_vals = np.asarray(predicted_values)\n",
    "    #plot_score_modelcomplex(scores,degrees)\n",
    "    \n",
    "    #print(type(predicted_vals))\n",
    "    #print(predicted_vals.shape[0], predicted_vals.shape[1])\n",
    "    \n",
    "    return predicted_vals# Return your answer\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8149fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to use the function plot_one() to replicate the figure \n",
    "# from the prompt once you have completed question one\n",
    "def plot_one(degree_predictions):\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #%matplotlib notebook\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(X_train, y_train, 'o', label='training data', markersize=10)\n",
    "    plt.plot(X_test, y_test, 'o', label='test data', markersize=10)\n",
    "    for i,degree in enumerate([1,3,6,9]):\n",
    "        plt.plot(np.linspace(0,10,100), degree_predictions[i], alpha=0.8, lw=2, label='degree={}'.format(degree))\n",
    "    plt.ylim(-1,2.5)\n",
    "    plt.legend(loc=4)\n",
    "\n",
    "#plot_one(answer_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70221556",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.metrics.regression'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9656/3311846673.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m  \u001b[0mscores\u001b[0m \u001b[1;31m# Your answer here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0manswer_two\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9656/3311846673.py\u001b[0m in \u001b[0;36manswer_two\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Your code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.metrics.regression'"
     ]
    }
   ],
   "source": [
    "def answer_two():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    # ans data structure: scores on train-set and test set\n",
    "    scores = ()\n",
    "    r2_train = np.zeros((10,))\n",
    "    r2_test = np.zeros((10,))\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    n = 15\n",
    "    # creat source data X data and y target\n",
    "    x = np.linspace(0,10,n) + np.random.randn(n)/5\n",
    "    y = np.sin(x)+x/6 + np.random.randn(n)/10\n",
    "    \n",
    "    # divide source data into train-set and test-set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "    \n",
    "    # parameter degree of PolynomialFeatures model\n",
    "    degrees = np.arange(0,10,1)\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(len(degrees)):\n",
    "        \n",
    "        # reshape X_data from source data\n",
    "        X_train_shape = X_train.reshape(X_train.size, 1)   \n",
    "        X_test_shape = X_test.reshape(X_test.size, 1)\n",
    "        \n",
    "        # create PolynomialFeatures Regression model, parameters: degree \n",
    "        poly = PolynomialFeatures(degree=degrees[i])\n",
    "        \n",
    "        # transform X_data from 1D to N=deg-D space for PolynomialRegg  \n",
    "        X_train_poly = poly.fit_transform(X_train_shape)\n",
    "        X_test_poly = poly.fit_transform(X_test_shape)\n",
    "        # SIEMPRE se transforma y fit/train el X_data, NO el y_data\n",
    "        \n",
    "        \n",
    "        # create LinearRegression model, fit/train with transformed X_train, y_train\n",
    "        # train-set data \n",
    "        linreg = LinearRegression().fit(X_train_poly,y_train)\n",
    "        \n",
    "        # score/performance of Transformer PolynomialRegre to Linear Reggresion\n",
    "        # for train-set and test-set \n",
    "        \n",
    "        train_score = linreg.score(X_train_poly,y_train)\n",
    "        test_score = linreg.score(X_test_poly,y_test)\n",
    "        \n",
    "        r2_train[i] = train_score\n",
    "        r2_test[i] = test_score\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "    #plot_score_modelcomplex(r2_train,degrees)\n",
    "    #plot_score_modelcomplex(test_scores,degrees)\n",
    "    scores = (np.asarray(r2_train),np.asarray(r2_test))\n",
    "    \n",
    "    #print(type(scores))\n",
    "    #print(len(r2_train), len(r2_test))\n",
    "    \n",
    "    return  scores # Your answer here\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2fd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    \n",
    "    # Your code here\n",
    "    degrees_complex = ()\n",
    "    \n",
    "    # parameter degree of PolynomialFeatures model: array of degrees for model complexity\n",
    "    degrees = np.arange(0,10,1)\n",
    "    \n",
    "    # get scores of train-set and test-set for PolynomialFeatures model\n",
    "    # for different N-Dim. degrees \n",
    "    scores = answer_two()\n",
    "    \n",
    "    train_set_scores = scores[0]\n",
    "    test_set_scores = scores[1]\n",
    "    \n",
    "    # plot scores vs degrees\n",
    "    #plot_score_modelcomplex(train_set_scores,degrees)\n",
    "    #plot_score_modelcomplex(test_set_scores,degrees)\n",
    "    \n",
    "    underfit = 0\n",
    "    overfit = 8\n",
    "    good_gen = 7\n",
    "    \n",
    "    degrees_complex = (underfit,overfit,good_gen)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return degrees_complex  # Return your answer\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.linear_model import Lasso, LinearRegression\n",
    "    from sklearn.metrics.regression import r2_score\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "        # ans data structure: scores on train-set and test set\n",
    "    scores = ()\n",
    "    LinearRegression_R2_test_score = 0 \n",
    "    Lasso_R2_test_score = 0\n",
    "    \n",
    "    \n",
    "    np.random.seed(0)\n",
    "    n = 15\n",
    "    # creat source data X data and y target\n",
    "    x = np.linspace(0,10,n) + np.random.randn(n)/5\n",
    "    y = np.sin(x)+x/6 + np.random.randn(n)/10\n",
    "    \n",
    "    # divide source data into train-set and test-set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "    \n",
    "\n",
    "    # reshape X_data from source data\n",
    "    X_train_shape = X_train.reshape(X_train.size, 1)   \n",
    "    X_test_shape = X_test.reshape(X_test.size, 1)\n",
    "        \n",
    "    # create PolynomialFeatures Regression model, parameters: degree=12\n",
    "    poly = PolynomialFeatures(degree=12)\n",
    "        \n",
    "    # transform X_data from 1D to N=deg-D space for PolynomialRegg  \n",
    "    X_train_poly = poly.fit_transform(X_train_shape)\n",
    "    X_test_poly = poly.fit_transform(X_test_shape)\n",
    "    # SIEMPRE se transforma Y  fit/train el X_data, NO el y_data\n",
    "        \n",
    "        \n",
    "    # create LinearRegression model, fit/train with transformed X_train, y_train\n",
    "    # train-set data \n",
    "    linreg = LinearRegression().fit(X_train_poly,y_train)\n",
    "    \n",
    "    # create LassoRegression regularized model, \n",
    "    # fit/train with tranformed X_data to N-D space\n",
    "    # parameters: alpha_reg=2, max_iter=100000\n",
    "    linlasso = Lasso(alpha=0.01, max_iter = 10000).fit(X_train_poly, y_train)\n",
    "    \n",
    "        \n",
    "    # score/performance of PolynomialLinReg model\n",
    "    train_score_simple = linreg.score(X_train_poly,y_train)\n",
    "    test_score_simple = linreg.score(X_test_poly,y_test)\n",
    "    LinearRegression_R2_test_score = test_score_simple\n",
    "    \n",
    "    # score/performance of Polynomial LassoReg regularized model    \n",
    "    train_score_lasso = linlasso.score(X_train_poly,y_train)\n",
    "    test_score_lasso = linlasso.score(X_test_poly,y_test)\n",
    "    Lasso_R2_test_score = test_score_lasso\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    return LinearRegression_R2_test_score,Lasso_R2_test_score # Your answer here\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "mush_df = pd.read_csv('mushrooms.csv')\n",
    "mush_df2 = pd.get_dummies(mush_df)\n",
    "\n",
    "X_mush = mush_df2.iloc[:,2:]\n",
    "y_mush = mush_df2.iloc[:,1]\n",
    "\n",
    "# use the variables X_train2, y_train2 for Question 5\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_mush, y_mush, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# For performance reasons in Questions 6 and 7, we will create a smaller version of the\n",
    "# entire mushroom dataset for use in those questions.  For simplicity we'll just re-use\n",
    "# the 25% test split created above as the representative subset.\n",
    "#\n",
    "\n",
    "\n",
    "# Use the variables X_subset, y_subset for Questions 6 and 7.\n",
    "X_subset = X_test2\n",
    "y_subset = y_test2\n",
    "mush_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af92a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from adspy_shared_utilities import plot_feature_importances\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    mush_df = pd.read_csv('mushrooms.csv')\n",
    "    mush_df2 = pd.get_dummies(mush_df)\n",
    "    \n",
    "    X_mush = mush_df2.iloc[:,2:]\n",
    "    y_mush = mush_df2.iloc[:,1]\n",
    "    \n",
    "    # use the variables X_train2, y_train2 for Question 5\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X_mush, y_mush, random_state=0)\n",
    "    \n",
    "    # create/fit train Decision Tree model, parameters: random_state=0\n",
    "    clf = DecisionTreeClassifier(random_state=0).fit(X_train2, y_train2)\n",
    "    \n",
    "    # clf.feature_importances_,  get weight of features for the model\n",
    "    feature_imp = clf.feature_importances_\n",
    "    \n",
    "    # bar graph of features(variables) of importance of the Decision Tree Model\n",
    "    plot_feature_importances(clf, X_train2.columns)\n",
    "    \n",
    "    # create feature importance DataFrame, columns: feature name, importance\n",
    "    df_feature_imp = pd.DataFrame({'feature name':X_train2.columns, 'importance':feature_imp}  )\n",
    "    \n",
    "    # sort df_feature_imp b importance, ascending order. get top 5 most important features\n",
    "    df_feature_imp = df_feature_imp.sort(['importance'], ascending=[0])['feature name'].head(5).tolist()\n",
    "    \n",
    "    #print(type(df_feature_imp))\n",
    "    return df_feature_imp# Your answer here\n",
    "\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a982553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88995557, 0.97189404, 0.9981947 , 1.        , 1.        ,\n",
       "        1.        ]),\n",
       " array([0.89003611, 0.96849028, 0.99786616, 1.        , 1.        ,\n",
       "        0.51649431]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import validation_curve\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    training_scores = np.zeros((6,1))\n",
    "    test_scores = np.zeros((6,1))\n",
    "    \n",
    "    mush_df = pd.read_csv('mushrooms.csv')\n",
    "    mush_df2 = pd.get_dummies(mush_df)\n",
    "    \n",
    "    X_mush = mush_df2.iloc[:,2:]\n",
    "    y_mush = mush_df2.iloc[:,1]\n",
    "    \n",
    "    # use the variables X_train2, y_train2 for Question 5\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X_mush, y_mush, random_state=0)\n",
    "    \n",
    "    # For performance reasons in Questions 6 and 7, we will create a smaller version of the\n",
    "    # entire mushroom dataset for use in those questions.  For simplicity we'll just re-use\n",
    "    # the 25% test split created above as the representative subset.\n",
    "    \n",
    "    # Use the variables X_subset, y_subset for Questions 6 and 7.\n",
    "    X_subset = X_test2\n",
    "    y_subset = y_test2\n",
    "    \n",
    "    \n",
    "    # create SVectorMachine unfitted/train,  parameters:  kernel='rbf' funct, C=1, random_state=0\n",
    "    clf = SVC(kernel='rbf', C=1, random_state=0)\n",
    "    \n",
    "    # range of gamma parameter values to iterate for evaluation of train-set and test-sets\n",
    "    param_range = np.logspace(-4, 1, 6)\n",
    "    \n",
    "    # cross-validation: get train-scores and test-scores of 3-fold validation curve\n",
    "    train_scores, test_scores = validation_curve(SVC(random_state=0), X_train2, y_train2, \n",
    "                                                 param_name='gamma', \n",
    "                                                 param_range=param_range )\n",
    "    \n",
    "    training_scores = np.array(list(map(np.mean, train_scores)))\n",
    "    test_scores = np.array(list(map(np.mean, test_scores)))\n",
    "    \n",
    "    #print(training_scores, test_scores)\n",
    "\n",
    "    return training_scores, test_scores # Your answer here\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3020ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_six' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9656/1387579508.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdegree_levs\u001b[0m\u001b[1;31m# Return your answer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0manswer_seven\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9656/1387579508.py\u001b[0m in \u001b[0;36manswer_seven\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdegree_levs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# get train-scores and test_scores from cross-val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtraining_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manswer_six\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# range of gamma parameter values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'answer_six' is not defined"
     ]
    }
   ],
   "source": [
    "def answer_seven():\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    degree_levs = ()\n",
    "    # get train-scores and test_scores from cross-val\n",
    "    training_scores, test_scores = answer_six()\n",
    "    \n",
    "    # range of gamma parameter values\n",
    "    param_range = np.logspace(-4, 1, 6)\n",
    "    \n",
    "    # plot score vs. model complexity (gamma parameter)\n",
    "    #plot_score_modelcomplex(training_scores,param_range)\n",
    "    #plot_score_modelcomplex(test_scores,param_range)\n",
    "    \n",
    "    # underfit\n",
    "    underfit = param_range[0]\n",
    "    overfit = param_range[5]\n",
    "    good_gen = param_range[2]\n",
    "    \n",
    "    degree_levs = (underfit, overfit, good_gen)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return degree_levs# Return your answer\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4668fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
