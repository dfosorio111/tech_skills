{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c79dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5f5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3364: DtypeWarning: Columns (11,12,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cargar database de train-set y test-set\n",
    "# cargar addresses y coordenadas lat-lon\n",
    "\n",
    "# data mining sobre df_train y df_test\n",
    "# eliminar columnas que no estan en test-set, etc\n",
    "# crear LabelEncoder sobre columnas con str\n",
    "\n",
    "# hacer train_test_split usando train-set:  (df_train[cols], df_train['compliance'])\n",
    "# crear X_train,y_train, X_test, y_test  usando df_train (train[cols], train['compliance'])\n",
    "# crear modelo RandomForestRegressor\n",
    "# optimizar parametros, fit/train con X_train,y_train creados\n",
    "# verificar que modelo tenga scoring='roc_auc'> 0.7\n",
    "\n",
    "\n",
    "def blight_model():\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    # cargar dataset de train-set y test-set\n",
    "    df_train = pd.read_csv('train.csv', encoding=\"ISO-8859-1\")\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "\n",
    "    # cargar addresses y coordenadas  lat-lon\n",
    "    addresses = pd.read_csv('addresses.csv')\n",
    "    latlons = pd.read_csv('latlons.csv')\n",
    "\n",
    "\n",
    "    # drop all Null-NaN entries df_train\n",
    "    df_train = df_train[np.isfinite(df_train['compliance'])]\n",
    "\n",
    "\n",
    "    # over-over write df_train.country, on condition =='USA'\n",
    "    df_train = df_train[df_train.country == 'USA']\n",
    "\n",
    "    # over-over write df_test.country, on condition =='USA'\n",
    "    df_test = df_test[df_test.country == 'USA']\n",
    "\n",
    "\n",
    "    # unir  df_train y df_test con addresses\n",
    "    add_latlons = pd.merge(addresses, latlons, on= 'address')\n",
    "    df_train = pd.merge(df_train, add_latlons, on='ticket_id')\n",
    "    df_test = pd.merge(df_test, add_latlons, on='ticket_id')\n",
    "\n",
    "    # df_train.drop([cols])  eliminar columnas que no esten en test_set y que sean data leakage\n",
    "    df_train.drop(['agency_name', 'inspector_name', 'violator_name', 'non_us_str_code', 'violation_description',\n",
    "                'grafitti_status', 'state_fee', 'admin_fee', 'ticket_issued_date', 'hearing_date',\n",
    "                'payment_amount', 'balance_due', 'payment_date', 'payment_status',\n",
    "                'collection_status', 'compliance_detail',\n",
    "                'violation_zip_code', 'country', 'address', 'violation_street_number',\n",
    "                'violation_street_name', 'mailing_address_str_number', 'mailing_address_str_name',\n",
    "                'city', 'state', 'zip_code', 'address'], axis=1, inplace=True)\n",
    "\n",
    "    # LabelEncoder()  crear Label Encoder de columnas con string\n",
    "    #  LabelEncoder() de columna 'disposition'\n",
    "    label_encoder = LabelEncoder()\n",
    "    # fi/train LabelEncoder con df_train y df_test en columna 'disposition'\n",
    "    label_encoder.fit(df_train['disposition'].append(df_test['disposition'], ignore_index=True))\n",
    "\n",
    "    # transform df_train['disposition'] con LabelEncoder()\n",
    "    df_train['disposition'] = label_encoder.transform(df_train['disposition'])\n",
    "    # transform df_test['disposition'] con LabelEncoder()\n",
    "    df_test['disposition'] = label_encoder.transform(df_test['disposition'])\n",
    "\n",
    "    #  LabelEncoder() de columna 'violation_code'\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df_train['violation_code'].append(df_test['violation_code'], ignore_index=True))\n",
    "    df_train['violation_code'] = label_encoder.transform(df_train['violation_code'])\n",
    "    df_test['violation_code'] = label_encoder.transform(df_test['violation_code'])\n",
    "\n",
    "    # df_train['lat'] fillna,  df_train['lon'] fillna\n",
    "    # df_test['lat'] fillna,  df_test['lon'] fillna\n",
    "    df_train['lat'] = df_train['lat'].fillna(df_train['lat'].mean())\n",
    "    df_train['lon'] = df_train['lon'].fillna(df_train['lon'].mean())\n",
    "    df_test['lat'] = df_test['lat'].fillna(df_test['lat'].mean())\n",
    "    df_test['lon'] = df_test['lon'].fillna(df_test['lon'].mean())\n",
    "\n",
    "    # extraer columnas de df_train\n",
    "    cols = list(df_train.columns.values)\n",
    "    # eliminar 'compliance' de lista de columnas de df_train\n",
    "    cols.remove('compliance')\n",
    "\n",
    "    # over-write df_test, on columns= cols\n",
    "    df_test = df_test[cols]\n",
    "\n",
    "    # dividir data-set entre train-set y test-set\n",
    "    # train-test split utilizando X_data = df_train[cols]  y_target = df_train['compliance']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_train[cols], df_train['compliance'])\n",
    "\n",
    "    # RandomForestRegressor crear modelo Arbol de Regresi칩n\n",
    "    regr_rf = RandomForestRegressor()\n",
    "\n",
    "    # crear grid_values sobre los que itera GridSearchCV para optimizar\n",
    "    grid_values = {'max_features': [3, 5], 'n_estimators': [10, 50], 'max_depth': [None, 10]}\n",
    "    \n",
    "    #  'max_features': [1, 3, 5], \n",
    "    # 'n_estimators': [10, 20, 50],\n",
    "    # 'max_depth': [None, 10, 30, 50] \n",
    "    \n",
    "    # GridSearchCV crear optimizador de parametros,  parametros: modelo= regr_rf, param_grid=grid_values, scoring='roc_auc'\n",
    "    grid_clf_auc = GridSearchCV(regr_rf, param_grid=grid_values, scoring='roc_auc')\n",
    "\n",
    "    # fit/train GridSearchCV con X_train, y_train\n",
    "    grid_clf_auc.fit(X_train, y_train)\n",
    "\n",
    "    # obtener los par치metros 칩ptimos del modelo, y el valor 칩ptimo de evaluation metric\n",
    "    print('Grid best parameter (max. AUC): ', grid_clf_auc.best_params_)\n",
    "    print('Grid best score (AUC): ', grid_clf_auc.best_score_)\n",
    "\n",
    "    # crear DataFrame con grid_clf_auc.predict(df_test) y index = df_test.ticket_id\n",
    "    df_prob = pd.DataFrame(grid_clf_auc.predict(df_test), df_test.ticket_id)\n",
    "    print(df_prob)\n",
    "\n",
    "\n",
    "    return df_prob\n",
    "\n",
    "\n",
    "blight_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fbd012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
