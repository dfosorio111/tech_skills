{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.email import EmailOperator\n",
    "from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\n",
    "from airflow.providers.snowflake.transfers.s3_to_snowflake import S3ToSnowflakeOperator\n",
    "\n",
    "from pendulum import datetime, duration\n",
    "\n",
    "# Instantiate DAG\n",
    "with DAG(\n",
    "    dag_id=\"s3_to_snowflake\",\n",
    "    start_date=datetime(2023, 1, 1),\n",
    "    schedule=\"@daily\",\n",
    "    default_args={\"retries\": 1, \"retry_delay\": duration(minutes=5)},\n",
    "    catchup=False,\n",
    "):\n",
    "    # Instantiate tasks within the DAG context\n",
    "    load_file = S3ToSnowflakeOperator(\n",
    "        task_id=\"load_file\",\n",
    "        s3_keys=[\"key_name.csv\"],\n",
    "        stage=\"snowflake_stage\",\n",
    "        table=\"my_table\",\n",
    "        schema=\"my_schema\",\n",
    "        file_format=\"csv\",\n",
    "        snowflake_conn_id=\"snowflake_default\",\n",
    "    )\n",
    "\n",
    "    snowflake_query = SnowflakeOperator(\n",
    "        task_id=\"run_query\", sql=\"SELECT COUNT(*) FROM my_table\"\n",
    "    )\n",
    "\n",
    "    send_email = EmailOperator(\n",
    "        task_id=\"send_email\",\n",
    "        to=\"noreply@astronomer.io\",\n",
    "        subject=\"Snowflake DAG\",\n",
    "        html_content=\"<p>The Snowflake DAG completed successfully.<p>\",\n",
    "    )\n",
    "\n",
    "    # Define dependencies\n",
    "    load_file >> snowflake_query >> send_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to load the config, contains a configuration error.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to configure formatter 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/logging/config.py:94\u001b[0m, in \u001b[0;36m_resolve\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(found, n)\n\u001b[1;32m     95\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'airflow' has no attribute 'utils' (most likely due to a circular import)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/logging/config.py:544\u001b[0m, in \u001b[0;36mDictConfigurator.configure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m     formatters[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfigure_formatter(\n\u001b[1;32m    545\u001b[0m                                         formatters[name])\n\u001b[1;32m    546\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/logging/config.py:676\u001b[0m, in \u001b[0;36mDictConfigurator.configure_formatter\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 676\u001b[0m     c \u001b[39m=\u001b[39m _resolve(cname)\n\u001b[1;32m    678\u001b[0m \u001b[39m# A TypeError would be raised if \"validate\" key is passed in with a formatter callable\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39m# that does not accept \"validate\" as a parameter\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/logging/config.py:97\u001b[0m, in \u001b[0;36m_resolve\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[39m__import__\u001b[39m(used)\n\u001b[0;32m---> 97\u001b[0m         found \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(found, n)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m found\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'airflow' has no attribute 'utils' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mairflow\u001b[39;00m \u001b[39mimport\u001b[39;00m DAG\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mairflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mempty\u001b[39;00m \u001b[39mimport\u001b[39;00m EmptyOperator\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime, timedelta\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/airflow/__init__.py:64\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m# Perform side-effects unless someone has explicitly opted out before import\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# WARNING: DO NOT USE THIS UNLESS YOU REALLY KNOW WHAT YOU'RE DOING.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_AIRFLOW__AS_LIBRARY\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 64\u001b[0m     settings\u001b[39m.\u001b[39;49minitialize()\n\u001b[1;32m     66\u001b[0m login: Callable \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m PY36 \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mversion_info \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m6\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/airflow/settings.py:570\u001b[0m, in \u001b[0;36minitialize\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m import_local_settings()\n\u001b[1;32m    569\u001b[0m \u001b[39mglobal\u001b[39;00m LOGGING_CLASS_PATH\n\u001b[0;32m--> 570\u001b[0m LOGGING_CLASS_PATH \u001b[39m=\u001b[39m configure_logging()\n\u001b[1;32m    571\u001b[0m State\u001b[39m.\u001b[39mstate_color\u001b[39m.\u001b[39mupdate(STATE_COLORS)\n\u001b[1;32m    573\u001b[0m configure_adapters()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/airflow/logging_config.py:74\u001b[0m, in \u001b[0;36mconfigure_logging\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     log\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mUnable to load the config, contains a configuration error.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[39m# When there is an error in the config, escalate the exception\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[39m# otherwise Airflow would silently fall back on the default config\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     76\u001b[0m validate_logging_config(logging_config)\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m logging_class_path\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/airflow/logging_config.py:69\u001b[0m, in \u001b[0;36mconfigure_logging\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m             task_handler_config[\u001b[39m\"\u001b[39m\u001b[39mfilters\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mmask_secrets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# Try to init logging\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     dictConfig(logging_config)\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     log\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mUnable to load the config, contains a configuration error.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/logging/config.py:811\u001b[0m, in \u001b[0;36mdictConfig\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdictConfig\u001b[39m(config):\n\u001b[1;32m    810\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Configure logging using a dictionary.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 811\u001b[0m     dictConfigClass(config)\u001b[39m.\u001b[39;49mconfigure()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/logging/config.py:547\u001b[0m, in \u001b[0;36mDictConfigurator.configure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m         formatters[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfigure_formatter(\n\u001b[1;32m    545\u001b[0m                                             formatters[name])\n\u001b[1;32m    546\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 547\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnable to configure \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    548\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mformatter \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m name) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39m# Next, do filters - they don't refer to anything else, either\u001b[39;00m\n\u001b[1;32m    550\u001b[0m filters \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mfilters\u001b[39m\u001b[39m'\u001b[39m, EMPTY_DICT)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to configure formatter 'airflow'"
     ]
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with DAG(\n",
    "  'etl_twitter_pipeline',\n",
    "  description=\"A simple twitter ETL pipeline using Python,PostgreSQL and Apache Airflow\",\n",
    "  start_date=datetime(year=2023, month=2, day=5),\n",
    "  schedule_interval=timedelta(minutes=2)\n",
    ") as dag:\n",
    "  \n",
    "  start_pipeline = EmptyOperator(\n",
    "    task_id='start_pipeline',\n",
    "  )\n",
    "  \n",
    "start_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msnscrape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtwitter\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msntwitter\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransform\u001b[39;00m \u001b[39mimport\u001b[39;00m transform_data\n\u001b[1;32m      6\u001b[0m \u001b[39m# Creating list to append tweet data to\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_data\u001b[39m():\n\u001b[1;32m      8\u001b[0m   \n\u001b[1;32m      9\u001b[0m     \u001b[39m# scrape tweets and append to a list\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transform'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from transform import transform_data\n",
    "\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "def extract_data():\n",
    "  \n",
    "    # scrape tweets and append to a list\n",
    "  for i,tweet in enumerate(sntwitter.TwitterSearchScraper('Chatham House since:2023-01-14').get_items()):\n",
    "    if i>1000:\n",
    "      break\n",
    "    tweets_list.append([tweet.date, tweet.user.username, tweet.rawContent, \n",
    "                          tweet.sourceLabel,tweet.user.location\n",
    "                          ])\n",
    "  \n",
    "      # convert tweets into a dataframe\n",
    "  tweets_df = pd.DataFrame(tweets_list, columns=['datetime', 'username', 'text', 'source', 'location'])\n",
    "\n",
    "      # save tweets as csv file\n",
    "  \n",
    "  transform_data(tweets_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_145786/82789270.py:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SQLContext\n\u001b[0;32m----> 4\u001b[0m Sc \u001b[39m=\u001b[39m SparkContext()\n\u001b[1;32m      5\u001b[0m sqlContext \u001b[39m=\u001b[39m SQLContext(sc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/context.py:430\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    427\u001b[0m     callsite \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_callsite\n\u001b[1;32m    429\u001b[0m     \u001b[39m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot run multiple SparkContexts at once; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexisting SparkContext(app=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, master=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m created by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[1;32m    435\u001b[0m             currentAppName,\n\u001b[1;32m    436\u001b[0m             currentMaster,\n\u001b[1;32m    437\u001b[0m             callsite\u001b[39m.\u001b[39mfunction,\n\u001b[1;32m    438\u001b[0m             callsite\u001b[39m.\u001b[39mfile,\n\u001b[1;32m    439\u001b[0m             callsite\u001b[39m.\u001b[39mlinenum,\n\u001b[1;32m    440\u001b[0m         )\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_145786/82789270.py:3 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "Sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m company_df \u001b[39m=\u001b[39m sqlContext\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39mcom.databricks.spark.csv\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39moptions(header\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m, inferschema\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mC:/Users/intellipaat/Downloads/spark-2.3.2-bin-hadoop2.7/Fortune5002017.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m company_df\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ]
    }
   ],
   "source": [
    "company_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('C:/Users/intellipaat/Downloads/spark-2.3.2-bin-hadoop2.7/Fortune5002017.csv')\n",
    "company_df.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
