{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Universidad de los Andes<br>Departamento de Ingeniería Eléctrica y Electrónica<br>IELE 4922 - Reinforcement Learning<br><br><p style=\"font-size:30px\">Tutorial : Value y Policy Iteration </p> \t|[<img src=\"images/uniandes_logo.png\" width=\"250\"/>](images/uniandes_logo.png)  \t|\n",
    "|-----------------------------------------------------------------------------------\t|---\t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se busca implementar los métodos de *Value Iteration* y *Policy Iteration* en un *gridworld*. Con este entorno se puede visualizar de forma gráfica e interactiva el comportamiento de estos métodos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El entorno: *Gridworld*\n",
    "El entorno consiste en una cuadrícula de 3x4, donde cada celda corresponde a una posible ubicación del agente o estado. El agente en cada celda tiene cuatro acciones posibles de movimiento: norte, sur, este y oeste (N, S, E, O). Sin embargo, estas acciones NO son confiables. Con una probabilidad de 0.8 el agente se moverá en la dirección prevista, y con una probabilidad de 0.2, el agente se moverá en una dirección aleatoria contigua. Es decir si se quiere mover al norte (N), las direcciones contiguas a las que se podrá mover con una probabilidad de 0.2 serán al este (E) o al oeste (O). Adicionalmente, aquellas acciones que lleven al agente fuera de la cuadrícula o a posiciones bloqueadas, no tendrán efecto en la posición del agente.\n",
    "\n",
    "El estado inicial del agente será en la celda (2,0). Si el agente llega a la celda con el diamante, recibirá una recompensa de +1.0 y el juego (*episodio*) terminará. Si el agente llega a la celda con la bomba, recibirá una recompensa de -1.0 y el episodio terminará. Por ende, los estados (0,3) y (1,3) serán considerados *estados terminales*. El agente recibe recompensa 0.0 por llegar a cualquier otra celda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/gridworld.png\" alt=\"centered image\" width=300/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones importantes:**\n",
    "\n",
    "* Para la correcta ejecución del código dado mantenga los nombres de las variables propuestos.\n",
    "\n",
    "* Debe editar y completar unicamente las celdas que comiencen con la instrucción #EDITABLE\n",
    "\n",
    "* No se preocupe si la ventana de gridworld no responde en algunos momentos, una vez ejecute los métodos para la evaluación (Todos aquellos deppues de las instrucciones para la **visualización**) esta se actualizará y podrá ver de forma interactiva la actualización de los valores deseados.\n",
    "\n",
    "* Si quiere volver a visualizar el gridworld, debe volver a ejecutar al menos todas las celdas de esa sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Librerias que contienen la dinámica del entorno gridworld\n",
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el objeto *gridworld* con las siguientes característica:\n",
    "* Tamaño de la cuadrícula: 3 filas y 4 columnas\n",
    "* La celda bloqueda estará en (1,1)\n",
    "* La celda con la bomba estará en (1,3)\n",
    "* La celda con el diamante estará en (0,3)\n",
    "\n",
    "Una vez creado el objeto, en una ventana emergente de pygame se encuentra la visualización del *gridworld*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gridworld.GridWorld at 0x232e30ce3d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw = GridWorld(rows=3, cols=4, walls=[(1,1)], pits=[(1,3)], goals=[(0,3)], live_reward=0.0)\n",
    "gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de estados\n",
    "Para acceder a una lista con los estados del gridworld se puede utilizar la propiedad *states* del objeto gridworld, así: `gw.states`\n",
    "\n",
    "¿Cuántos estados tiene el problema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_states = +11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(\"num_states = +\"+str(len(gw.states)))\n",
    "gw.states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "\n",
    "```\n",
    "num_states = 11\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de acciones\n",
    "\n",
    "Para explorar el espacio de acciones puede hacer uso de la función `gw.get_allowed_actions(state)`, que recibe como parámetro el estado. Este estado se define como una tupla (*x,y*), con la posición donde se encuentra el agente.\n",
    "\n",
    "\n",
    "¿Cuál es el espacio de acciones en el estado inicial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las acciones posibles en estado inicial (2,0): \n",
      "allowed_actions = ['N', 'S', 'E', 'W']\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(\"Las acciones posibles en estado inicial (2,0): \")\n",
    "print(\"allowed_actions = \"+str(gw.get_allowed_actions((2,0))) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "allowed_actions = ['N', 'S', 'E', 'W']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obstante, se sabe que las acciones no son confiables ya que tienen un ruido inherente. Esto es, con una probabilidad de 0.8 el agente se mueve en la dirección prevista, y con una probabilidad de 0.2, el agente se mueve en una dirección aleatoria hacia los lados.\n",
    "\n",
    "Por ejemplo, si la acción deseada es 'N', con probabilidad de 0.8 se moverá al norte. Con probabilidad de 0.1 al este y con probabilidad de 0.1 al oeste.\n",
    "\n",
    "Verifique esto para la acción 'E'. Utilice los atributos `gw.real_actions[action]` y `gw.action_probabilitites`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E', 'N', 'S']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "gw.real_actions['E']\n",
    "#gw.action_probabilities(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "['E', 'N', 'S']\n",
    "[0.8, 0.1, 0.1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de la acción\n",
    "Una vez se conoce el estado actual y la acción que va a ejecutar el agente, esta se puede aplicar al entorno. En respuesta, el agente percibe el nuevo estado del entorno, así como una señal de recompensa que indica que tan buena fue se acción. Igualmente, se recibe una señal de *done* que indica si se alcanzó un estado terminal. En este caso, el episodio finaliza.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/rl_bloques.PNG\" alt=\"centered image\" width=350/>\n",
    "</center>\n",
    "\n",
    "La recompensa del agente del gridworld se define como:\n",
    "\\begin{equation*}\n",
    "R(s,a,s') = \\begin{cases}\n",
    "1 &\\text{si $s=(0,3)$}\\\\\n",
    "-1 &\\text{si $s=(1,3)$}\\\\\n",
    "0 &\\text{d.l.c}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "El método `gw.step(state, action, random)` permite aplicar una acción al gridworld. Este método retorna el nuevo estado, la recompensa, la accion ejecutada y la bandera de terminado (done), todo en ese orden. \n",
    "\n",
    "¿Qué sucede si el agente está en el estado (2,1) y realiza la acción 'N' ? Imprima los resultados con el parámetro `random = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avanzar step 't' en estado (2,1) aplicar acción N con random=False\n",
      "new_state = (2, 1)\n",
      "reward = 0.0\n",
      "action = N\n",
      "done = False\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(\"Avanzar step 't' en estado (2,1) aplicar acción N con random=False\")\n",
    "new_state, reward, _, done  = gw.step((2,1), 'N', random=False)\n",
    "print(\"new_state = \"+str(new_state) )\n",
    "print(\"reward = \"+str(reward) )\n",
    "print(\"action = \"+str(_) )\n",
    "print(\"done = \"+str(done ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**\n",
    "```\n",
    "new_state = (2, 1)\n",
    "reward = 0.0\n",
    "action = N\n",
    "done = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que el agente, al tratar de moverse en la dirección bloqueada, se queda en su misma posición.\n",
    "\n",
    "Ahora bien, repita esto con el parámetro `random = True`. En este caso, dada la aleatoriedad del entorno, puede que el resultado obtenido no sea igual al anterior. No obstante, la mayoría de las veces (el 80% para ser exacto) sí lo será. Sientase en la libertad de ejecutar varias veces la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicar step 't' en estado (2,1) aplicar acción N con random=True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 1), 0.0, 'N', False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(\"Aplicar step 't' en estado (2,1) aplicar acción N con random=True\")\n",
    "gw.step((2,1), 'N', random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se reinicia el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reiniciar el entorno\n"
     ]
    }
   ],
   "source": [
    "print(\"Reiniciar el entorno\")\n",
    "pygame.quit\n",
    "gw = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cierre la ventana emergente y reinicie el kernel de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Valor, $V$\n",
    "\n",
    "La política $\\pi$ indica, para un entorno determinístico, cuál acción debe ejecutarse en cada estado, con el objetivo de que el agente reciba la mayor utilidad. \n",
    "\n",
    "Siguiendo una política $\\pi$,  el valor de un estado $V(s)$ en el tiempo $t$ formalmente corresponde a **la suma descontada de las recompensas recibidas, si el agente tiene como estado inicial $s$ y se comporta óptimamente en adelante**. En palabras simples, indica cuánta utilidad  recibiría el agente si comenzara en $s$ y se comportara bien de ahí en adelante, \n",
    "\n",
    "$$V_{t}(s)=\\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{t-1}(s'))$$\n",
    "\n",
    "Donde $\\gamma$ es un factor conocido como *tasa de descuento*. Permite balancear la preferencia de tener recompensas a corto o largo plazo.\n",
    "\n",
    "Dada un política $\\pi(s)$ es posible encontrar el valor de cada estado por medio de programación dinámica, así ($\\gamma=0.9$):\n",
    "<center>\n",
    "    <img src=\"images/fvalor.png\" alt=\"centered image\" width=650/>\n",
    "</center>\n",
    "\n",
    "Se puede encontrar el valor de cada estado del gridworld a partir de una política fija. Suponga que tenemos la siguiente política determinística:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/gridworld2.png\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El nuevo MDP: <gridworld.GridWorld object at 0x00000232E31B1A60>\n"
     ]
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "\n",
    "gw = GridWorld(3, 4, [(1,1)], [(1,3)], [(0,3)], 0.0)\n",
    "print(\"El nuevo MDP: \"+str(gw) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw.states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina la política deterministica propuesta en la figura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 'S', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'S', (1, 2): 'N', (1, 3): 'E', (2, 0): 'E', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "\n",
    "state = gw.states\n",
    "action = ['S','E','E','E','S','N','E','E','E','N','W']\n",
    "# --------------------------------------------\n",
    "\n",
    "for i, s in enumerate(state):\n",
    "    gw.policy[s] = action[i] \n",
    "    \n",
    "print(gw.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**\n",
    "```\n",
    "{(0, 0): 'S', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'S', (1, 2): 'N', (1, 3): 'E', (2, 0): 'E', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se implementa la función de actualización para estimar los valores de los estados, según la política $\\pi(s)$, definida anteirormente. \n",
    "\n",
    "**Tenga en cuenta que:**\n",
    "* Se va a asumir que las acciones dadas por la política son determinísticas. Esto es, se aplican al entorno con probabilidad 1\n",
    "* La regla de actualización para estados terminales es: $V_{t}(s)=\\sum_{s'} P(s'|s,\\pi(s))R(s,a,s')$\n",
    "* La regla de actualización para estados no terminales es: $V_{t}(s)=\\sum_{s'} P(s'|s,\\pi(s))(R(s,a,s')+\\gamma V_{t-1}(s'))$\n",
    "* Para obtener el valor de un estado en t-1 usamos `v(s)=gw.state_values[state] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de valor de estados: actualizar la función de valor de estados v_pi\n",
    "# parametros: gw=MDP, gamma=tasa de descuento\n",
    "\n",
    "def update_values(gw, gamma):\n",
    "    # vectores para los valores de estado\n",
    "    \n",
    "    # valores de estado, para cada estado inicializar en 0 \n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    # nuevos valores de estado, para cada estado inicializar en 0\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    \n",
    "    # iterar sobre los estados posibles del MDP\n",
    "    for state in gw.states: \n",
    "        # Tomar acción de politica dada para el estado\n",
    "        action = gw.policy[state]\n",
    "        \n",
    "        # Dar un paso en par estado-accion (random=False deterministico)\n",
    "        # retorna: estado siguiente, recompensa, accion\n",
    "        new_state, reward, _, done = gw.step(state, action, random=False)\n",
    "\n",
    "        # Actualizar valores de estado\n",
    "        if(done): # llega a estado terminal\n",
    "            # actualización para un estado terminal\n",
    "            new_values[state] = reward # valor de estado para estado terminal es recompensa\n",
    "        else:\n",
    "            # actualización para un estado no terminal\n",
    "            new_values[state] = reward + gamma*gw.state_values[new_state] #borrar\n",
    "            # valor de estado para estado no terminal es recompensa + descuento*valor de estado[s_t+1]\n",
    "            \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poner a prueba la función se va a escoger una tasa de descuento de 0.9 y a realizar la estimación de los valores durante un horizonte de 15 iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "H = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:**\n",
    "1. Ejecute la siguiente celda y haga *clic* sobre la ventana del gridworld. \n",
    "2. Podra ver como se actualizan los valores de cada estado presionando la tecla **Espacio**. \n",
    "3. Una vez completetadas las 10 iteraciones, puede cerrar el gridworld presionando la tecla **Esc**.\n",
    "4. Si tienes un error en la función 'update_values', despues de corregirlo es necesario que vuelvas a correr las 4 celdas anteriores para poder volver a lanzar la interfaz del gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.update_values = MethodType(update_values, gw)\n",
    "gw.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al cabo de 10 iteraciones, el valor de los estados, siguiendo la política $\\pi(s)$ son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = 4.783\n",
      "V(0, 1) = 8.100\n",
      "V(0, 2) = 9.000\n",
      "V(0, 3) = 10.000\n",
      "V(1, 0) = 5.314\n",
      "V(1, 2) = 8.100\n",
      "V(1, 3) = -1.000\n",
      "V(2, 0) = 5.905\n",
      "V(2, 1) = 6.561\n",
      "V(2, 2) = 7.290\n",
      "V(2, 3) = 6.561\n"
     ]
    }
   ],
   "source": [
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota que los estados cercanos a (0,3) tienen valores cercanos y +1. Esto se debe a que el estado terminal (0,3) tiene un valor igual a la recompensa recibida en este estado y se propaga a los demas estados, de acuerdo a la política seguida. Note que los estados mas lejanos a (0,3) como por ejemplo (0,0), (1,0) y (2,0) tienes valores bajos pero siguen siendo positivos. Esto es porque la política define una secuencia de acciones desde cualquiera de estos tres estados al estado terminal (0,3). Sin embargo, la recompensa recibida presenta un mayor descuento, pues se deben realizar más pasos para llegar hasta (0,3).\n",
    "\n",
    "* Desde (0,0) hasta (0,3) --> 7 pasos --> $v(s)=\\gamma^7*R=0.4783$\n",
    "* Desde (1,0) hasta (0,3) --> 6 pasos --> $v(s)=\\gamma^6*R=0.5314$\n",
    "* Desde (2,0) hasta (0,3) --> 5 pasos --> $v(s)=\\gamma^5*R=0.5905$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ahora sí, es momento de implementar los métodos que permiten obtener una política para maximizar las recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration $V^{*}$\n",
    "\n",
    "Este método permite aproximar los valores óptimos de los estados, de tal manera que se pueda encontrar, al final, una política $\\pi^{*}(s)$ que permita maximizar la recompensa recibida por el agente a lo largo del horizonte.\n",
    "\n",
    "$$V^{*}(s)=\\max_{\\pi} \\mathbb{E}\\left[ \\sum_{t=0}^{H} \\gamma^{t} R(s_t,a_t,s_{t+1}) | \\pi, s_0=s \\right]$$\n",
    "\n",
    "Cuando el horizonte es $H=0$, el valor de todos los estados es igual a su valor de inicialización, generalmente cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El MDP: <gridworld.GridWorld object at 0x00000232E31B1730>\n"
     ]
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "from colorama import Fore\n",
    "\n",
    "gw = GridWorld(3, 4, [(1,1)], [(1,3)], [(0,3)], 0.0)\n",
    "print(\"El MDP: \"+str(gw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de estados iniciales: {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.0, (1, 0): 0.0, (1, 2): 0.0, (1, 3): 0.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Inicializar valores de los estados\n",
    "gw.state_values = gw.init_values()\n",
    "print(\"Valores de estados iniciales: \"+str(gw.state_values) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los valores de los siguientes estados: (2,0), (0,3), (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de estado 2,0: 0.0\n",
      "Valor de estado 0,3: 0.0\n",
      "Valor de estado 1,3: 0.0\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(\"Valor de estado 2,0: \"+str(gw.state_values[2,0]) )\n",
    "print(\"Valor de estado 0,3: \"+str(gw.state_values[0,3]) )\n",
    "print(\"Valor de estado 1,3: \"+str(gw.state_values[1,3]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**\n",
    "```\n",
    "V(2,0) = 0.0\n",
    "V(0,3) = 0.0\n",
    "V(1,3) = 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando el horizonte es $H=1$, el valor del estado $s$ corresponde la recompensa recibida para todas las transiciones a $s'$ posibles + el valor del nuevo estado $s'$:\n",
    "$$V_{1}^{*}(s)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{0}^{*}(s'))$$\n",
    "\n",
    "Pero si en $H=1$ el agente está en un estado terminal, por ejemplo (0,3), el valor de $V_1(0,3)$ se actualizaría así:\n",
    "$$V_{1}^{*}(0,3)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')$$\n",
    "En este caso se omite el término $V_{0}^{*}(s')$ ya que esta transición no existe al finalizarce el episodio. Por esta razón, \"el valor de los estados terminales es igual a la recompensa recibida en ellos\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.0,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 2): 0.0,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): 0.0,\n",
       " (2, 1): 0.0,\n",
       " (2, 2): 0.0,\n",
       " (2, 3): 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se ajusta el valor de los estados terminales\n",
    "\n",
    "# definir valor de estados terminales\n",
    "gw.state_values[(0,3)] = 1.0\n",
    "gw.state_values[(1,3)] = -1.0  \n",
    "gw.state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el horizonte es $H=2$, para estados no terminales:\n",
    "$$V_{2}^{*}(s)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s'))$$\n",
    "Por ejemplo, si el agente está en (0,2) y se tiene un descuento $\\gamma=0.90$:\n",
    "\n",
    "* Si pretende ejecutar 'E':\n",
    "    * Si efectivamente se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(0.0+\\gamma V_1(0,3))$\n",
    "    * Si, por ruido, se ejecuta 'N':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,2))$\n",
    "    * Si, por ruido, se ejecuta 'S':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(1,2))$\n",
    "    * Valor = $0.8(0.9*1.0)+0.1(0.0)+0.1*(0.0)=0.72$\n",
    "    \n",
    "* Si pretende ejecutar 'W':\n",
    "    * Si efectivamente se ejecuta 'W':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(0.0+\\gamma V_1(0,1))$\n",
    "    * Si, por ruido, se ejecuta 'N':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,2))$\n",
    "    * Si, por ruido, se ejecuta 'S':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(1,2))$\n",
    "    * Valor = $0.8(0.0)+0.1(0.0)+0.1*(0.0)=0.0$\n",
    "\n",
    "* Si pretende ejecutar 'N':\n",
    "    * Si efectivamente se ejecuta 'N':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(0.0+\\gamma V_1(0,2))$\n",
    "    * Si, por ruido, se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,3))$\n",
    "    * Si, por ruido, se ejecuta 'W':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,1))$\n",
    "    * Valor = $0.8(0.0)+0.1(0.9*1.0)+0.1*(0.0)=0.09$\n",
    "    \n",
    "* Si pretende ejecutar 'S':\n",
    "    * Si efectivamente se ejecuta 'S':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(0.0+\\gamma V_1(1,2))$\n",
    "    * Si, por ruido, se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,3))$\n",
    "    * Si, por ruido, se ejecuta 'W':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,1))$\n",
    "    * Valor = $0.8(0.0)+0.1(0.9*1.0)+0.1*(0.0)=0.09$\n",
    "    \n",
    "Por ende, la acción que maximiza el valor del estado (0,2) es 'E' y $V_{2}^{*}(0,2)=0.72$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    ">Inicializar el valor de todos los estados como $V_{0}^{*}(s)=0$<br>\n",
    ">Para $k=1, \\cdots, H$:<br>\n",
    ">> Para todos los estados $s$:<br>\n",
    ">>> $\\displaystyle V_{k}^{*}(s)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{k-1}^{*}(s'))$<br>\n",
    ">>> $\\displaystyle \\pi_{k}^{*}(s)=arg\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{k-1}^{*}(s'))$<br>\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aleatoria incial: {(0, 0): 'W', (0, 1): 'W', (0, 2): 'N', (0, 3): 'E', (1, 0): 'W', (1, 2): 'W', (1, 3): 'E', (2, 0): 'W', (2, 1): 'E', (2, 2): 'S', (2, 3): 'S'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 0.09000000000000001\n",
      "Suma valor de acciones para acción S : 0.09000000000000001\n",
      "Suma valor de acciones para acción E : 0.7200000000000001\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.7200000000000001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : -0.09000000000000001\n",
      "Suma valor de acciones para acción S : -0.09000000000000001\n",
      "Suma valor de acciones para acción E : -0.7200000000000001\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.7200000000000001\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : -0.09000000000000001\n",
      "Suma valor de acciones para acción W : -0.09000000000000001\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: S\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.7200000000000001, (0, 3): 10.0, (1, 0): 0.0, (1, 2): 0.0, (1, 3): -1.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0}\n",
      "Política óptima: {(0, 0): 'N', (0, 1): 'N', (0, 2): 'E', (0, 3): 'E', (1, 0): 'N', (1, 2): 'W', (1, 3): 'E', (2, 0): 'N', (2, 1): 'N', (2, 2): 'N', (2, 3): 'S'}\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "\n",
    "# value_iteration: encontrar valores de estado óptimos para cada estado y política óptima para cada estado \n",
    "# params: gw=MDP gridworld, gamma=descuento\n",
    "\n",
    "def value_iteration(gw,gamma):   \n",
    "    \n",
    "    # valores de estado, para cada estado inicializar en 0 \n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    # nuevos valores de estado, para cada estado inicializar en 0\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)    \n",
    "    \n",
    "    \n",
    "    # politica aleatoria \n",
    "    # inicializar politica aleatoria \n",
    "    gw.policy = {}\n",
    "    keys = gw.states # keys del diccionario son los estados posibles del MDP\n",
    "    \n",
    "    # iterar sobre estados posibles del MDP\n",
    "    for state in keys:    \n",
    "        # inicializar valores de política aleatoria escogiendo acción aleatoria en estado \n",
    "        gw.policy[state] = random.choice(gw.get_allowed_actions(state) )\n",
    "    print(\"Política aleatoria incial: \"+str(gw.policy) )    \n",
    "    \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:          \n",
    "        \n",
    "        # Dar un paso en par estado-accion con accion dada por la política aleatoria\n",
    "        new_state, reward, _, done = gw.step(state, gw.policy[(state)], random=True)\n",
    "        \n",
    "        #print(\"Estado anterior: \"+str(state) )\n",
    "        #print(\"Acción aleatoria: \"+str(gw.policy[state]) )\n",
    "        #print(\"Estado siguiente: \"+str(new_state) )\n",
    "        #print(\"Recompensa estado siguiente: \"+str(reward))\n",
    "        #print(\"\")\n",
    "        \n",
    "        if(done==True): # llega a estado terminal\n",
    "            # actualización para un estado terminal        \n",
    "            new_values[state] = reward # valor de estado para estado terminal es recompensa\n",
    "            print(\"Estado terminal: \"+str(state) )\n",
    "            print(\"Recompensa estado terminal: \"+str(reward) )\n",
    "        else: # llega a estado no terminal\n",
    "            # actualización para un estado no terminal\n",
    "            \n",
    "            accion_opt = -1 # acción óptima en estado (política)\n",
    "            val_estado_opt = -100 # valor óptimo en estado \n",
    "            print(\"Estado actual: \"+str(state) )\n",
    "            \n",
    "            \n",
    "            # iterar sobre acciones permitidas en estado state (accion - E)\n",
    "            for action in gw.get_allowed_actions(state):\n",
    "                \n",
    "                suma_actions = 0 # suma de valores de par estado-accion real que puede tomar (en un estado)\n",
    "                # iterar sobre Las posibles acciones (reales) que puede tomar el agente dada una acción\n",
    "                \n",
    "                for real_action in  gw.real_actions[action]:\n",
    "                    # si acción real es igual a la acción que toma: probabilidad 0.8\n",
    "                    if real_action==action: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities[0]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.8\n",
    "                        \n",
    "                    # si acción real es diferente a la acción que toma:  probabilidad 0.1\n",
    "                    else: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities[1]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.1\n",
    "                \n",
    "                print(\"Suma valor de acciones para acción \"+str(action)+' : '+str( suma_actions) )\n",
    "                if suma_actions>val_estado_opt:\n",
    "                    val_estado_opt = suma_actions\n",
    "                    accion_opt=action\n",
    "        \n",
    "            # actualizar valor de estado con el valor óptimo de la acción\n",
    "            new_values[state]= val_estado_opt\n",
    "            # actualizar política con acción óptima para el estado\n",
    "            gw.policy[state] = accion_opt\n",
    "            \n",
    "            \n",
    "            print(\"Valor de estado óptimo: \"+str(new_values[state]) )\n",
    "            print(\"Acción óptima en estado: \"+str(gw.policy[state]) )\n",
    "            print(\"\")\n",
    "            \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values)         \n",
    "    print(\"Valores de estado óptimos: \"+str(new_values) )\n",
    "    print(\"Política óptima: \"+str (gw.policy))\n",
    "        \n",
    "value_iteration(gw, 0.9)\n",
    "\n",
    "#gw.get_allowed_actions(state): Permite conocer el espacio de acciones dado un estado.\n",
    "#gw.real_actions[action]: Las posibles acciones (reales) que puede tomar el agente dada una acción.\n",
    "#gw.action_probabilities[action]: La probabilidad de que el agente tomé las acciones reales.\n",
    "#gw.step(state, action, random): Permite ejecutar un paso en el gridworld (MDP). Con este método se puede conocer el estado siguiente, la recompensa, la accion tomada y la bandera de un estado terminal. Nota: si el parámetro random es Falso, las acciones son determinisiticas (con probabilidad 1, toma la acción dada).\n",
    "#gw.state_values[state]: Los valores del estado state.\n",
    "#gw.policy[state]: La acción dada por la política para el estado state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n",
    "\n",
    "# value_iteration: encontrar valores de estado óptimos para cada estado y política óptima para cada estado \n",
    "# params: gw=MDP gridworld, gamma=descuento\n",
    "\n",
    "def value_iteration2(gw,gamma):   \n",
    "    \n",
    "    # valores de estado, para cada estado inicializar en 0 \n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    # nuevos valores de estado, para cada estado inicializar en 0\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)    \n",
    "    \n",
    "    \n",
    "    # politica aleatoria \n",
    "    # inicializar politica aleatoria \n",
    "    gw.policy = {}\n",
    "    keys = gw.states # keys del diccionario son los estados posibles del MDP\n",
    "    \n",
    "    # iterar sobre estados posibles del MDP\n",
    "    for state in keys:    \n",
    "        # inicializar valores de política aleatoria escogiendo acción aleatoria en estado \n",
    "        gw.policy[state] = random.choice(gw.get_allowed_actions(state) )\n",
    "    print(\"Política aleatoria incial: \"+str(gw.policy) )    \n",
    "    \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:          \n",
    "        \n",
    "        # Dar un paso en estado siguiendo accion dada por la política actual\n",
    "        new_state, reward, _, done = gw.step(state, gw.policy[(state)], random=True)\n",
    "        \n",
    "        #print(\"Estado anterior: \"+str(state) )\n",
    "        #print(\"Acción aleatoria: \"+str(gw.policy[state]) )\n",
    "        #print(\"Estado siguiente: \"+str(new_state) )\n",
    "        #print(\"Recompensa estado siguiente: \"+str(reward))\n",
    "        #print(\"\")\n",
    "        \n",
    "        if(done==True): # llega a estado terminal\n",
    "            # actualización para un estado terminal        \n",
    "            # el valor de estado de un estado terminal es 0\n",
    "            \n",
    "            new_values[state] = 0 # valor de estado para estado terminal es 0\n",
    "            print(\"Estado terminal: \"+str(state) )\n",
    "            print(\"Recompensa estado terminal: \"+str(reward) )\n",
    "            \n",
    "        else: # llega a estado no terminal\n",
    "            # actualización para un estado no terminal\n",
    "            accion_opt = -1 # acción óptima en estado (política)\n",
    "            val_estado_opt = -100 # valor óptimo en estado \n",
    "            print(\"Estado actual: \"+str(state) )\n",
    "            \n",
    "            \n",
    "            # iterar sobre acciones permitidas en estado state (accion - E)\n",
    "            for action in gw.get_allowed_actions(state):\n",
    "                \n",
    "                suma_actions = 0 # suma de valores de par estado-accion real que puede tomar (en un estado)\n",
    "                \n",
    "                # iterar sobre Las posibles acciones (reales) que puede tomar el agente, dada una acción  (s')\n",
    "                for real_action in  gw.real_actions2[action]:\n",
    "                    \n",
    "                    # si acción real es igual a la acción que toma: probabilidad 0.8\n",
    "                    if real_action==action: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities2[0]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.8\n",
    "                        \n",
    "                    # si acción real es diferente a la acción que toma:  probabilidad 0.2\n",
    "                    else: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities2[1]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.2\n",
    "                \n",
    "                print(\"Suma valor de acciones para acción \"+str(action)+' : '+str( suma_actions) )\n",
    "                if suma_actions>val_estado_opt:\n",
    "                    val_estado_opt = suma_actions\n",
    "                    accion_opt=action\n",
    "        \n",
    "            # actualizar valor de estado con el valor óptimo de la acción\n",
    "            new_values[state]= val_estado_opt\n",
    "            # actualizar política con acción óptima para el estado\n",
    "            gw.policy[state] = accion_opt\n",
    "            \n",
    "            print(\"Valor de estado óptimo: \"+str(new_values[state]) )\n",
    "            print(\"Acción óptima en estado: \"+str(gw.policy[state]) )\n",
    "            print(\"\")\n",
    "            \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values)         \n",
    "    print(\"Valores de estado óptimos: \"+str(new_values) )\n",
    "    print(\"Política óptima: \"+str (gw.policy))\n",
    "    \n",
    "    \n",
    "#gw.get_allowed_actions(state): Permite conocer el espacio de acciones dado un estado.\n",
    "#gw.real_actions[action]: Las posibles acciones (reales) que puede tomar el agente dada una acción.\n",
    "#gw.action_probabilities[action]: La probabilidad de que el agente tomé las acciones reales.\n",
    "#gw.step(state, action, random): Permite ejecutar un paso en el gridworld (MDP). Con este método se puede conocer el estado siguiente, la recompensa, la accion tomada y la bandera de un estado terminal. Nota: si el parámetro random es Falso, las acciones son determinisiticas (con probabilidad 1, toma la acción dada).\n",
    "#gw.state_values[state]: Los valores del estado state.\n",
    "#gw.policy[state]: La acción dada por la política para el estado state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de clase Gridworld (MDP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acciones dado un estado['N', 'S', 'E', 'W']\n"
     ]
    }
   ],
   "source": [
    "print(\"acciones dado un estado\"+str(gw.get_allowed_actions((0,0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acciones que puede tomar el agente dada una acción['N', 'E', 'W']\n"
     ]
    }
   ],
   "source": [
    "print(\"acciones que puede tomar el agente dada una acción\"+str(gw.real_actions['N']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de acción 1 (real): 0.8\n",
      "Probabilidad de acción 2 (ruido1): 0.1\n",
      "Probabilidad de acción 3 (ruido2): 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Probabilidad de acción 1 (real): \"+str(gw.action_probabilities[0]))\n",
    "print(\"Probabilidad de acción 2 (ruido1): \"+str(gw.action_probabilities[1]))\n",
    "print(\"Probabilidad de acción 3 (ruido2): \"+str(gw.action_probabilities[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estado siguiente: (0, 3)\n",
      "recompensa estado siguiente: 0.0\n",
      "accion anterior: E\n",
      "bandera estado siguiente: False\n"
     ]
    }
   ],
   "source": [
    "state = (0,2) # estado anterior\n",
    "action = 'E' # acción estado anterior\n",
    "\n",
    "# Dar un paso en par estado-accion (random=[False deterministico/True estocástico] )\n",
    "new_state, reward, _, done = gw.step(state, action, random=False)\n",
    "print(\"estado siguiente: \"+str(new_state) )\n",
    "print(\"recompensa estado siguiente: \"+str(reward) )\n",
    "print(\"accion anterior: \"+str(_))\n",
    "print(\"bandera estado siguiente: \"+str(done) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de estado actuales v_pi(0,2): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Valores de estado actuales v_pi(0,2): \")\n",
    "gw.state_values[(0,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción dada por la política para el estado (0,2): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Acción dada por la política para el estado (0,2): \")\n",
    "gw.policy[(0,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementemos el algoritmo de value iteration. Las siguientes funciones o atributos pueden ser útiles:\n",
    "* `gw.get_allowed_actions(state)`: Permite conocer el espacio de acciones dado un estado.\n",
    "* `gw.real_actions[action]`: Las posibles acciones (reales) que puede tomar el agente dada una acción.\n",
    "* `gw.action_probabilities[action]`: La probabilidad de que el agente tomé las acciones reales.\n",
    "* `gw.step(state, action, random)`: Permite ejecutar un paso en el gridworld (MDP). Con este método se puede conocer el estado siguiente, la recompensa, la accion tomada y la bandera de un estado terminal. Nota: si el parámetro random es Falso, las acciones son determinisiticas (con probabilidad 1, toma la acción dada).\n",
    "* `gw.state_values[state]`: Los valores del estado *state*.\n",
    "* `gw.policy[state]`: La acción dada por la política para el estado *state*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tenga en cuenta:** Si tiene un error en la función 'value_iteration', despues de corregirlo es necesario que vuelva a ejecutar la celda anterior para tener en cuenta el cambio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique con la siguiente celda que el valor del estado (0,2) es el correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aleatoria incial: {(0, 0): 'E', (0, 1): 'S', (0, 2): 'N', (0, 3): 'E', (1, 0): 'S', (1, 2): 'E', (1, 3): 'N', (2, 0): 'N', (2, 1): 'E', (2, 2): 'W', (2, 3): 'N'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 0.06480000000000001\n",
      "Suma valor de acciones para acción S : 0.06480000000000001\n",
      "Suma valor de acciones para acción E : 0.5184000000000001\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.5184000000000001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 1.4184\n",
      "Suma valor de acciones para acción S : 0.9\n",
      "Suma valor de acciones para acción E : 7.2648\n",
      "Suma valor de acciones para acción W : 0.06480000000000001\n",
      "Valor de estado óptimo: 7.2648\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 0.42840000000000006\n",
      "Suma valor de acciones para acción S : -0.09000000000000001\n",
      "Suma valor de acciones para acción E : -0.6552000000000001\n",
      "Suma valor de acciones para acción W : 0.06480000000000001\n",
      "Valor de estado óptimo: 0.42840000000000006\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.7200000000000001\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : -0.09000000000000001\n",
      "Suma valor de acciones para acción W : -0.09000000000000001\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: S\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 0.0, (0, 1): 0.5184000000000001, (0, 2): 7.2648, (0, 3): 10.0, (1, 0): 0.0, (1, 2): 0.42840000000000006, (1, 3): -1.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0}\n",
      "Política óptima: {(0, 0): 'N', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'N', (1, 2): 'N', (1, 3): 'N', (2, 0): 'N', (2, 1): 'N', (2, 2): 'N', (2, 3): 'S'}\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "value_iteration(gw, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado\\*:**\n",
    "```\n",
    "Para H=2, V(0, 2) = 0.7200000000000001\n",
    "Para H=2, acción que maximiza el valor = E\n",
    "```\n",
    "\n",
    "\\* Recuerde reiniciar los valores y ejecutar una única vez el entorno si desea tener un horizonte H=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora resuelva el gridworld por *value iteration* utilizando la función que usted implementó:\n",
    "* Incialice los estados iniciales\n",
    "* Establezca un $\\gamma=0.90$, un horizonte de 15 iteraciones y el estado inicial en (2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.2.0 (SDL 2.0.22, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "El nuevo MDP: <gridworld.GridWorld object at 0x00000196B2C77190>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.0,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 2): 0.0,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): 0.0,\n",
       " (2, 1): 0.0,\n",
       " (2, 2): 0.0,\n",
       " (2, 3): 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "# Se define una política inicial cualquiera\n",
    "#gw.policy = dict.fromkeys(gw.states, 'N')\n",
    "\n",
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "\n",
    "# crear MDP=GridWorld\n",
    "# params: 3 filas, 4 columnas,  (1,1)=celda bloqueada, (1,3)=bomba, (0,3)=diamante, 0.0=valor de estados inicial\n",
    "gw = GridWorld(3, 4, [(1,1)], [(1,3)], [(0,3)], 0.0)\n",
    "print(\"El nuevo MDP: \"+str(gw) )\n",
    "\n",
    "# inicializar valores de estado\n",
    "\n",
    "# definir valor de estados terminales\n",
    "gw.state_values[(0,3)] = 1.0\n",
    "gw.state_values[(1,3)] = -1.0  \n",
    "init_state= gw.state_values\n",
    "\n",
    "gamma=0.9 # tasa descuento\n",
    "H=15 # horizonte de tiempo\n",
    "\n",
    "init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:**\n",
    "1. Ejecute la siguiente celda y haga *clic* sobre la ventana del gridworld. \n",
    "2. Podra ver como se actualizan los valores de cada estado presionando la tecla **Espacio**. \n",
    "3. Una vez completes las 30 iteraciones, presiona la tecla **Enter** para ver la política\n",
    "4. Puede cerrar el gridworld presionando la tecla **Esc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aleatoria incial: {(0, 0): 'E', (0, 1): 'W', (0, 2): 'E', (0, 3): 'S', (1, 0): 'W', (1, 2): 'W', (1, 3): 'E', (2, 0): 'N', (2, 1): 'N', (2, 2): 'N', (2, 3): 'W'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 0.09000000000000001\n",
      "Suma valor de acciones para acción S : 0.09000000000000001\n",
      "Suma valor de acciones para acción E : 0.7200000000000001\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.7200000000000001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : -0.09000000000000001\n",
      "Suma valor de acciones para acción S : -0.09000000000000001\n",
      "Suma valor de acciones para acción E : -0.7200000000000001\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.7200000000000001\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : -0.09000000000000001\n",
      "Suma valor de acciones para acción W : -0.09000000000000001\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: S\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.7200000000000001, (0, 3): 10.0, (1, 0): 0.0, (1, 2): 0.0, (1, 3): -1.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0}\n",
      "Política óptima: {(0, 0): 'N', (0, 1): 'N', (0, 2): 'E', (0, 3): 'S', (1, 0): 'N', (1, 2): 'W', (1, 3): 'E', (2, 0): 'N', (2, 1): 'N', (2, 2): 'N', (2, 3): 'S'}\n",
      "Política aleatoria incial: {(0, 0): 'N', (0, 1): 'W', (0, 2): 'E', (0, 3): 'S', (1, 0): 'E', (1, 2): 'N', (1, 3): 'S', (2, 0): 'S', (2, 1): 'S', (2, 2): 'S', (2, 3): 'W'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 0.06480000000000001\n",
      "Suma valor de acciones para acción S : 0.06480000000000001\n",
      "Suma valor de acciones para acción E : 0.5184000000000001\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.5184000000000001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 1.4184\n",
      "Suma valor de acciones para acción S : 0.9\n",
      "Suma valor de acciones para acción E : 7.2648\n",
      "Suma valor de acciones para acción W : 0.06480000000000001\n",
      "Valor de estado óptimo: 7.2648\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 0.42840000000000006\n",
      "Suma valor de acciones para acción S : -0.09000000000000001\n",
      "Suma valor de acciones para acción E : -0.6552000000000001\n",
      "Suma valor de acciones para acción W : 0.06480000000000001\n",
      "Valor de estado óptimo: 0.42840000000000006\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.7200000000000001\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : -0.09000000000000001\n",
      "Suma valor de acciones para acción W : -0.09000000000000001\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: S\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 0.0, (0, 1): 0.5184000000000001, (0, 2): 7.2648, (0, 3): 10.0, (1, 0): 0.0, (1, 2): 0.42840000000000006, (1, 3): -1.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0}\n",
      "Política óptima: {(0, 0): 'N', (0, 1): 'E', (0, 2): 'E', (0, 3): 'S', (1, 0): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'N', (2, 2): 'N', (2, 3): 'S'}\n",
      "Política aleatoria incial: {(0, 0): 'N', (0, 1): 'N', (0, 2): 'E', (0, 3): 'W', (1, 0): 'N', (1, 2): 'E', (1, 3): 'E', (2, 0): 'N', (2, 1): 'E', (2, 2): 'S', (2, 3): 'E'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 0.04665600000000001\n",
      "Suma valor de acciones para acción S : 0.04665600000000001\n",
      "Suma valor de acciones para acción E : 0.3732480000000001\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.3732480000000001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 1.0270800000000002\n",
      "Suma valor de acciones para acción S : 1.0270800000000002\n",
      "Suma valor de acciones para acción E : 5.323968\n",
      "Suma valor de acciones para acción W : 0.09331200000000002\n",
      "Valor de estado óptimo: 5.323968\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 6.177312000000001\n",
      "Suma valor de acciones para acción S : 1.2551040000000002\n",
      "Suma valor de acciones para acción E : 7.892388\n",
      "Suma valor de acciones para acción W : 1.0656360000000002\n",
      "Valor de estado óptimo: 7.892388\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 5.179212000000001\n",
      "Suma valor de acciones para acción S : -0.051444000000000004\n",
      "Suma valor de acciones para acción E : -0.066168\n",
      "Suma valor de acciones para acción W : 0.9622800000000001\n",
      "Valor de estado óptimo: 5.179212000000001\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 0.30844800000000006\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.03855600000000001\n",
      "Suma valor de acciones para acción W : 0.03855600000000001\n",
      "Valor de estado óptimo: 0.30844800000000006\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.7200000000000001\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : -0.09000000000000001\n",
      "Suma valor de acciones para acción W : -0.09000000000000001\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: S\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 0.3732480000000001, (0, 1): 5.323968, (0, 2): 7.892388, (0, 3): 10.0, (1, 0): 0.0, (1, 2): 5.179212000000001, (1, 3): -1.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.30844800000000006, (2, 3): 0.0}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'W', (1, 0): 'N', (1, 2): 'N', (1, 3): 'E', (2, 0): 'N', (2, 1): 'N', (2, 2): 'N', (2, 3): 'S'}\n",
      "Política aleatoria incial: {(0, 0): 'W', (0, 1): 'N', (0, 2): 'W', (0, 3): 'E', (1, 0): 'S', (1, 2): 'E', (1, 3): 'S', (2, 0): 'S', (2, 1): 'S', (2, 2): 'N', (2, 3): 'E'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 0.7814880000000001\n",
      "Suma valor de acciones para acción S : 0.51274944\n",
      "Suma valor de acciones para acción E : 3.86684928\n",
      "Suma valor de acciones para acción W : 0.3023308800000001\n",
      "Valor de estado óptimo: 3.86684928\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 4.5771642\n",
      "Suma valor de acciones para acción S : 4.5771642\n",
      "Suma valor de acciones para acción E : 6.640833600000001\n",
      "Suma valor de acciones para acción W : 1.2270528\n",
      "Valor de estado óptimo: 6.640833600000001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.061676480000001\n",
      "Suma valor de acciones para acción S : 5.108189760000001\n",
      "Suma valor de acciones para acción E : 8.376444000000001\n",
      "Suma valor de acciones para acción W : 5.00970096\n",
      "Valor de estado óptimo: 8.376444000000001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 0.26873856000000007\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.03359232000000001\n",
      "Suma valor de acciones para acción W : 0.03359232000000001\n",
      "Valor de estado óptimo: 0.26873856000000007\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.058648440000001\n",
      "Suma valor de acciones para acción S : 0.5982116400000002\n",
      "Suma valor de acciones para acción E : 0.018075239999999992\n",
      "Suma valor de acciones para acción W : 4.46710788\n",
      "Valor de estado óptimo: 6.058648440000001\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 0.0\n",
      "Suma valor de acciones para acción S : 0.0\n",
      "Suma valor de acciones para acción E : 0.0\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.0\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 0.027760320000000005\n",
      "Suma valor de acciones para acción S : 0.027760320000000005\n",
      "Suma valor de acciones para acción E : 0.22208256000000004\n",
      "Suma valor de acciones para acción W : 0.0\n",
      "Valor de estado óptimo: 0.22208256000000004\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 3.7290326400000007\n",
      "Suma valor de acciones para acción S : 0.22208256000000004\n",
      "Suma valor de acciones para acción E : 0.4938894000000001\n",
      "Suma valor de acciones para acción W : 0.4938894000000001\n",
      "Valor de estado óptimo: 3.7290326400000007\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.6922396800000001\n",
      "Suma valor de acciones para acción S : 0.027760320000000005\n",
      "Suma valor de acciones para acción E : -0.09000000000000001\n",
      "Suma valor de acciones para acción W : 0.13208256000000002\n",
      "Valor de estado óptimo: 0.13208256000000002\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 3.86684928, (0, 1): 6.640833600000001, (0, 2): 8.376444000000001, (0, 3): 10.0, (1, 0): 0.26873856000000007, (1, 2): 6.058648440000001, (1, 3): -1.0, (2, 0): 0.0, (2, 1): 0.22208256000000004, (2, 2): 3.7290326400000007, (2, 3): 0.13208256000000002}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aleatoria incial: {(0, 0): 'W', (0, 1): 'N', (0, 2): 'N', (0, 3): 'S', (1, 0): 'S', (1, 2): 'N', (1, 3): 'N', (2, 0): 'W', (2, 1): 'E', (2, 2): 'S', (2, 3): 'S'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 3.7298229408\n",
      "Suma valor de acciones para acción S : 1.1391832224000003\n",
      "Suma valor de acciones para acción E : 5.1536030976000005\n",
      "Suma valor de acciones para acción W : 3.1563343872000003\n",
      "Valor de estado óptimo: 5.1536030976000005\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 5.8832965872\n",
      "Suma valor de acciones para acción S : 5.8832965872\n",
      "Suma valor de acciones para acción E : 7.226389728000001\n",
      "Suma valor de acciones para acción W : 3.9794815296\n",
      "Valor de estado óptimo: 7.226389728000001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.528714704000001\n",
      "Suma valor de acciones para acción S : 5.859901900800001\n",
      "Suma valor de acciones para acción E : 8.4991583196\n",
      "Suma valor de acciones para acción W : 6.0805585116000005\n",
      "Valor de estado óptimo: 8.4991583196\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 2.8325044224000004\n",
      "Suma valor de acciones para acción S : 0.048372940800000014\n",
      "Suma valor de acciones para acción E : 0.5415081984000001\n",
      "Suma valor de acciones para acción W : 0.5415081984000001\n",
      "Valor de estado óptimo: 2.8325044224000004\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.486318039600001\n",
      "Suma valor de acciones para acción S : 3.140181860400001\n",
      "Suma valor de acciones para acción E : 0.36949289760000015\n",
      "Suma valor de acciones para acción W : 5.451719774400001\n",
      "Valor de estado óptimo: 6.486318039600001\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 0.21347919360000006\n",
      "Suma valor de acciones para acción S : 0.019987430400000006\n",
      "Suma valor de acciones para acción E : 0.18408591360000004\n",
      "Suma valor de acciones para acción W : 0.024186470400000007\n",
      "Valor de estado óptimo: 0.21347919360000006\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 0.49551238080000015\n",
      "Suma valor de acciones para acción S : 0.49551238080000015\n",
      "Suma valor de acciones para acción E : 2.724878361600001\n",
      "Suma valor de acciones para acción W : 0.03997486080000001\n",
      "Valor de estado óptimo: 2.724878361600001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 4.3941017376\n",
      "Suma valor de acciones para acción S : 2.7167783616000007\n",
      "Suma valor de acciones para acción E : 0.9759907404000001\n",
      "Suma valor de acciones para acción W : 1.0407907404000003\n",
      "Valor de estado óptimo: 4.3941017376\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.372499632\n",
      "Suma valor de acciones para acción S : 0.44259981120000014\n",
      "Suma valor de acciones para acción E : 0.016986873600000015\n",
      "Suma valor de acciones para acción W : 2.606790931200001\n",
      "Valor de estado óptimo: 2.606790931200001\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 5.1536030976000005, (0, 1): 7.226389728000001, (0, 2): 8.4991583196, (0, 3): 10.0, (1, 0): 2.8325044224000004, (1, 2): 6.486318039600001, (1, 3): -1.0, (2, 0): 0.21347919360000006, (2, 1): 2.724878361600001, (2, 2): 4.3941017376, (2, 3): 2.606790931200001}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'S', (1, 0): 'N', (1, 2): 'N', (1, 3): 'N', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
      "Política aleatoria incial: {(0, 0): 'W', (0, 1): 'E', (0, 2): 'W', (0, 3): 'W', (1, 0): 'E', (1, 2): 'N', (1, 3): 'E', (2, 0): 'N', (2, 1): 'W', (2, 2): 'E', (2, 3): 'S'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 4.824793584576001\n",
      "Suma valor de acciones para acción S : 3.1536025384320006\n",
      "Suma valor de acciones para acción E : 5.921750280960001\n",
      "Suma valor de acciones para acción W : 4.429343907072\n",
      "Valor de estado óptimo: 5.921750280960001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.431749131708002\n",
      "Suma valor de acciones para acción S : 6.431749131708002\n",
      "Suma valor de acciones para acción E : 7.420144141152001\n",
      "Suma valor de acciones para acción W : 5.011344381312001\n",
      "Valor de estado óptimo: 7.420144141152001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.669769065632001\n",
      "Suma valor de acciones para acción S : 6.220524064032002\n",
      "Suma valor de acciones para acción E : 8.548692872328001\n",
      "Suma valor de acciones para acción W : 6.551693476488002\n",
      "Valor de estado óptimo: 8.548692872328001\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 4.2204450263040005\n",
      "Suma valor de acciones para acción S : 0.6635558154240002\n",
      "Suma valor de acciones para acción E : 2.5224405903360005\n",
      "Suma valor de acciones para acción W : 2.5224405903360005\n",
      "Valor de estado óptimo: 4.2204450263040005\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.613162613676001\n",
      "Suma valor de acciones para acción S : 3.6575218746360005\n",
      "Suma valor de acciones para acción E : 0.44039340514799996\n",
      "Suma valor de acciones para acción W : 5.830542393660002\n",
      "Valor de estado óptimo: 6.613162613676001\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 2.3038553640960004\n",
      "Suma valor de acciones para acción S : 0.41815719936000023\n",
      "Suma valor de acciones para acción E : 2.2360509457920013\n",
      "Suma valor de acciones para acción W : 0.4278435448320001\n",
      "Valor de estado óptimo: 2.3038553640960004\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 2.3765947041600013\n",
      "Suma valor de acciones para acción S : 2.3765947041600013\n",
      "Suma valor de acciones para acción E : 3.6542313561600004\n",
      "Suma valor de acciones para acción W : 0.6441831244800004\n",
      "Valor de estado óptimo: 3.6542313561600004\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.149999224864001\n",
      "Suma valor de acciones para acción S : 3.6436034874240004\n",
      "Suma valor de acciones para acción E : 2.856127250412001\n",
      "Suma valor de acciones para acción W : 2.941150200300001\n",
      "Valor de estado óptimo: 5.149999224864001\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.08991965980799993\n",
      "Suma valor de acciones para acción S : 2.5069698106560008\n",
      "Suma valor de acciones para acción E : 2.021500654272001\n",
      "Suma valor de acciones para acción W : 3.3083644348800005\n",
      "Valor de estado óptimo: 3.3083644348800005\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 5.921750280960001, (0, 1): 7.420144141152001, (0, 2): 8.548692872328001, (0, 3): 10.0, (1, 0): 4.2204450263040005, (1, 2): 6.613162613676001, (1, 3): -1.0, (2, 0): 2.3038553640960004, (2, 1): 3.6542313561600004, (2, 2): 5.149999224864001, (2, 3): 3.3083644348800005}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'W', (1, 0): 'N', (1, 2): 'N', (1, 3): 'E', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
      "Política aleatoria incial: {(0, 0): 'S', (0, 1): 'N', (0, 2): 'E', (0, 3): 'N', (1, 0): 'S', (1, 2): 'E', (1, 3): 'S', (2, 0): 'E', (2, 1): 'W', (2, 2): 'N', (2, 3): 'N'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.464430700281281\n",
      "Suma valor de acciones para acción S : 4.23949091692896\n",
      "Suma valor de acciones para acción E : 6.255301359283201\n",
      "Suma valor de acciones para acción W : 5.176457779944961\n",
      "Valor de estado óptimo: 6.255301359283201\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.644843665425361\n",
      "Suma valor de acciones para acción S : 6.644843665425361\n",
      "Suma valor de acciones para acción E : 7.49068481348352\n",
      "Suma valor de acciones para acción W : 5.599286147698561\n",
      "Valor de estado óptimo: 7.49068481348352\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.722871840779841\n",
      "Suma valor de acciones para acción S : 6.329290054550401\n",
      "Suma valor de acciones para acción E : 8.56456699374036\n",
      "Suma valor de acciones para acción W : 6.707070775369801\n",
      "Valor de estado óptimo: 8.56456699374036\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.023340307025921\n",
      "Suma valor de acciones para acción S : 2.4184559668838403\n",
      "Suma valor de acciones para acción E : 3.779024926993921\n",
      "Suma valor de acciones para acción W : 3.779024926993921\n",
      "Valor de estado óptimo: 5.023340307025921\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.660243503307001\n",
      "Suma valor de acciones para acción S : 4.213184077132922\n",
      "Suma valor de acciones para acción E : 0.5128822887472801\n",
      "Suma valor de acciones para acción W : 5.994359370594001\n",
      "Valor de estado óptimo: 6.660243503307001\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 3.5749482237619206\n",
      "Suma valor de acciones para acción S : 2.1950036669721604\n",
      "Suma valor de acciones para acción E : 3.2182336115712005\n",
      "Suma valor de acciones para acción W : 2.2459628972851204\n",
      "Valor de estado óptimo: 3.5749482237619206\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 3.3018934894416008\n",
      "Suma valor de acciones para acción S : 3.3018934894416008\n",
      "Suma valor de acciones para acción E : 4.365761086010882\n",
      "Suma valor de acciones para acción W : 2.3165375062579203\n",
      "Valor de estado óptimo: 4.365761086010882\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.388110703040321\n",
      "Suma valor de acciones para acción S : 4.334633063095682\n",
      "Suma valor de acciones para acción E : 3.440706958582201\n",
      "Suma valor de acciones para acción W : 3.689731141903801\n",
      "Valor de estado óptimo: 5.388110703040321\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.04125272937696023\n",
      "Suma valor de acciones para acción S : 3.1432751224905613\n",
      "Suma valor de acciones para acción E : 2.589775192252801\n",
      "Suma valor de acciones para acción W : 3.915752241041282\n",
      "Valor de estado óptimo: 3.915752241041282\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.255301359283201, (0, 1): 7.49068481348352, (0, 2): 8.56456699374036, (0, 3): 10.0, (1, 0): 5.023340307025921, (1, 2): 6.660243503307001, (1, 3): -1.0, (2, 0): 3.5749482237619206, (2, 1): 4.365761086010882, (2, 2): 5.388110703040321, (2, 3): 3.915752241041282}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'N', (1, 0): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
      "Política aleatoria incial: {(0, 0): 'S', (0, 1): 'S', (0, 2): 'N', (0, 3): 'N', (1, 0): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'W', (2, 2): 'N', (2, 3): 'E'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.7409557342329105\n",
      "Suma valor de acciones para acción S : 4.853943776607669\n",
      "Suma valor de acciones para acción E : 6.408370815675957\n",
      "Suma valor de acciones para acción W : 5.518894728651727\n",
      "Valor de estado óptimo: 6.408370815675957\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.727081217480257\n",
      "Suma valor de acciones para acción S : 6.727081217480257\n",
      "Suma valor de acciones para acción E : 7.514811501920094\n",
      "Suma valor de acciones para acción W : 5.852140245110939\n",
      "Valor de estado óptimo: 7.514811501920094\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.740649868706577\n",
      "Suma valor de acciones para acción S : 6.369536955594558\n",
      "Suma valor de acciones para acción E : 8.570232944734263\n",
      "Suma valor de acciones para acción W : 6.763526010442399\n",
      "Valor de estado óptimo: 8.570232944734263\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.408018233948571\n",
      "Suma valor de acciones para acción S : 3.478163976373249\n",
      "Suma valor de acciones para acción E : 4.501527483532725\n",
      "Suma valor de acciones para acción W : 4.501527483532725\n",
      "Valor de estado óptimo: 5.408018233948571\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.67591015079069\n",
      "Suma valor de acciones para acción S : 4.388861621486662\n",
      "Suma valor de acciones para acción E : 0.5357409927102614\n",
      "Suma valor de acciones para acción W : 6.051116315091303\n",
      "Valor de estado óptimo: 6.67591015079069\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 4.331468858938216\n",
      "Suma valor de acciones para acción S : 3.288626558988135\n",
      "Suma valor de acciones para acción E : 3.917193949698741\n",
      "Suma valor de acciones para acción W : 3.3478086888794887\n",
      "Valor de estado óptimo: 4.331468858938216\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 3.950023285340037\n",
      "Suma valor de acciones para acción S : 3.950023285340037\n",
      "Suma valor de acciones para acción E : 4.66527670167099\n",
      "Suma valor de acciones para acción W : 3.3597997165905418\n",
      "Valor de estado óptimo: 4.66527670167099\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.5407115218157355\n",
      "Suma valor de acciones para acción S : 4.624775905623727\n",
      "Suma valor de acciones para acción E : 3.9036934921209823\n",
      "Suma valor de acciones para acción W : 4.2276998604990945\n",
      "Valor de estado óptimo: 5.5407115218157355\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.11734766496734428\n",
      "Suma valor de acciones para acción S : 3.6566892785170677\n",
      "Suma valor de acciones para acción E : 3.081759315243439\n",
      "Suma valor de acciones para acción W : 4.141857407882747\n",
      "Valor de estado óptimo: 4.141857407882747\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.408370815675957, (0, 1): 7.514811501920094, (0, 2): 8.570232944734263, (0, 3): 10.0, (1, 0): 5.408018233948571, (1, 2): 6.67591015079069, (1, 3): -1.0, (2, 0): 4.331468858938216, (2, 1): 4.66527670167099, (2, 2): 5.5407115218157355, (2, 3): 4.141857407882747}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'N', (1, 0): 'N', (1, 2): 'N', (1, 3): 'S', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aleatoria incial: {(0, 0): 'N', (0, 1): 'N', (0, 2): 'N', (0, 3): 'S', (1, 0): 'S', (1, 2): 'N', (1, 3): 'W', (2, 0): 'S', (2, 1): 'N', (2, 2): 'N', (2, 3): 'W'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.867113395870334\n",
      "Suma valor de acciones para acción S : 5.146859537026616\n",
      "Suma valor de acciones para acción E : 6.474139295848676\n",
      "Suma valor de acciones para acción W : 5.677502001752897\n",
      "Valor de estado óptimo: 6.474139295848676\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.758738619819388\n",
      "Suma valor de acciones para acción S : 6.758738619819388\n",
      "Suma valor de acciones para acción E : 7.523233790554286\n",
      "Suma valor de acciones para acción W : 5.966693057632305\n",
      "Valor de estado óptimo: 7.523233790554286\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.746900755381478\n",
      "Suma valor de acciones para acción S : 6.382988343742106\n",
      "Suma valor de acciones para acción E : 8.572152878597247\n",
      "Suma valor de acciones para acción W : 6.782817159979714\n",
      "Valor de estado óptimo: 8.572152878597247\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.587470269397432\n",
      "Suma valor de acciones para acción S : 4.0921008605462585\n",
      "Suma valor de acciones para acción E : 4.8603586991582475\n",
      "Suma valor de acciones para acción W : 4.8603586991582475\n",
      "Valor de estado óptimo: 5.587470269397432\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.681399633779832\n",
      "Suma valor de acciones para acción S : 4.500144209278492\n",
      "Suma valor de acciones para acción E : 0.5499850019894998\n",
      "Suma valor de acciones para acción W : 6.076640310558797\n",
      "Valor de estado óptimo: 6.681399633779832\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 4.7034802288978\n",
      "Suma valor de acciones para acción S : 3.928364678890344\n",
      "Suma valor de acciones para acción E : 4.235553063562924\n",
      "Suma valor de acciones para acción W : 3.9952114167953265\n",
      "Valor de estado óptimo: 4.7034802288978\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 4.247495459470969\n",
      "Suma valor de acciones para acción S : 4.247495459470969\n",
      "Suma valor de acciones para acción E : 4.829062102008108\n",
      "Suma valor de acciones para acción W : 3.9584073847362937\n",
      "Valor de estado óptimo: 4.829062102008108\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.599297378429134\n",
      "Suma valor de acciones para acción S : 4.781954365567167\n",
      "Suma valor de acciones para acción E : 4.081633284210157\n",
      "Suma valor de acciones para acción W : 4.458495175737692\n",
      "Valor de estado óptimo: 5.599297378429134\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.1514312036728634\n",
      "Suma valor de acciones para acción S : 3.8535685373484414\n",
      "Suma valor de acciones para acción E : 3.2649045003850254\n",
      "Suma valor de acciones para acción W : 4.2720794624167775\n",
      "Valor de estado óptimo: 4.2720794624167775\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.474139295848676, (0, 1): 7.523233790554286, (0, 2): 8.572152878597247, (0, 3): 10.0, (1, 0): 5.587470269397432, (1, 2): 6.681399633779832, (1, 3): -1.0, (2, 0): 4.7034802288978, (2, 1): 4.829062102008108, (2, 2): 5.599297378429134, (2, 3): 4.2720794624167775}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'S', (1, 0): 'N', (1, 2): 'N', (1, 3): 'W', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
      "Política aleatoria incial: {(0, 0): 'N', (0, 1): 'E', (0, 2): 'N', (0, 3): 'S', (1, 0): 'N', (1, 2): 'E', (1, 3): 'E', (2, 0): 'W', (2, 1): 'S', (2, 2): 'N', (2, 3): 'W'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.921143870787315\n",
      "Suma valor de acciones para acción S : 5.282742171742418\n",
      "Suma valor de acciones para acción E : 6.502273190071237\n",
      "Suma valor de acciones para acción W : 5.746925153883198\n",
      "Valor de estado óptimo: 6.502273190071237\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.770894624899221\n",
      "Suma valor de acciones para acción S : 6.770894624899221\n",
      "Suma valor de acciones para acción E : 7.52613215488979\n",
      "Suma valor de acciones para acción W : 6.01556237531082\n",
      "Valor de estado óptimo: 7.52613215488979\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.749041113739905\n",
      "Suma valor de acciones para acción S : 6.387698777471366\n",
      "Suma valor de acciones para acción E : 8.572819726113938\n",
      "Suma valor de acciones para acción W : 6.789548055313024\n",
      "Valor de estado óptimo: 8.572819726113938\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.6671249415025855\n",
      "Suma valor de acciones para acción S : 4.392250413297954\n",
      "Suma valor de acciones para acción E : 5.028964351193334\n",
      "Suma valor de acciones para acción W : 5.028964351193334\n",
      "Valor de estado óptimo: 5.6671249415025855\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.683276039630203\n",
      "Suma valor de acciones para acción S : 4.542820079509162\n",
      "Suma valor de acciones para acción E : 0.5554305231323743\n",
      "Suma valor de acciones para acción W : 6.086038259453854\n",
      "Valor de estado óptimo: 6.683276039630203\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 4.880907403747683\n",
      "Suma valor de acciones para acción S : 4.244434574587948\n",
      "Suma valor de acciones para acción E : 4.4031102582924095\n",
      "Suma valor de acciones para acción W : 4.312691309652987\n",
      "Valor de estado óptimo: 4.880907403747683\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 4.404174698105263\n",
      "Suma valor de acciones para acción S : 4.404174698105263\n",
      "Suma valor de acciones para acción E : 4.900725290830437\n",
      "Suma valor de acciones para acción W : 4.255736943167876\n",
      "Valor de estado óptimo: 4.900725290830437\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.629710477119719\n",
      "Suma valor de acciones para acción S : 4.850596853267216\n",
      "Suma valor de acciones para acción E : 4.181159944038887\n",
      "Suma valor de acciones para acción W : 4.582187444544646\n",
      "Valor de estado óptimo: 5.629710477119719\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.16842391567613202\n",
      "Suma valor de acciones para acción S : 3.9643211286162123\n",
      "Suma valor de acciones para acción E : 3.3703843645575904\n",
      "Suma valor de acciones para acción W : 4.3259812640864865\n",
      "Valor de estado óptimo: 4.3259812640864865\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.502273190071237, (0, 1): 7.52613215488979, (0, 2): 8.572819726113938, (0, 3): 10.0, (1, 0): 5.6671249415025855, (1, 2): 6.683276039630203, (1, 3): -1.0, (2, 0): 4.880907403747683, (2, 1): 4.900725290830437, (2, 2): 5.629710477119719, (2, 3): 4.3259812640864865}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'S', (1, 0): 'N', (1, 2): 'N', (1, 3): 'E', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
      "Política aleatoria incial: {(0, 0): 'N', (0, 1): 'N', (0, 2): 'W', (0, 3): 'S', (1, 0): 'E', (1, 2): 'E', (1, 3): 'W', (2, 0): 'S', (2, 1): 'S', (2, 2): 'N', (2, 3): 'S'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.944193177897783\n",
      "Suma valor de acciones para acción S : 5.342886438928354\n",
      "Suma valor de acciones para acción E : 6.5140609833622936\n",
      "Suma valor de acciones para acción W : 5.776882528692935\n",
      "Valor de estado óptimo: 6.5140609833622936\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.775573513977315\n",
      "Suma valor de acciones para acción S : 6.775573513977315\n",
      "Suma valor de acciones para acción E : 7.527133990682198\n",
      "Suma valor de acciones para acción W : 6.036340484731452\n",
      "Valor de estado óptimo: 7.527133990682198\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.749782096742117\n",
      "Suma valor de acciones para acción S : 6.389310642473828\n",
      "Suma valor de acciones para acción E : 8.573048618916973\n",
      "Suma valor de acciones para acción W : 6.791863770437622\n",
      "Valor de estado óptimo: 8.573048618916973\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.701719186321755\n",
      "Suma valor de acciones para acción S : 4.534335820168797\n",
      "Suma valor de acciones para acción E : 5.104816211325565\n",
      "Suma valor de acciones para acción W : 5.104816211325565\n",
      "Valor de estado óptimo: 5.701719186321755\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.6839250463687545\n",
      "Suma valor de acciones para acción S : 4.564886387092917\n",
      "Suma valor de acciones para acción E : 0.5582277182910292\n",
      "Suma valor de acciones para acción W : 6.090186466824775\n",
      "Valor de estado óptimo: 6.6839250463687545\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 4.960676900393892\n",
      "Suma valor de acciones para acción S : 4.394600273210362\n",
      "Suma valor de acciones para acción E : 4.477845120470438\n",
      "Suma valor de acciones para acción W : 4.463576241770856\n",
      "Valor de estado óptimo: 4.960676900393892\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 4.474477818675981\n",
      "Suma valor de acciones para acción S : 4.474477818675981\n",
      "Suma valor de acciones para acción E : 4.935522095875677\n",
      "Suma valor de acciones para acción W : 4.39638388304781\n",
      "Valor de estado óptimo: 4.935522095875677\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.64236233847627\n",
      "Suma valor de acciones para acción S : 4.8837951334687215\n",
      "Suma valor de acciones para acción E : 4.222875296649764\n",
      "Suma valor de acciones para acción W : 4.636690995905408\n",
      "Valor de estado óptimo: 5.64236233847627\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.17601225670855858\n",
      "Suma valor de acciones para acción S : 4.010718766850829\n",
      "Suma valor de acciones para acción E : 3.4140448239100545\n",
      "Suma valor de acciones para acción W : 4.3527298572939825\n",
      "Valor de estado óptimo: 4.3527298572939825\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.5140609833622936, (0, 1): 7.527133990682198, (0, 2): 8.573048618916973, (0, 3): 10.0, (1, 0): 5.701719186321755, (1, 2): 6.6839250463687545, (1, 3): -1.0, (2, 0): 4.960676900393892, (2, 1): 4.935522095875677, (2, 2): 5.64236233847627, (2, 3): 4.3527298572939825}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'S', (1, 0): 'N', (1, 2): 'N', (1, 3): 'W', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aleatoria incial: {(0, 0): 'S', (0, 1): 'E', (0, 2): 'E', (0, 3): 'N', (1, 0): 'W', (1, 2): 'S', (1, 3): 'N', (2, 0): 'W', (2, 1): 'W', (2, 2): 'E', (2, 3): 'E'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.953831455684855\n",
      "Suma valor de acciones para acción S : 5.368945361815668\n",
      "Suma valor de acciones para acción E : 6.5189566885627475\n",
      "Suma valor de acciones para acción W : 5.789544123292416\n",
      "Valor de estado óptimo: 6.5189566885627475\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.777376337496317\n",
      "Suma valor de acciones para acción S : 6.777376337496317\n",
      "Suma valor de acciones para acción E : 7.527479123943016\n",
      "Suma valor de acciones para acción W : 6.0450080263436465\n",
      "Valor de estado óptimo: 7.527479123943016\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.750037064781619\n",
      "Suma valor de acciones para acción S : 6.389868092546902\n",
      "Suma valor de acciones para acción E : 8.573127629875716\n",
      "Suma valor de acciones para acción W : 6.792664103166899\n",
      "Valor de estado óptimo: 8.573127629875716\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.716433361558767\n",
      "Suma valor de acciones para acción S : 4.597996821821519\n",
      "Suma valor de acciones para acción E : 5.137964223689721\n",
      "Suma valor de acciones para acción W : 5.137964223689721\n",
      "Valor de estado óptimo: 5.716433361558767\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.684148259793409\n",
      "Suma valor de acciones para acción S : 4.574054137876103\n",
      "Suma valor de acciones para acción E : 0.5593869861653918\n",
      "Suma valor de acciones para acción W : 6.091813019550896\n",
      "Valor de estado óptimo: 6.684148259793409\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 4.995895723815925\n",
      "Suma valor de acciones para acción S : 4.462345277947864\n",
      "Suma valor de acciones para acción E : 4.5131915568348955\n",
      "Suma valor de acciones para acción W : 4.531303016088011\n",
      "Valor de estado óptimo: 4.995895723815925\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 4.507849440528802\n",
      "Suma valor de acciones para acción S : 4.507849440528802\n",
      "Suma valor de acciones para acción E : 4.950894860960536\n",
      "Suma valor de acciones para acción W : 4.460081345541224\n",
      "Valor de estado óptimo: 4.950894860960536\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.648368709170772\n",
      "Suma valor de acciones para acción S : 4.898443559488183\n",
      "Suma valor de acciones para acción E : 4.24333136188772\n",
      "Suma valor de acciones para acción W : 4.66294177366654\n",
      "Valor de estado óptimo: 5.648368709170772\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.17955829761932268\n",
      "Suma valor de acciones para acción S : 4.033523794870991\n",
      "Suma valor de acciones para acción E : 3.4357111844081265\n",
      "Suma valor de acciones para acción W : 4.364246570859373\n",
      "Valor de estado óptimo: 4.364246570859373\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.5189566885627475, (0, 1): 7.527479123943016, (0, 2): 8.573127629875716, (0, 3): 10.0, (1, 0): 5.716433361558767, (1, 2): 6.684148259793409, (1, 3): -1.0, (2, 0): 4.995895723815925, (2, 1): 4.950894860960536, (2, 2): 5.648368709170772, (2, 3): 4.364246570859373}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'N', (1, 0): 'N', (1, 2): 'N', (1, 3): 'N', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
      "Política aleatoria incial: {(0, 0): 'W', (0, 1): 'E', (0, 2): 'S', (0, 3): 'N', (1, 0): 'W', (1, 2): 'N', (1, 3): 'E', (2, 0): 'N', (2, 1): 'E', (2, 2): 'S', (2, 3): 'S'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.957828038890697\n",
      "Suma valor de acciones para acción S : 5.380011243447831\n",
      "Suma valor de acciones para acción E : 6.520970073749909\n",
      "Suma valor de acciones para acción W : 5.794833920276115\n",
      "Valor de estado óptimo: 6.520970073749909\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.778072557898434\n",
      "Suma valor de acciones para acción S : 6.778072557898434\n",
      "Suma valor de acciones para acción E : 7.527598135820259\n",
      "Suma valor de acciones para acción W : 6.0485950580749215\n",
      "Valor de estado óptimo: 7.527598135820259\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.750125014665388\n",
      "Suma valor de acciones para acción S : 6.3900598682061265\n",
      "Suma valor de acciones para acción E : 8.573154830070221\n",
      "Suma valor de acciones para acción W : 6.792939799309194\n",
      "Valor de estado óptimo: 8.573154830070221\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.7226068208457574\n",
      "Suma valor de acciones para acción S : 4.626002926228045\n",
      "Suma valor de acciones para acción E : 5.152168737436393\n",
      "Suma valor de acciones para acción W : 5.152168737436393\n",
      "Valor de estado óptimo: 5.7226068208457574\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.684225236891923\n",
      "Suma valor de acciones para acción S : 4.578398813984363\n",
      "Suma valor de acciones para acción E : 0.5599346705141839\n",
      "Suma valor de acciones para acción W : 6.092521417565439\n",
      "Valor de estado óptimo: 6.684225236891923\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 5.011043172952194\n",
      "Suma valor de acciones para acción S : 4.492256073777348\n",
      "Suma valor de acciones para acción E : 4.528753917575308\n",
      "Suma valor de acciones para acción W : 4.561154538831189\n",
      "Valor de estado óptimo: 5.011043172952194\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 4.522628098860388\n",
      "Suma valor de acciones para acción S : 4.522628098860388\n",
      "Suma valor de acciones para acción E : 4.957986545575852\n",
      "Suma valor de acciones para acción W : 4.488205996120363\n",
      "Valor de estado óptimo: 4.957986545575852\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.650949475915047\n",
      "Suma valor de acciones para acción S : 4.905188199466748\n",
      "Suma valor de acciones para acción E : 4.252184058225525\n",
      "Suma valor de acciones para acción W : 4.674570827098362\n",
      "Valor de estado óptimo: 5.650949475915047\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.181135375202713\n",
      "Suma valor de acciones para acción S : 4.043392906221461\n",
      "Suma valor de acciones para acción E : 3.4450397223960922\n",
      "Suma valor de acciones para acción W : 4.3696076619803\n",
      "Valor de estado óptimo: 4.3696076619803\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.520970073749909, (0, 1): 7.527598135820259, (0, 2): 8.573154830070221, (0, 3): 10.0, (1, 0): 5.7226068208457574, (1, 2): 6.684225236891923, (1, 3): -1.0, (2, 0): 5.011043172952194, (2, 1): 4.957986545575852, (2, 2): 5.650949475915047, (2, 3): 4.3696076619803}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'N', (1, 0): 'N', (1, 2): 'N', (1, 3): 'E', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
      "Política aleatoria incial: {(0, 0): 'W', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'S', (1, 2): 'E', (1, 3): 'W', (2, 0): 'W', (2, 1): 'N', (2, 2): 'N', (2, 3): 'S'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.95946959196125\n",
      "Suma valor de acciones para acción S : 5.384648049870261\n",
      "Suma valor de acciones para acción E : 6.521792578304197\n",
      "Suma valor de acciones para acción W : 5.7970203736135435\n",
      "Valor de estado óptimo: 6.521792578304197\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.778341899134398\n",
      "Suma valor de acciones para acción S : 6.778341899134398\n",
      "Suma valor de acciones para acción E : 7.527639142098208\n",
      "Suma valor de acciones para acción W : 6.050066117547582\n",
      "Valor de estado óptimo: 7.527639142098208\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.750155309874384\n",
      "Suma valor de acciones para acción S : 6.390126002786009\n",
      "Suma valor de acciones para acción E : 8.573164206026593\n",
      "Suma valor de acciones para acción W : 6.79303486381718\n",
      "Valor de estado óptimo: 8.573164206026593\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.72516768085217\n",
      "Suma valor de acciones para acción S : 4.638020312277816\n",
      "Suma valor de acciones para acción E : 5.158158103212136\n",
      "Suma valor de acciones para acción W : 5.158158103212136\n",
      "Valor de estado óptimo: 5.72516768085217\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.6842517489708335\n",
      "Suma valor de acciones para acción S : 4.580263893979107\n",
      "Suma valor de acciones para acción E : 0.5601693875386742\n",
      "Suma valor de acciones para acción W : 6.092811558100859\n",
      "Valor de estado óptimo: 6.6842517489708335\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 5.01748958567647\n",
      "Suma valor de acciones para acción S : 4.505163759193104\n",
      "Suma valor de acciones para acción E : 4.53577881225643\n",
      "Suma valor de acciones para acción W : 4.573979583967396\n",
      "Valor de estado óptimo: 5.01748958567647\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 4.529329651212666\n",
      "Suma valor de acciones para acción S : 4.529329651212666\n",
      "Suma valor de acciones para acción E : 4.961121200862487\n",
      "Suma valor de acciones para acción W : 4.500388662729233\n",
      "Valor de estado óptimo: 4.961121200862487\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.6521256492422385\n",
      "Suma valor de acciones para acción S : 4.908167101338887\n",
      "Suma valor de acciones para acción E : 4.256283240778443\n",
      "Suma valor de acciones para acción W : 4.679916036967241\n",
      "Valor de estado óptimo: 5.6521256492422385\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.18185014241058117\n",
      "Suma valor de acciones para acción S : 4.047967659036397\n",
      "Suma valor de acciones para acción E : 3.449382206204043\n",
      "Suma valor de acciones para acción W : 4.371948312237061\n",
      "Valor de estado óptimo: 4.371948312237061\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.521792578304197, (0, 1): 7.527639142098208, (0, 2): 8.573164206026593, (0, 3): 10.0, (1, 0): 5.72516768085217, (1, 2): 6.6842517489708335, (1, 3): -1.0, (2, 0): 5.01748958567647, (2, 1): 4.961121200862487, (2, 2): 5.6521256492422385, (2, 3): 4.371948312237061}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'N', (1, 2): 'N', (1, 3): 'W', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
      "Política aleatoria incial: {(0, 0): 'E', (0, 1): 'S', (0, 2): 'S', (0, 3): 'S', (1, 0): 'W', (1, 2): 'W', (1, 3): 'W', (2, 0): 'N', (2, 1): 'W', (2, 2): 'W', (2, 3): 'N'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 5.960139511215239\n",
      "Suma valor de acciones para acción S : 5.386569585049779\n",
      "Suma valor de acciones para acción E : 6.522126605634782\n",
      "Suma valor de acciones para acción W : 5.797917079703095\n",
      "Valor de estado óptimo: 6.522126605634782\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 6.7784462929004805\n",
      "Suma valor de acciones para acción S : 6.7784462929004805\n",
      "Suma valor de acciones para acción E : 7.527653273916826\n",
      "Suma valor de acciones para acción W : 6.0506657019567\n",
      "Valor de estado óptimo: 7.527653273916826\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 7.750165751127987\n",
      "Suma valor de acciones para acción S : 6.39014878204784\n",
      "Suma valor de acciones para acción E : 8.573167435949768\n",
      "Suma valor de acciones para acción W : 6.793067618260478\n",
      "Valor de estado óptimo: 8.573167435949768\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 10.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 5.726220838932412\n",
      "Suma valor de acciones para acción S : 4.643122684240449\n",
      "Suma valor de acciones para acción E : 5.160656124971823\n",
      "Suma valor de acciones para acción W : 5.160656124971823\n",
      "Valor de estado óptimo: 5.726220838932412\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 6.684260885746523\n",
      "Suma valor de acciones para acción S : 4.581113124861788\n",
      "Suma valor de acciones para acción E : 0.560276086974195\n",
      "Suma valor de acciones para acción W : 6.092937346233196\n",
      "Valor de estado óptimo: 6.684260885746523\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 5.020195701002069\n",
      "Suma valor de acciones para acción S : 4.510667472475566\n",
      "Suma valor de acciones para acción E : 4.538846418608569\n",
      "Suma valor de acciones para acción W : 4.579431655674637\n",
      "Valor de estado óptimo: 5.020195701002069\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 4.532272635763675\n",
      "Suma valor de acciones para acción S : 4.532272635763675\n",
      "Suma valor de acciones para acción E : 4.962532283609661\n",
      "Suma valor de acciones para acción W : 4.5055943178423075\n",
      "Valor de estado óptimo: 4.962532283609661\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 5.65263751543796\n",
      "Suma valor de acciones para acción S : 4.909506723633372\n",
      "Suma valor de acciones para acción E : 4.2580767506498605\n",
      "Suma valor de acciones para acción W : 4.682281230460168\n",
      "Valor de estado óptimo: 5.65263751543796\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : 0.18216665653313696\n",
      "Suma valor de acciones para acción S : 4.049969441343821\n",
      "Suma valor de acciones para acción E : 3.4512781329120195\n",
      "Suma valor de acciones para acción W : 4.373005815555748\n",
      "Valor de estado óptimo: 4.373005815555748\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 6.522126605634782, (0, 1): 7.527653273916826, (0, 2): 8.573167435949768, (0, 3): 10.0, (1, 0): 5.726220838932412, (1, 2): 6.684260885746523, (1, 3): -1.0, (2, 0): 5.020195701002069, (2, 1): 4.962532283609661, (2, 2): 5.65263751543796, (2, 3): 4.373005815555748}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'S', (1, 0): 'N', (1, 2): 'N', (1, 3): 'W', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gw\u001b[38;5;241m.\u001b[39mvalue_iteration \u001b[38;5;241m=\u001b[39m MethodType(value_iteration, gw)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_value_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\WORKSPACE\\RL-IELE\\Taller 2 Programación Dinámica\\tutorial\\gridworld.py:241\u001b[0m, in \u001b[0;36mGridWorld.solve_value_iteration\u001b[1;34m(self, gamma, horizon, init_state)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m escape:\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m pygame\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[1;32m~\\WORKSPACE\\RL-IELE\\Taller 2 Programación Dinámica\\tutorial\\gridworld.py:467\u001b[0m, in \u001b[0;36mGridWorld.follow_policy\u001b[1;34m(self, init_state, flag_q, gamma)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_draw_q_values()\n\u001b[1;32m--> 467\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr_draw_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_draw_reward(reward, utility, done)\n\u001b[0;32m    469\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n",
      "File \u001b[1;32m~\\WORKSPACE\\RL-IELE\\Taller 2 Programación Dinámica\\tutorial\\gridworld.py:662\u001b[0m, in \u001b[0;36mGridWorld.r_draw_agent\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mr_draw_agent\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(state\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerminal Diamante\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (state\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerminal Bomba\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 662\u001b[0m         row \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    663\u001b[0m         col \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    664\u001b[0m         center \u001b[38;5;241m=\u001b[39m [MARGIN\u001b[38;5;241m+\u001b[39mcol\u001b[38;5;241m*\u001b[39mCELL_SIZE\u001b[38;5;241m+\u001b[39mCELL_SIZE\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, MARGIN\u001b[38;5;241m+\u001b[39mrow\u001b[38;5;241m*\u001b[39mCELL_SIZE\u001b[38;5;241m+\u001b[39mCELL_SIZE\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=init_state )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mError en la función ´value_iteration´\n",
      "name 'random' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gw.value_iteration = MethodType(value_iteration2, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=init_state )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Para un horizonte de 15, los valores de los estados son: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de estado finales: \n",
      "{(0, 0): 6.522126605634782, (0, 1): 7.527653273916826, (0, 2): 8.573167435949768, (0, 3): 10.0, (1, 0): 5.726220838932412, (1, 2): 6.684260885746523, (1, 3): -1.0, (2, 0): 5.020195701002069, (2, 1): 4.962532283609661, (2, 2): 5.65263751543796, (2, 3): 4.373005815555748}\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(\"Valores de estado finales: \")\n",
    "print(gw.state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "V(0, 0) = 0.645\n",
    "V(0, 1) = 0.744\n",
    "V(0, 2) = 0.848\n",
    "V(0, 3) = 1.000\n",
    "V(1, 0) = 0.566\n",
    "V(1, 2) = 0.572\n",
    "V(1, 3) = -1.000\n",
    "V(2, 0) = 0.491\n",
    "V(2, 1) = 0.431\n",
    "V(2, 2) = 0.475\n",
    "V(2, 3) = 0.277\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La política aprendida es para maximizar la recompensa desde el estado inicial (2,0) es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aprendida: \n",
      "{(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'S', (1, 0): 'N', (1, 2): 'N', (1, 3): 'W', (2, 0): 'N', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(\"Política aprendida: \")\n",
    "print(gw.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "Acción a tomar en (0, 0): E\n",
    "Acción a tomar en (0, 1): E\n",
    "Acción a tomar en (0, 2): E\n",
    "Acción a tomar en (0, 3): N\n",
    "Acción a tomar en (1, 0): N\n",
    "Acción a tomar en (1, 2): N\n",
    "Acción a tomar en (1, 3): N\n",
    "Acción a tomar en (2, 0): N\n",
    "Acción a tomar en (2, 1): W\n",
    "Acción a tomar en (2, 2): N\n",
    "Acción a tomar en (2, 3): W\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Es otro método que busca aproximar los valores óptimos de cada estado basados en la política actual $\\pi(s)$. La politica puede mejorarse con el tiempo, de acuerdo a los valores Q de cada par estado-acción.\n",
    "\n",
    "### Valores Q\n",
    "$Q^{*}(s,a)$ es el valor esperado de la utilidad, si el agente comenzara en el estado $s$, tomando la acción $a$ y comportandose óptimamente en adelante.\n",
    "\n",
    "Los valores Q pueden aproximarse así:\n",
    "$$Q_{k+1}^{*}(s,a)\\leftarrow \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma \\max_{a'}Q_{k}^{*}(s',a'))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "from colorama import Fore\n",
    "\n",
    "gw = GridWorld(3, 4, [(1,1)], [(1,3)], [(0,3)], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se inicializan los valores de los estados en 0.0, al igual que los valores Q para todo par estado acción. Además, inicializamos una política que por defecto siempre toma el norte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.0,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 2): 0.0,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): 0.0,\n",
       " (2, 1): 0.0,\n",
       " (2, 2): 0.0,\n",
       " (2, 3): 0.0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar valores y política\n",
    "import random\n",
    "gw.state_values = gw.init_values()\n",
    "gw.state_q_values = gw.init_qvalues()\n",
    "\n",
    "# definir valor de estados terminales\n",
    "gw.state_values[(0,3)] = 1.0\n",
    "gw.state_values[(1,3)] = -1.0  \n",
    "\n",
    "gw.policy = dict.fromkeys(gw.states, 'N') # cambiar política\n",
    "gw.updateGrid()\n",
    "\n",
    "gw.state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique la cantidad de valores q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (0, 1): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (0, 2): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (0, 3): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (1, 0): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (1, 2): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (1, 3): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (2, 0): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (2, 1): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (2, 2): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0},\n",
       " (2, 3): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "gw.state_q_values\n",
    "#type(gw.state_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "\n",
    "```\n",
    "num_q_values = 44\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    "\n",
    "> Inicializar $\\pi_{0}(s)$ de forma arbitraria<br>\n",
    "> Inicializar $V_{0}(s)$ y $Q_{0}(s,a)$ en 0.0 para todos los estados y acciones<br>\n",
    "> policy_stable = False<br>\n",
    "> Mientras(not policy_stable):<br>\n",
    ">> 1. Evaluar la política<br>\n",
    ">> Para $k=1,\\cdots,H$:<br>\n",
    ">>> Para todos los estados $s$:<br>\n",
    ">>>> Para todas las acciones $s$ permitidas en $s$:<br>\n",
    ">>>>> $$Q_{k}(s,a)=\\sum_{s'} P(s'|s,\\pi(s))(R(s,a,s')+\\gamma Q_{k-1}(s',\\pi(s')))$$<br>\n",
    ">>>> Valor del estado $s$ corresponde al valor Q de la acción a ejecutar según la política $\\pi_k(s)$ <br>\n",
    ">>>> $V_{k}(s)=Q_{k}(s,\\pi_{k}(s))$ <br>\n",
    ">\n",
    ">> 2. Mejorar la política <br>\n",
    ">> policy_stable = True <br>\n",
    ">> Para todos los estados $s$:<br>\n",
    ">>> best_action = $arg\\max_{a}Q_k(s,)$<br>\n",
    ">>> si *best_action* es diferente a lo que dice $\\pi_{k}(s)$:<br>\n",
    ">>>> $\\pi_{k}(s) = best\\_action$<br>\n",
    ">>>> policy_stable = False<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente la función de *policy evaluation* en donde se actualizan los valores $Q(s,a)$ y $V(s)$ de acuerdo a la política:\n",
    "\n",
    "Las siguientes funciones pueden ser útiles:\n",
    "\n",
    "* `gw.get_allowed_actions(state)`: Permite conocer el espacio de acciones.\n",
    "* `gw.real_actions[action]`: Las posibles acciones (reales) que puede tomar el agente.\n",
    "* `gw.action_probabilities[action]`: La probabilidad de que el agente tomé las acciones reales.\n",
    "* `gw.step(state, action, random)`: Permite ejecutar un paso en el gridworld. Con este método se puede conocer el estado siguiente, la recompensa, la accion tomada y la bandera de un estado terminal. Nota: si el parámetro random es Falso, las acciones son determinisiticas (con probabilidad 1, toma la acción dada).\n",
    "* `gw.state_values[state]`: Los valores del estado *state*.\n",
    "* `gw.policy[state]`: La acción dada por la política para el estado *state*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw.get_allowed_actions((0,0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "\n",
    "# policy_evaluation: encontrar valores de estado a través de la función de valor par estado-acción óptimos y política óptima para cada estado \n",
    "# params: gw=MDP gridworld, gamma=descuento\n",
    "\n",
    "def policy_evaluation(gw,gamma):   \n",
    "    \n",
    "    # valores de estado, para cada estado inicializar en 0 \n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    # nuevos valores de estado, para cada estado inicializar en 0\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)    \n",
    "    \n",
    "    # valores de par estado-accion, para cada estado inicializar en 0\n",
    "    state_q_values = gw.init_qvalues()\n",
    "    # nuevos valores de par estado-accion, para cada estado inicializar en 0\n",
    "    new_state_q_values = gw.init_qvalues()\n",
    "    \n",
    "    # politica aleatoria \n",
    "    # inicializar politica aleatoria \n",
    "    gw.policy = {}\n",
    "    keys = gw.states # keys del diccionario son los estados posibles del MDP\n",
    "    \n",
    "    # iterar sobre estados posibles del MDP\n",
    "    for state in keys:    \n",
    "        # inicializar valores de política aleatoria escogiendo acción aleatoria en estado \n",
    "        gw.policy[state] = random.choice(gw.get_allowed_actions(state) )\n",
    "        \n",
    "        \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:          \n",
    "        #print(\"Estado actual\")\n",
    "        # Dar un paso en par estado-accion con accion dada por la política\n",
    "        new_state, reward, _, done = gw.step(state, gw.policy[(state)], random=True)\n",
    "        \n",
    "        # actualización para un estado terminal\n",
    "        if(done==True): # llega a estado terminal             \n",
    "            new_values[state] = reward # valor de estado para estado terminal es recompensa\n",
    "            \n",
    "            #gw.state_values[state] = reward\n",
    "            print(\"Estado terminal: \"+str(state) )\n",
    "            print(\"Recompensa estado terminal: \"+str(reward) )\n",
    "            \n",
    "        else: # llega a estado no terminal\n",
    "            print(\"Estado actual: \"+str(state) )\n",
    "            # actualización para un estado no terminal          \n",
    "            \n",
    "            # iterar sobre acciones permitidas en estado state\n",
    "            for action in gw.get_allowed_actions(state):\n",
    "                \n",
    "                suma_actions = 0.0 # suma de valores de par estado-accion real que puede tomar (en un estado)\n",
    "                \n",
    "                # iterar sobre las posibles acciones (reales) que puede tomar el agente dada una acción\n",
    "                for real_action in  gw.real_actions[action]:\n",
    "                    # si acción real es igual a la acción que toma: probabilidad 0.8\n",
    "                    if real_action==action: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities[0]*(reward+gamma*gw.state_q_values[new_state][real_action]) #probabilidad 0.8\n",
    "\n",
    "                    # si acción real es diferente a la acción que toma:  probabilidad 0.1\n",
    "                    else: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities[1]*(reward+gamma*gw.state_q_values[new_state][real_action]) #probabilidad 0.1\n",
    "                \n",
    "                \n",
    "                print(\"Q(s,a) en acción: \"+str(action) )\n",
    "                print(suma_actions)\n",
    "                # la suma de valor de par estado-accion para todas las acciones posibles, dada una acción es el nuevo valor de par estado-accion\n",
    "                new_state_q_values[state][action] = suma_actions\n",
    "                \n",
    "            new_values[state] = new_state_q_values[state][gw.policy[state]]  \n",
    "            #gw.state_values[state] =  gw.max_val(gw.state_q_values[state])\n",
    "            \n",
    "            print(\"Nuevo valor de estado V(s): \"+ str(new_values[state]) )\n",
    "            print(\"\")\n",
    "            \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values) \n",
    "    \n",
    "    state_q_values = copy.deepcopy(new_state_q_values) \n",
    "    gw.state_q_values = copy.deepcopy(new_state_q_values) \n",
    "    policy_improvement(gw)\n",
    "    \n",
    "    \n",
    "policy_evaluation(gw,0.9)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "V(0, 0) = 0.951\n",
    "V(0, 1) = 0.964\n",
    "V(0, 2) = 0.977\n",
    "V(0, 3) = 1.000\n",
    "V(1, 0) = 0.939\n",
    "V(1, 2) = 0.890\n",
    "V(1, 3) = -1.000\n",
    "V(2, 0) = 0.925\n",
    "V(2, 1) = 0.913\n",
    "V(2, 2) = 0.900\n",
    "V(2, 3) = 0.790\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aleatoria incial: {(0, 0): 'N', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'N', (1, 2): 'N', (1, 3): 'N', (2, 0): 'E', (2, 1): 'N', (2, 2): 'S', (2, 3): 'E'}\n",
      "Estado actual: (0, 0)\n",
      "Suma valor de acciones para acción N : 0.5894191140883287\n",
      "Suma valor de acciones para acción S : 0.5327872992189849\n",
      "Suma valor de acciones para acción E : 0.644969148704666\n",
      "Suma valor de acciones para acción W : 0.5733931355903742\n",
      "Valor de estado óptimo: 0.644969148704666\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Suma valor de acciones para acción N : 0.6702998792691713\n",
      "Suma valor de acciones para acción S : 0.6702998792691713\n",
      "Suma valor de acciones para acción E : 0.7443801453743135\n",
      "Suma valor de acciones para acción W : 0.5983661157276152\n",
      "Valor de estado óptimo: 0.7443801453743135\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Suma valor de acciones para acción N : 0.7673859324901228\n",
      "Suma valor de acciones para acción S : 0.5687327151749094\n",
      "Suma valor de acciones para acción E : 0.8477662777370815\n",
      "Suma valor de acciones para acción W : 0.663719980810607\n",
      "Valor de estado óptimo: 0.8477662777370815\n",
      "Acción óptima en estado: E\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Suma valor de acciones para acción N : 0.5663141587317062\n",
      "Suma valor de acciones para acción S : 0.45522436798365595\n",
      "Suma valor de acciones para acción E : 0.5099540737361922\n",
      "Suma valor de acciones para acción W : 0.5099540737361922\n",
      "Valor de estado óptimo: 0.5663141587317062\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Suma valor de acciones para acción N : 0.571859032392272\n",
      "Suma valor de acciones para acción S : 0.3038022318646796\n",
      "Suma valor de acciones para acción E : -0.6009091701644661\n",
      "Suma valor de acciones para acción W : 0.5308293321262527\n",
      "Valor de estado óptimo: 0.571859032392272\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Suma valor de acciones para acción N : 0.4906812517911425\n",
      "Suma valor de acciones para acción S : 0.4362232759124359\n",
      "Suma valor de acciones para acción E : 0.405324336186471\n",
      "Suma valor de acciones para acción W : 0.4484171209988176\n",
      "Valor de estado óptimo: 0.4906812517911425\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Suma valor de acciones para acción N : 0.3971479666850273\n",
      "Suma valor de acciones para acción S : 0.3971479666850273\n",
      "Suma valor de acciones para acción E : 0.419883697678049\n",
      "Suma valor de acciones para acción W : 0.43083667781089263\n",
      "Valor de estado óptimo: 0.43083667781089263\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Suma valor de acciones para acción N : 0.47546845990209285\n",
      "Suma valor de acciones para acción S : 0.4060648766897139\n",
      "Suma valor de acciones para acción E : 0.29390372416328836\n",
      "Suma valor de acciones para acción W : 0.4044542920699692\n",
      "Valor de estado óptimo: 0.47546845990209285\n",
      "Acción óptima en estado: N\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Suma valor de acciones para acción N : -0.6522525668036881\n",
      "Suma valor de acciones para acción S : 0.26739197968846806\n",
      "Suma valor de acciones para acción E : 0.13460011480367554\n",
      "Suma valor de acciones para acción W : 0.2772904873898593\n",
      "Valor de estado óptimo: 0.2772904873898593\n",
      "Acción óptima en estado: W\n",
      "\n",
      "Valores de estado óptimos: {(0, 0): 0.644969148704666, (0, 1): 0.7443801453743135, (0, 2): 0.8477662777370815, (0, 3): 1.0, (1, 0): 0.5663141587317062, (1, 2): 0.571859032392272, (1, 3): -1.0, (2, 0): 0.4906812517911425, (2, 1): 0.43083667781089263, (2, 2): 0.47546845990209285, (2, 3): 0.2772904873898593}\n",
      "Política óptima: {(0, 0): 'E', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'N', (1, 2): 'N', (1, 3): 'N', (2, 0): 'N', (2, 1): 'W', (2, 2): 'N', (2, 3): 'W'}\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "\n",
    "# policy_evaluation: encontrar valores de par estado-acción óptimos y política óptima para cada estado \n",
    "# params: gw=MDP gridworld, gamma=descuento\n",
    "\n",
    "def policy_evaluation(gw,gamma):   \n",
    "    \n",
    "    # valores de estado, para cada estado inicializar en 0 \n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    # nuevos valores de estado, para cada estado inicializar en 0\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)    \n",
    "    \n",
    "    # valores de par estado-accion, para cada estado inicializar en 0\n",
    "    states = [gw.states]\n",
    "    action_values = {gw.get_allowed_actions((0,0))[0]: 0.0, gw.get_allowed_actions((0,0))[1]: 0.0, gw.get_allowed_actions((0,0))[2]: 0.0, gw.get_allowed_actions((0,0))[3]: 0.0}\n",
    "    \n",
    "    state_q_values= {key: action_values[k] for key in states}\n",
    "    \n",
    "\n",
    "    \n",
    "    # politica aleatoria \n",
    "    # inicializar politica aleatoria \n",
    "    gw.policy = {}\n",
    "    keys = gw.states # keys del diccionario son los estados posibles del MDP\n",
    "    \n",
    "    # iterar sobre estados posibles del MDP\n",
    "    for state in keys:    \n",
    "        # inicializar valores de política aleatoria escogiendo acción aleatoria en estado \n",
    "        gw.policy[state] = random.choice(gw.get_allowed_actions(state) )\n",
    "    \n",
    "    print(\"Política aleatoria incial: \"+str(gw.policy) )    \n",
    "    \n",
    "    \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:          \n",
    "        \n",
    "        # Dar un paso en par estado-accion con accion dada por la política aleatoria\n",
    "        new_state, reward, _, done = gw.step(state, gw.policy[(state)], random=True)\n",
    "        \n",
    "        if(done==True): # llega a estado terminal\n",
    "            # actualización para un estado terminal        \n",
    "            new_values[state] = reward # valor de estado para estado terminal es recompensa\n",
    "            print(\"Estado terminal: \"+str(state) )\n",
    "            print(\"Recompensa estado terminal: \"+str(reward) )\n",
    "        else: # llega a estado no terminal\n",
    "            # actualización para un estado no terminal\n",
    "            \n",
    "            accion_opt = -1 # acción óptima en estado (política)\n",
    "            val_estado_opt = -100 # valor óptimo en estado \n",
    "            print(\"Estado actual: \"+str(state) )\n",
    "            # iterar sobre acciones permitidas en estado state\n",
    "            for action in gw.get_allowed_actions(state):\n",
    "                \n",
    "                suma_actions = 0 # suma de valores de par estado-accion real que puede tomar (en un estado)\n",
    "                # iterar sobre Las posibles acciones (reales) que puede tomar el agente dada una acción\n",
    "                for real_action in  gw.real_actions[action]:\n",
    "                    # si acción real es igual a la acción que toma: probabilidad 0.8\n",
    "                    if real_action==action: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities[0]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.8\n",
    "                        \n",
    "                    # si acción real es diferente a la acción que toma:  probabilidad 0.1\n",
    "                    else: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities[1]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.1\n",
    "                \n",
    "                print(\"Suma valor de acciones para acción \"+str(action)+' : '+str( suma_actions) )\n",
    "                if suma_actions>val_estado_opt:\n",
    "                    val_estado_opt = suma_actions\n",
    "                    accion_opt=action\n",
    "        \n",
    "            # actualizar valor de estado con el valor óptimo de la acción\n",
    "            new_values[state]= val_estado_opt\n",
    "            # actualizar política con acción óptima para el estado\n",
    "            gw.policy[state] = accion_opt\n",
    "            \n",
    "            \n",
    "            print(\"Valor de estado óptimo: \"+str(new_values[state]) )\n",
    "            print(\"Acción óptima en estado: \"+str(gw.policy[state]) )\n",
    "            print(\"\")\n",
    "            \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values)         \n",
    "    print(\"Valores de estado óptimos: \"+str(new_values) )\n",
    "    print(\"Política óptima: \"+str (gw.policy))\n",
    "    \n",
    "        \n",
    "value_iteration(gw, 0.9)\n",
    "\n",
    "#gw.get_allowed_actions(state): Permite conocer el espacio de acciones dado un estado.\n",
    "#gw.real_actions[action]: Las posibles acciones (reales) que puede tomar el agente dada una acción.\n",
    "#gw.action_probabilities[action]: La probabilidad de que el agente tomé las acciones reales.\n",
    "#gw.step(state, action, random): Permite ejecutar un paso en el gridworld (MDP). Con este método se puede conocer el estado siguiente, la recompensa, la accion tomada y la bandera de un estado terminal. Nota: si el parámetro random es Falso, las acciones son determinisiticas (con probabilidad 1, toma la acción dada).\n",
    "#gw.state_values[state]: Los valores del estado state.\n",
    "#gw.policy[state]: La acción dada por la política para el estado state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n",
    "def policy_evaluation(gw, gamma):        \n",
    "    g_tmp = {}\n",
    "    \n",
    "    \n",
    "    for state in gw.states:\n",
    "        # TO DO: Actualice los valores de los estados con Policy Evaluation\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        gw.state_values[state] =  gw.max_val(gw.state_q_values[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora implemente la función *policy improvement* que permitirá ajustar la política $\\pi(s)$ de acuerdo a la nueva estimación de los valores $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDITABLE\n",
    "def policy_improvement(gw):\n",
    "    # Bandera de que la política es estable\n",
    "    policy_stable = True\n",
    "    \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:\n",
    "        # TO DO: Actualice la política\n",
    "        # best_action es accion máxima de valor de par estado-accion, para el estado actual\n",
    "        best_action = max(gw.state_q_values[state])\n",
    "        \n",
    "        # si acción con maximo valor de par estado-accion, para el estado actual es diferentes a lo que dice la política actual\n",
    "        if best_action!=gw.policy[state]:\n",
    "            # actualizar política con acción con maximo valor de par estado-accion\n",
    "            gw.policy[state]=best_action\n",
    "            policy_stable = False\n",
    "            \n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora resuelva el gridworld por *policy iteration* utilizando las funciones que usted implementó:\n",
    "* Incialice los estados iniciales\n",
    "* Establezca un $\\gamma=0.99$, un horizonte de 15 iteraciones y el estado inicial en (2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El nuevo MDP: <gridworld.GridWorld object at 0x00000189490249D0>\n",
      "Ajuste de valores iniciales: {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 1.0, (1, 0): 0.0, (1, 2): 0.0, (1, 3): -1.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0}\n",
      "Valores par estado-acción iniciales: {(0, 0): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (0, 1): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (0, 2): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (0, 3): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (1, 0): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (1, 2): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (1, 3): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (2, 0): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (2, 1): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (2, 2): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}, (2, 3): {'N': 0.0, 'S': 0.0, 'E': 0.0, 'W': 0.0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.0,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 2): 0.0,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): 0.0,\n",
       " (2, 1): 0.0,\n",
       " (2, 2): 0.0,\n",
       " (2, 3): 0.0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "\n",
    "# crear MDP=GridWorld\n",
    "# params: 3 filas, 4 columnas,  (1,1)=celda bloqueada, (1,3)=bomba, (0,3)=diamante, 0.0=valor de estados inicial\n",
    "gw = GridWorld(3, 4, [(1,1)], [(1,3)], [(0,3)], 0.0)\n",
    "print(\"El nuevo MDP: \"+str(gw) )\n",
    "\n",
    "# Inicializar valores de los estados\n",
    "gw.state_values = gw.init_values()\n",
    "#print(\"Valores de estados iniciales: \"+str(gw.state_values) )\n",
    "\n",
    "# Se ajusta el valor de los estados terminales\n",
    "# definir valor de estados terminales\n",
    "gw.state_values[(0,3)] = 1.0\n",
    "gw.state_values[(1,3)] = -1.0  \n",
    "print(\"Ajuste de valores iniciales: \"+str(gw.state_values) )\n",
    "# TO DO: Actualice los valores q para Value iteration\n",
    "#q = dict.fromkeys(gw.get_allowed_actions((state)), 0.0)        \n",
    "#q\n",
    "\n",
    "# valores de par estado-accion, para cada estado inicializar en 0\n",
    "state_q_values = gw.init_qvalues()\n",
    "print(\"Valores par estado-acción iniciales: \"+str(state_q_values) )\n",
    "gamma=0.99 # tasa descuento\n",
    "H=15 # horizonte de tiempo\n",
    "\n",
    "init_state = gw.state_values\n",
    "init_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:**\n",
    "1. Ejecute la siguiente celda y haga clic sobre la ventana del gridworld. \n",
    "2. Podrá ir viendo como se actualizan los valores de cada estado cada vez que presionas la tecla **Espacio**. \n",
    "3. Tambien puede presionar la tecla Q en la ventana del gridworld para ver los valores Q de cada par (s,a)\n",
    "4. Si tiene un error en la función 'policy_evaluation' o en 'policy_improvement', despues de corregirlo es necesario volver a correr las 5 celdas anteriores para poder volver a lanzar la interfaz del gridworld.\n",
    "5. Una vez completadas las iteraciones, aparecerá un mensaje que dice TESTING. Presiona la tecla **Enter** para ver la política\n",
    "6. Para cerrar el gridworld presiona la tecla **Esc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (0, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (0, 3)\n",
      "Recompensa estado terminal: 1.0\n",
      "Estado actual: (1, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (1, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado terminal: (1, 3)\n",
      "Recompensa estado terminal: -1.0\n",
      "Estado actual: (2, 0)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 1)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 2)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n",
      "Estado actual: (2, 3)\n",
      "Q(s,a) en acción: N\n",
      "0.0\n",
      "Q(s,a) en acción: S\n",
      "0.0\n",
      "Q(s,a) en acción: E\n",
      "0.0\n",
      "Q(s,a) en acción: W\n",
      "0.0\n",
      "Nuevo valor de estado V(s): 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gw.policy_evaluation = MethodType(policy_evaluation, gw)\n",
    "gw.policy_improvement = MethodType(policy_improvement, gw)\n",
    "gw.solve_policy_iteration(gamma=gamma, horizon=H, init_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con *policy iteration*, los valores de los estados son: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "V(0, 0) = 0.951\n",
    "V(0, 1) = 0.964\n",
    "V(0, 2) = 0.977\n",
    "V(0, 3) = 1.000\n",
    "V(1, 0) = 0.939\n",
    "V(1, 2) = 0.890\n",
    "V(1, 3) = -1.000\n",
    "V(2, 0) = 0.925\n",
    "V(2, 1) = 0.913\n",
    "V(2, 2) = 0.900\n",
    "V(2, 3) = 0.790\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La política aprendida es para maximizar la recompensa desde el estado inicial (2,0) es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "Acción a tomar en (0, 0): E\n",
    "Acción a tomar en (0, 1): E\n",
    "Acción a tomar en (0, 2): E\n",
    "Acción a tomar en (0, 3): N\n",
    "Acción a tomar en (1, 0): N\n",
    "Acción a tomar en (1, 2): W\n",
    "Acción a tomar en (1, 3): N\n",
    "Acción a tomar en (2, 0): N\n",
    "Acción a tomar en (2, 1): W\n",
    "Acción a tomar en (2, 2): W\n",
    "Acción a tomar en (2, 3): S\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto termina la implementación de los algoritmos de Policy y Value iteration, puede vovlerlos a ejecutar como desee y variar sus parámetros para ver los efectos que estos tienen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serpientes y Escaleras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.0,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 0.0,\n",
       " (0, 4): 0.0,\n",
       " (0, 5): 0.0,\n",
       " (0, 6): 0.0,\n",
       " (0, 7): 0.0,\n",
       " (0, 8): 0.0,\n",
       " (0, 9): 0.0,\n",
       " (0, 10): 0.0,\n",
       " (0, 11): 0.0,\n",
       " (0, 12): 0.0,\n",
       " (0, 13): 0.0,\n",
       " (0, 14): 0.0,\n",
       " (0, 15): 0.0,\n",
       " (0, 16): 0.0,\n",
       " (0, 17): 0.0,\n",
       " (0, 18): 0.0,\n",
       " (0, 19): 0.0,\n",
       " (0, 20): 0.0,\n",
       " (0, 21): 0.0,\n",
       " (0, 22): -1.0,\n",
       " (0, 23): 0.0,\n",
       " (0, 24): 0.0,\n",
       " (0, 25): 0.0,\n",
       " (0, 26): 0.0,\n",
       " (0, 27): 0.0,\n",
       " (0, 28): 0.0,\n",
       " (0, 29): 0.0,\n",
       " (0, 30): 0.0,\n",
       " (0, 31): 0.0,\n",
       " (0, 32): 0.0,\n",
       " (0, 33): 0.0,\n",
       " (0, 34): 0.0,\n",
       " (0, 35): 0.0,\n",
       " (0, 36): -1.0,\n",
       " (0, 37): 0.0,\n",
       " (0, 38): 0.0,\n",
       " (0, 39): 0.0,\n",
       " (0, 40): 0.0,\n",
       " (0, 41): 0.0,\n",
       " (0, 42): 0.0,\n",
       " (0, 43): 0.0,\n",
       " (0, 44): -1.0,\n",
       " (0, 45): 0.0,\n",
       " (0, 46): 0.0,\n",
       " (0, 47): 0.0,\n",
       " (0, 48): 0.0,\n",
       " (0, 49): 0.0,\n",
       " (0, 50): 0.0,\n",
       " (0, 51): 0.0,\n",
       " (0, 52): 0.0,\n",
       " (0, 53): 0.0,\n",
       " (0, 54): 0.0,\n",
       " (0, 55): 0.0,\n",
       " (0, 56): 0.0,\n",
       " (0, 57): 0.0,\n",
       " (0, 58): 0.0,\n",
       " (0, 59): 0.0,\n",
       " (0, 60): 0.0,\n",
       " (0, 61): 0.0,\n",
       " (0, 62): 0.0,\n",
       " (0, 63): 0.0,\n",
       " (0, 64): 0.0,\n",
       " (0, 65): 0.0,\n",
       " (0, 66): -1.0,\n",
       " (0, 67): 0.0,\n",
       " (0, 68): 0.0,\n",
       " (0, 69): 0.0,\n",
       " (0, 70): 0.0,\n",
       " (0, 71): 0.0,\n",
       " (0, 72): 0.0,\n",
       " (0, 73): 0.0,\n",
       " (0, 74): 0.0,\n",
       " (0, 75): 0.0,\n",
       " (0, 76): 0.0,\n",
       " (0, 77): 0.0,\n",
       " (0, 78): 0.0,\n",
       " (0, 79): 1.0,\n",
       " (0, 80): 0.0,\n",
       " (0, 81): 0.0,\n",
       " (0, 82): 0.0,\n",
       " (0, 83): 0.0,\n",
       " (0, 84): 0.0,\n",
       " (0, 85): 0.0,\n",
       " (0, 86): 0.0,\n",
       " (0, 87): 0.0,\n",
       " (0, 88): -1.0,\n",
       " (0, 89): 0.0,\n",
       " (0, 90): 0.0,\n",
       " (0, 91): 0.0,\n",
       " (0, 92): 0.0,\n",
       " (0, 93): 0.0,\n",
       " (0, 94): 0.0,\n",
       " (0, 95): 0.0,\n",
       " (0, 96): 0.0,\n",
       " (0, 97): 0.0,\n",
       " (0, 98): 0.0,\n",
       " (0, 99): 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from gridworld2 import GridWorld2, pygame\n",
    "import copy\n",
    "from colorama import Fore\n",
    "import random\n",
    "\n",
    "# crear MDP=GridWorld2 (Serpientes y Escaleras)\n",
    "# params: 1 filas, 100 columnas,  []=celda bloqueada, (1,3)=bomba, (0,3)=diamante, 0.0=valor de estados inicial\n",
    "\n",
    "diamantes = [(0,79),(0,99)]\n",
    "bombas=[(0,22),(0,36),(0,44),(0,66),(0,88)]\n",
    "\n",
    "gw = GridWorld2(1, 100, [], bombas, diamantes, 0.0)\n",
    "\n",
    "# Inicializar valores y política\n",
    "gw.state_values = gw.init_values()\n",
    "gw.state_q_values = gw.init_qvalues()\n",
    "\n",
    "\n",
    "# definir valor de estados \n",
    "for state in gw.state_values:\n",
    "    if state in diamantes:\n",
    "        gw.state_values[state] = 1.0\n",
    "    elif state in bombas:\n",
    "        gw.state_values[state] = -1.0\n",
    "    else:\n",
    "        gw.state_values[state] = 0.0\n",
    "        \n",
    "# definir política siempre acción='N'\n",
    "gw.policy = dict.fromkeys(gw.states, 'N') # cambiar política\n",
    "gw.updateGrid()\n",
    "\n",
    "\n",
    "def nueva_casilla_azul(state):\n",
    "    gw.state_values[state] = 1.0\n",
    "    gw.updateGrid()\n",
    "    \n",
    "def nueva_casilla_roja(state):\n",
    "    gw.state_values[state] = -1.0\n",
    "    gw.updateGrid()\n",
    "    \n",
    "\n",
    "gw.state_values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo una política $\\pi$,  el valor de un estado $V(s)$ en el tiempo $t$ formalmente corresponde a **la suma descontada de las recompensas recibidas, si el agente tiene como estado inicial $s$ y se comporta óptimamente en adelante**. En palabras simples, indica cuánta utilidad  recibiría el agente si comenzara en $s$ y se comportara bien de ahí en adelante, \n",
    "\n",
    "$$V_{t}(s)=\\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{t-1}(s'))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de valor de estados: actualizar la función de valor de estados v(s) dada una politica pi\n",
    "# parametros: gw=MDP, gamma=tasa de descuento\n",
    "\n",
    "def update_values(gw, gamma):\n",
    "    \n",
    "    # vectores para los valores de estado\n",
    "    # valores de estado, para cada estado inicializar en 0 \n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    # nuevos valores de estado, para cada estado inicializar en 0\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    \n",
    "    # iterar sobre los estados posibles del MDP\n",
    "    for state in gw.states: \n",
    "        # Tomar acción de politica dada para el estado\n",
    "        action = gw.policy[state]\n",
    "        \n",
    "        # Dar un paso en par estado-accion (random=False deterministico)\n",
    "        # retorna: estado siguiente, recompensa, accion\n",
    "        new_state, reward, _, done = gw.step(state, action, random=False)\n",
    "\n",
    "        # Actualizar valores de estado\n",
    "        if(done): # llega a estado terminal\n",
    "            # actualización para un estado terminal\n",
    "            new_values[state] = reward # valor de estado v(s) para estado terminal es recompensa\n",
    "        else:\n",
    "            \n",
    "            # valor de estado v(s) es la suma descontada de recompensas de las acciones posibles s'\n",
    "            suma_actions = 0.0\n",
    "            \n",
    "            # iterar sobre Las posibles acciones (reales) que puede tomar el agente dada una acción\n",
    "            for real_action in  gw.real_actions[action]:\n",
    "                # si acción real es igual a la acción que toma: probabilidad 0.8\n",
    "                if real_action==action: \n",
    "                    new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                    suma_actions+=gw.action_probabilities[0]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.8\n",
    "                        \n",
    "                # si acción real es diferente a la acción que toma:  probabilidad 0.2\n",
    "                else: \n",
    "                    new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                    suma_actions+=gw.action_probabilities[1]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.2\n",
    "                    \n",
    "            print(\"Suma valor de acciones para acción \"+str(action)+' : '+str( suma_actions) )\n",
    "            # actualización para un estado no terminal\n",
    "            new_values[state] = suma_actions\n",
    "                \n",
    "            \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MethodType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gw\u001b[38;5;241m.\u001b[39mupdate_values \u001b[38;5;241m=\u001b[39m \u001b[43mMethodType\u001b[49m(update_values, gw)\n\u001b[0;32m      2\u001b[0m gw\u001b[38;5;241m.\u001b[39msolve_dynamic_programming(gamma\u001b[38;5;241m=\u001b[39mgamma, horizon\u001b[38;5;241m=\u001b[39mH)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MethodType' is not defined"
     ]
    }
   ],
   "source": [
    "gamma=0.9\n",
    "H=15\n",
    "gw.update_values = MethodType(update_values, gw)\n",
    "gw.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n",
    "\n",
    "# value_iteration: encontrar valores de estado óptimos para cada estado y politica óptima(acción óptima) para cada estado \n",
    "# params: gw=MDP gridworld, gamma=descuento\n",
    "\n",
    "def value_iteration(gw,gamma):   \n",
    "    \n",
    "    # valores de estado, para cada estado inicializar en 0 \n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    # nuevos valores de estado, para cada estado inicializar en 0\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)    \n",
    "    \n",
    "    \n",
    "    # politica aleatoria \n",
    "    # inicializar politica aleatoria \n",
    "    gw.policy = {}\n",
    "    keys = gw.states # keys del diccionario son los estados posibles del MDP\n",
    "    \n",
    "    # iterar sobre estados posibles del MDP\n",
    "    for state in keys:    \n",
    "        # inicializar valores de política aleatoria escogiendo acción aleatoria en estado \n",
    "        gw.policy[state] = random.choice(gw.get_allowed_actions(state) )\n",
    "    print(\"Política aleatoria incial: \"+str(gw.policy) )    \n",
    "    \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:          \n",
    "        \n",
    "        # Dar un paso en par estado-accion con accion dada por la política aleatoria\n",
    "        new_state, reward, _, done = gw.step(state, gw.policy[(state)], random=True)\n",
    "        \n",
    "        if(done==True): # llega a estado terminal\n",
    "            # actualización para un estado terminal        \n",
    "            new_values[state] = reward # valor de estado para estado terminal es recompensa\n",
    "            print(\"Estado terminal: \"+str(state) )\n",
    "            print(\"Recompensa estado terminal: \"+str(reward) )\n",
    "        else: # llega a estado no terminal\n",
    "            # actualización para un estado no terminal\n",
    "            \n",
    "            accion_opt = -1 # acción óptima en estado (política)\n",
    "            val_estado_opt = -100 # valor óptimo en estado \n",
    "            print(\"Estado actual: \"+str(state) )\n",
    "            # iterar sobre acciones permitidas en estado state\n",
    "            for action in gw.get_allowed_actions(state):\n",
    "                \n",
    "                suma_actions = 0 # suma de valores de par estado-accion real que puede tomar (en un estado)\n",
    "                # iterar sobre Las posibles acciones (reales) que puede tomar el agente dada una acción\n",
    "                for real_action in  gw.real_actions[action]:\n",
    "                    # si acción real es igual a la acción que toma: probabilidad 0.8\n",
    "                    if real_action==action: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities[0]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.8\n",
    "                        \n",
    "                    # si acción real es diferente a la acción que toma:  probabilidad 0.1\n",
    "                    else: \n",
    "                        new_state, reward, _, done = gw.step(state, real_action, random=False)\n",
    "                        suma_actions+=gw.action_probabilities[1]*(reward+gamma*gw.state_values[new_state]) #probabilidad 0.1\n",
    "                \n",
    "                print(\"Suma valor de acciones para acción \"+str(action)+' : '+str( suma_actions) )\n",
    "                if suma_actions>val_estado_opt:\n",
    "                    val_estado_opt = suma_actions\n",
    "                    accion_opt=action\n",
    "        \n",
    "            # actualizar valor de estado con el valor óptimo de la acción\n",
    "            new_values[state]= val_estado_opt\n",
    "            # actualizar política con acción óptima para el estado\n",
    "            gw.policy[state] = accion_opt\n",
    "            \n",
    "            \n",
    "            print(\"Valor de estado óptimo: \"+str(new_values[state]) )\n",
    "            print(\"Acción óptima en estado: \"+str(gw.policy[state]) )\n",
    "            print(\"\")\n",
    "            \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values)         \n",
    "    print(\"Valores de estado óptimos: \"+str(new_values) )\n",
    "    print(\"Política óptima: \"+str (gw.policy))\n",
    "        \n",
    "value_iteration(gw, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.9\n",
    "H=15\n",
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=init_state )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
