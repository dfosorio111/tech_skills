{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escaleras y Serpientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Diego Fernando Osorio Diaz df.osorio11@uniandes.edu.co 20153517\n",
    "\n",
    "Emmanuel González González e.gonzalezg@uniandes.edu.co 201614679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/escalerita.png\" alt=\"centered image\" width=300/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones importantes:**\n",
    "\n",
    "* La meta del jugador es ganar la partida llegando a una de las casillas marcadas en azul.\n",
    "* El jugador pierde la partida si cae en una de las casillas marcadas en rojo.\n",
    "* En cada jugada, antes de lanzar el dado, el jugador decide si quiere avanzar o retroceder el número de casillas indicadas por el dado.\n",
    "* En las casillas 1 y 100 la ficha rebota (si se supera el extremo, se avanza en la otra dirección la cantidad restante)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El presente informe presenta el desarrollo de algoritmos de programación dinámica que permiten evaluar la función de valor de estados $V(s)$ siguiendo una política $\\pi(s)$.\n",
    "Por otro lado se presenta el desarrollo de algortimos de \n",
    "value iteration para aproximar el valor de estados óptimos $V^*(s) $ y política óptima $\\pi^*(s)$. La diferencia entre la programación dinámica y el método de value iteration es que el primer permite evaluar el valor de los estados dada una política predeterminada, mientras que en value iteration se busca aproximar los valores de estados óptimos (mejores valores de estados) y encontrar la política óptima que son las mejores acciones en cada estado que permitan maximizar la recompensa recibida por el agente a lo largo del horizonte.\n",
    "\n",
    "\n",
    "Con base en la definición de estados, recompensas, acciones y probabilidades se procedió a resolver el MDP finito mediante 2 métodos. Se implementaron algoritmos de Programación Dinámica para evaluar la función de valor de los estados del MDP, siguiente una política dada. Se realizó la implementación de la función de valor siguiendo la política que siempre escoge la acción de Avanzar, y la implementación para la función de valor siguiendo la política aleatoria (escoge las acciones Avanzar o Retroceder aleatoriamente). \n",
    "\n",
    "Del mismo modo se realizó la implementación del algoritmo Value Iteration para aproximar los valores de estados óptimos y la política óptima que maximiza la recompensa recibida por el agente a lo  largo del horizonte de tiempo. Finalmente se realizaron 2 escenarios de variaciones en la definición de las coordenadas de los estados terminales (bombas y metas) para observar el comportamiento del algoritmo.\n",
    "\n",
    "\n",
    "Se observa que la programación dinámica evaluada para la política avanzar que los valores de estados cercanos a los estados terminales son más favorables o se tiene una mayor recompensa en relación con los valores de estados cercanos a estados terminales en la política aleatoria, donde el valor de estados no supera el valor de 1. Por otro lado, se observa un mayor número de estados favorables con valor de estado positivo para la política de avanzar en relación con la política aleatoria. Por otro lado se observa que los valores de estados para estados negativos es más negativo o menos favorable implementando la política que siempre avanza. \n",
    "Para los resultados de value iteration se presenta el valor de estados óptimos y la política óptima obtenida para cada escenario.\n",
    "\n",
    "\n",
    "El algoritmo de value iteration convergió a valores de estado óptimo que son mayores para estados cercanos a las recompensas y a los estados cercanos a las coordenadas de entrada a las escaleras, mientras que el valor de estado óptimo es menor para estados cercanos a las bombas y a los estados cercanos a las coordenadas de entrada de las serpientes. El valor de estado óptimo es máximo para las recompensas con un valor constante de +10 y mínimo para las bombas -10, ya que se programó los algoritmos asumiendo que el valor de estado en estados terminales es la recompensa (paso adicional). El valor de estado óptimo máximo para estados no terminales en el escenario con tasa de descuento $\\gamma=0.9$ es 4.13 para los estados (7,0) y (1,1) que corresponden a las coordenadas de entrada y salida de una escalera, mientras que el valor de estado óptimo mínimo para estados no terminales en el escenario con tasa de descuento $\\gamma=0.9$ es 0.45 para los estados (9,0) y (2,7) que corresponden a las coordenadas de entrada y salida de una serpiente.\n",
    "\n",
    "Del mismo modo el valor de estado óptimo máximo para estados no terminales en el escenario con tasa de descuento $\\gamma=0.99$ es 6.79 para el estado (1,3), mientras que el valor de estado óptimo mínimo para estados no terminales en el escenario con tasa de descuento $\\gamma=0.99$ es 3.01 para los estados (7,8) y (6,5). \n",
    "\n",
    "A partir de los resultados obtenidos para el algoritmo de value iteration se observa que para una tasa de descuento mayor ($\\gamma=0.99$) que permite balancear la preferencia de tener recompensas a corto plazo o largo plazo, el algoritmo se comporta de manera menos conservadora y prefiere estados donde tenga una mayor recompensa inmedianta en el corto plazo, por ende en los resultados en simulación para este escenario los valores de estado óptimos eran mayores para estados cercanos a la meta, mientras que para valores inferiores de la tasa de descuento $\\gamma=0.9$ el algoritmo es más conservador y prefiere obtener una recompensa a largo plazo en el horizonte de tiempo H, por ende los valores de estado óptimos mayores se encontraban cerca a escaleras de manera relativa en relación con el escenario de $\\gamma=0.99$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "El valor de estado óptimo aproximado para las entradas y salidas de escaleras y serpientes es el mismo. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "El Proceso de Decisión de Markov (MDP) del juego \"Escaleras y Serpientes\" se detalla a continuación:\n",
    "\n",
    "* Estados: casillas de coordenadas sobre tablero 10x10 donde cada casilla es un estado. El estado inicial es (9,0) en todos los escenarios(última fila, primera columna de arriba para abajo)), mientras que los estados terminales (versión original) son: metas en (0,0) y (2,0), bombas en (7,2),(6,3),(5,4),(3,6),(1,8). Los demás estados del MDP son estados no terminales, entre los cuales están las escaleras y serpientes. Las coordenadas de estado inicial de las escaleras están dadas en (9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0), y tienen su respectivo estado final de escalera en coordenadas (7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0) para cada pareja. Del mismo modo las coordenadas de estado inicial de las serpientes (0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7) y su respectivo estado final de la serpiente en (7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8).\n",
    "\n",
    "* Recompensas: Las recompensas para los estados del MDP se definieron de la siguiente manera: metas  $r(s_t)=10$, bombas $r(s_t)=-1$ y la recompensa para todos los estados no terminales(recompensa de transición) $r(s)=0.1$. Es decir se tiene mayor recompensa por alcanzar la meta, se tiene una penalización negativa pero menor magnitud para las bombas y una recompensa de transición de 0.1 para todos los estados. Si el agente llega a estados terminales entonces recibe la recompensa del respectivo estado terminal y el episodio termina.\n",
    "\n",
    "* Acciones: Las acciones posibles son Avanzar 'A' y Retroceder 'R'. Las acciones reales para la acción Avanzar son las 6 posibles acciones de casillas que puede avanzar hacia adelante dadas por el dado, mientras que las acciones reales para la acción Retroceder son las 6 posible acciones que puede retroceder hacia atrás dadas por el dado en dirección contraria, incluidas las casillas a las que puede rebotar.\n",
    "\n",
    "* Probabilidades $p(s',r|s,a), s',r,a$: las probabilidades son uniformes y equiprobables para las 6 acciones reales $s'$ tanto para avanzar como para retroceder, entonces la probabilidad de cada acción real que pueden tomar las acciones posibles de Avanza y Retroceder es 1/6 para cada una de las acciones reales que puede tomar, hacia adelante o hacia atrás.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.2.0 (SDL 2.0.22, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Librerias que contienen la dinámica del entorno gridworld\n",
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit\n",
    "gw = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método subpolitica permite cambiar la política que toma el agente. El parámetro de entrada son las acciones de la política predeterminada que toma el agente.\n",
    "\n",
    "El método itera sobre los estados posibles y asigna la política (acción) a cada estado, a partir de la política que recibe por parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subpolitica (x):\n",
    "    # iterar sobre estados\n",
    "    for i in gw.states:\n",
    "        # asignar politica en estado i como x[i[0],i[1]] \n",
    "        gw.policy[i]=x[i[0]][i[1]]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inipoli():\n",
    "    state = gw.states\n",
    "\n",
    "    # definir politica para avanzar\n",
    "    # iterar sobre estados\n",
    "    for i, s in enumerate(state):\n",
    "        if s[0]%2==0 and s[1]!=0 : # si fila es par y columna no es 0, acción W\n",
    "            gw.policy[s] = 'W'\n",
    "        elif s[0]%2==0 and s[1]==0 : # si fila es par y columna es 0, acción N\n",
    "            gw.policy[s] = 'N'\n",
    "        elif s[0]%2==1 and s[1]!=9 : # si fila es par y columna no es 9, acción E  \n",
    "            gw.policy[s] = 'E'\n",
    "        elif s[0]%2==1 and s[1]==9 : # si fila es par y columna es 9, acción N\n",
    "            gw.policy[s] = 'N'\n",
    "    a=[[]]\n",
    "    r=[[]]\n",
    "\n",
    "    # iterar sobre filas\n",
    "    for i in range (10):\n",
    "        # iterar sobre columnas\n",
    "        for j in range(10):\n",
    "            a[i].append('')\n",
    "            r[i].append('')\n",
    "        a.append([])\n",
    "        r.append([])\n",
    "\n",
    "    for i in gw.states:\n",
    "        a[i[0]][i[1]]=gw.policy[i]\n",
    "\n",
    "    for i, s in enumerate(state):\n",
    "        if s[0]%2==0 and s[1]!=9 :\n",
    "            gw.policy[s] = 'E'\n",
    "        elif s[0]%2==0 and s[1]==9 :\n",
    "            gw.policy[s] = 'S'\n",
    "        elif s[0]%2==1 and s[1]!=0 :\n",
    "            gw.policy[s] = 'W'\n",
    "        elif s[0]%2==1 and s[1]==0 :\n",
    "            gw.policy[s] = 'S'\n",
    "    for i in gw.states:\n",
    "        r[i[0]][i[1]]=gw.policy[i]\n",
    "    subpolitica(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se inicializan las política de avanzar y retroceder, manteniendo las convenciones del gridworld en donde se define para cada estado cúal es la respectiva acción para avanzar y retroceder con las acciones posibles definidas preestablecidas en la clase Gridworld.py.\n",
    "\n",
    "Entonces se recorren todos los estados del MDP y siguiendo las convenciones y las direcciones que el agente debe tomar, se asigna para cada estado la política de avanzar y retroceder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = gw.states\n",
    "\n",
    "# definir politica para avanzar\n",
    "# iterar sobre estados\n",
    "for i, s in enumerate(state):\n",
    "    if s[0]%2==0 and s[1]!=0 : # si fila es par y columna no es 0, acción W\n",
    "        gw.policy[s] = 'W'\n",
    "    elif s[0]%2==0 and s[1]==0 : # si fila es par y columna es 0, acción N\n",
    "        gw.policy[s] = 'N'\n",
    "    elif s[0]%2==1 and s[1]!=9 : # si fila es par y columna no es 9, acción E  \n",
    "        gw.policy[s] = 'E'\n",
    "    elif s[0]%2==1 and s[1]==9 : # si fila es par y columna es 9, acción N\n",
    "        gw.policy[s] = 'N'\n",
    "a=[[]]\n",
    "r=[[]]\n",
    "\n",
    "# iterar sobre filas\n",
    "for i in range (10):\n",
    "    # iterar sobre columnas\n",
    "    for j in range(10):\n",
    "        a[i].append('')\n",
    "        r[i].append('')\n",
    "    a.append([])\n",
    "    r.append([])\n",
    "\n",
    "for i in gw.states:\n",
    "    a[i[0]][i[1]]=gw.policy[i]\n",
    "\n",
    "for i, s in enumerate(state):\n",
    "    if s[0]%2==0 and s[1]!=9 :\n",
    "        gw.policy[s] = 'E'\n",
    "    elif s[0]%2==0 and s[1]==9 :\n",
    "        gw.policy[s] = 'S'\n",
    "    elif s[0]%2==1 and s[1]!=0 :\n",
    "        gw.policy[s] = 'W'\n",
    "    elif s[0]%2==1 and s[1]==0 :\n",
    "        gw.policy[s] = 'S'\n",
    "for i in gw.states:\n",
    "    r[i[0]][i[1]]=gw.policy[i]\n",
    "subpolitica(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de solución: Programación dinámica y Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo una política $\\pi$,  el valor de un estado $V(s)$ en el tiempo $t$ formalmente corresponde a **la suma descontada de las recompensas recibidas, si el agente tiene como estado inicial $s$ y se comporta óptimamente en adelante**. En palabras simples, indica cuánta utilidad  recibiría el agente si comenzara en $s$ y se comportara bien de ahí en adelante, \n",
    "\n",
    "$$V_{t}(s)=\\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{t-1}(s'))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces el método de programación dinámica aproxima el valor de estados dada una política en función de la recompensa de la transición que obtiene al tomar la acción a en el estado s y pasar al estado s' $R(s,a,s')$, y la suma descontada con el valor del estado siguiente $V_t-1 (s')$ en la actualización anterior.\n",
    "A continuación se presentan los métodos para aproximar la función de valor de estados para 2 políticas: Avanzar y Aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir el algoritmo de función de valor dada la política de avanzar se realizó el siguiente procedimiento. Se definieron los vectores de los valores de estado $v(s)$ y $v(s')$ que se van a actualizar en el horizonte tempora.\n",
    "Para la política de avanzar la acción posibles es siempre avanzar, mientras la 6 acciones reales son las 6 casillas correspondientes a avanzar (incluido en los casos que rebota) entonces se generan vectores de valores de estado de las 6 acciones posibles al avanzar y sus respectivas recompensas.\n",
    "\n",
    "El algoritmo itera sobre todos los estados posibles y actualiza el valor de los estados de la siguiente manera: para estados terminales (bombas y metas) el valor del estado es la recompensa obtenida en el estado (+10 o -10), si el estado es una casilla de coordenadas de entrada de serpientes o escaleras asigna el valor de estado de las casillas de coordenadas de salida de de serpientes o escaleras, respectivamente. \n",
    "\n",
    "Para los estados no terminales, da un paso e itera sobre las 6 acciones reales posibles que tiene hacia adelante (que depende de lo que sacó en el dado) y realiza la siguiente actualización:\n",
    "* si el estado siguiente es terminal y aún tiene pasos, para las acciones reales que generan estados terminales pero aun tiene pasos entonces se genera el rebote en las coordenadas (0,0) y (9,0), y cambia la política en estos estados a retroceder y avanzar respectivamente, haciendo uso de la función subpolitica(x) definida anteriormente y realiza una actualización \"manual\" de la política para el estado.\n",
    "* si el estado es no terminal realiza la actualización de la suma descontada de los valores de estado de las 6 acciones reales que puede tomar en la política de avanzar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de valor de estados: actualizar la función de valor de estados v(s) dada una politica pi\n",
    "# politica pi: accion es siempre avanzar\n",
    "# parametros: gw=MDP, gamma=tasa de descuento\n",
    "def update_values(gw, gamma):\n",
    "    # Arreglos para los valores\n",
    "    \n",
    "    # vectores de valor de estados v(s), vs'()\n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    stvalues=[0,0,0,0,0,0] # valor de 6 estados posibles\n",
    "    re=[0,0,0,0,0,0] # recompensa de 6 estados posibles\n",
    "    ra=0 # recompensa auxiliar\n",
    "    sa=0 # estado auxiliar\n",
    "    \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:\n",
    "        \n",
    "        # inicializar valores de estado y recompensas en 0 \n",
    "        stvalues=[0,0,0,0,0,0]\n",
    "        re=[0,0,0,0,0,0]\n",
    "        new_state=state\n",
    "        \n",
    "        # si estado es terminal (meta o bomba)\n",
    "        if state in gw.goals or state in gw.pits:\n",
    "            \n",
    "            # Tomar acción de politica dada\n",
    "            action = gw.policy[state]\n",
    "            # Dar un paso\n",
    "            new_state, reward, _, done = gw.step(state, action, random=False)\n",
    "            # Actualizar valores\n",
    "            new_values[state] = reward # actualización valor de estado terminal es recompensa\n",
    "        \n",
    "        # si estado es serpiente\n",
    "        elif state in serp:\n",
    "            for s in serp:\n",
    "                if s==state:\n",
    "                    ind=serp.index(s) # obtener indice de serpiente\n",
    "                    \n",
    "                    # valor de serpiente es valor de estado terminal de la serpiente\n",
    "                    new_values[state]=gw.state_values[serpf[ind]]\n",
    "                    \n",
    "        # si estado es escalera\n",
    "        elif state in esca:\n",
    "            for s in esca:\n",
    "                if s==state:\n",
    "                    ind=esca.index(s)  # obtener indice de escalera\n",
    "                    # valor de escalera es valor de estado terminal de la escalera\n",
    "                    new_values[state]=gw.state_values[escaf[ind]]\n",
    "        \n",
    "        # si estado no terminal \n",
    "        else:\n",
    "            # iterar sobre posibles estados de politica avanzar\n",
    "            for i in range(len(re)):\n",
    "                \n",
    "                # accion está dada por politica avanzar\n",
    "                action = gw.policy[new_state]\n",
    "                # Dar un paso\n",
    "                new_state, reward, ac, done = gw.step(new_state, action, random=False)\n",
    "                \n",
    "                \n",
    "                re[i]=reward # recompensa en i\n",
    "                stvalues[i]=gw.state_values[new_state] # valor de estado en i es valor de estado vs'(s)\n",
    "                # Actualizar valores\n",
    "                \n",
    "                # si estado siguiente es terminal (meta o bomba) Y tiene pasos posibles [0-5] (6 pasos)\n",
    "                if(new_state in gw.goals or new_state in gw.pits) and i<5:\n",
    "                    \n",
    "                    # si estado siguiente es meta final, cambia política a retroceder\n",
    "                    if new_state==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    # si estado siguiente es inicio de tabler, cambia política a avanzar\n",
    "                    if new_state==(9,0):\n",
    "                        subpolitica(a)\n",
    "                        \n",
    "                    # actualización para un estado terminal 'manual'\n",
    "                    if gw.policy[new_state]=='N':\n",
    "                        new_state = (new_state[0]-1, new_state[1])\n",
    "                    if gw.policy[new_state]=='S':\n",
    "                        new_state = (new_state[0]+1, new_state[1])\n",
    "                    if gw.policy[new_state]=='W':\n",
    "                        new_state = (new_state[0], new_state[1]-1)\n",
    "                    if gw.policy[new_state]=='E':\n",
    "                        new_state = (new_state[0], new_state[1]+1)\n",
    "                    ra=gw.state_values[new_state]\n",
    "            \n",
    "            # cambiar politica avanzar \n",
    "            subpolitica(a)\n",
    "            \n",
    "            # si recompensa auxiliar es 0\n",
    "            if ra==0:\n",
    "                # actualizar nuevo valor de estado\n",
    "                new_values[state]=(gamma*sum(stvalues)-0.6)/6\n",
    "            else:\n",
    "                # actualizar nuevo valor de estado\n",
    "                stvalues[5]=ra\n",
    "                new_values[state]=(gamma*sum(stvalues)-0.6)/6\n",
    "                ra=0\n",
    "    \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de función de valor para la política aleatoria es análogo al algoritmo de función de valor para la política de  avanzar, con la diferencia que este contempla a su vez la acción posibles de Retroceder y las 6 acciones reales que puede tomar, que dependen del dado, al retroceder.\n",
    "\n",
    "\n",
    "El algoritmo itera sobre todos los estados posibles y actualiza el valor de los estados de la siguiente manera: para estados terminales (bombas y metas) el valor del estado es la recompensa obtenida en el estado (+10 o -10), si el estado es una casilla de coordenadas de entrada de serpientes o escaleras asigna el valor de estado de las casillas de coordenadas de salida de de serpientes o escaleras, respectivamente. \n",
    "\n",
    "Para los estados no terminales, da un paso e itera sobre las 6 acciones reales posibles de la acción Avanzar que tiene hacia adelante (que depende de lo que sacó en el dado) y realiza la siguiente actualización:\n",
    "* si el estado siguiente es terminal y aún tiene pasos, para las acciones reales que generan estados terminales pero aun tiene pasos entonces se genera el rebote en las coordenadas (0,0) y (9,0), y cambia la política en estos estados a retroceder y avanzar respectivamente, haciendo uso de la función subpolitica(x) definida anteriormente y realiza una actualización \"manual\" de la política para el estado.\n",
    "\n",
    "Ahora realiza este mismo procedimiento, pero itera sobre las 6 acciones posibles de la acción Retroceder que tiene hacia atrás, que tambipen dependen del dado, finalmente realiza la suma total de valores de estado posibles de las acciones reales para Avanzar y Retroceder y realiza la actualización.\n",
    "* si el estado es no terminal realiza la actualización de la suma descontada de los valores de estado de las 12 acciones reales que puede tomar en la política aleatoria (Avanzar-Retroceder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de valor de estados: actualizar la función de valor de estados v(s) dada una politica pi\n",
    "# politica pi: accion es aleatoria (avanzar-retroceder)\n",
    "# parametros: gw=MDP, gamma=tasa de descuento\n",
    "def update_values2(gw, gamma):\n",
    "    # Arreglos para los valores\n",
    "    \n",
    "    # vectores de valor de estados v(s), vs'()\n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)\n",
    "     \n",
    "    stvalues=[0,0,0,0,0,0] # valor de estados avanzar\n",
    "    stvaluesr=[0,0,0,0,0,0] # valor de estados retroceder\n",
    "    \n",
    "    re=[0,0,0,0,0,0] # recompensa estados avanzar \n",
    "    rer=[0,0,0,0,0,0] # recompensa estados retroceder \n",
    "    ra=0 # recompensa auxiliar\n",
    "    sa=0 # estado auxiliar\n",
    "    \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:\n",
    "        # definir politica avanzar \n",
    "        subpolitica(a)\n",
    "        \n",
    "        stvalues=[0,0,0,0,0,0] # inicializar valores de estados para avanzar\n",
    "        stvaluesr=[0,0,0,0,0,0] # inicializar valores de estados para retroceder\n",
    "        rer=[0,0,0,0,0,0] # inicializar recompensas de estados de retroceder\n",
    "        re=[0,0,0,0,0,0]# inicializar valores de estados de avanzar\n",
    "        \n",
    "        new_state=state\n",
    "        \n",
    "        # si estado es terminal\n",
    "        if state in gw.goals or state in gw.pits:\n",
    "            # Tomar acción de politica dada\n",
    "            action = gw.policy[state]\n",
    "            # Dar un paso\n",
    "            new_state, reward, _, done = gw.step(state, action, random=False)\n",
    "            \n",
    "            # Actualizar valores\n",
    "            new_values[state] = reward # valor de estado terminal es recompensa\n",
    "        elif state in serp:\n",
    "            for s in serp:\n",
    "                if s==state:\n",
    "                    ind=serp.index(s)\n",
    "                    new_values[state]=gw.state_values[serpf[ind]]\n",
    "        elif state in esca:\n",
    "            for s in esca:\n",
    "                if s==state:\n",
    "                    ind=esca.index(s)\n",
    "                    new_values[state]=gw.state_values[escaf[ind]]\n",
    "        else:\n",
    "            \n",
    "            for i in range(len(re)):\n",
    "                action = gw.policy[new_state]\n",
    "                # Dar un paso\n",
    "                new_state, reward, ac, done = gw.step(new_state, action, random=False)\n",
    "                re[i]=reward\n",
    "                stvalues[i]=gw.state_values[new_state]\n",
    "                # Actualizar valores\n",
    "                if(new_state in gw.goals or new_state in gw.pits) and i<5:\n",
    "                    if new_state==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    if new_state==(9,0):\n",
    "                        subpolitica(a)\n",
    "                    # actualización para un estado terminal\n",
    "                    \n",
    "                    if gw.policy[new_state]=='N':\n",
    "                        new_state = (new_state[0]-1, new_state[1])\n",
    "                    if gw.policy[new_state]=='S':\n",
    "                        new_state = (new_state[0]+1, new_state[1])\n",
    "                    if gw.policy[new_state]=='W':\n",
    "                        new_state = (new_state[0], new_state[1]-1)\n",
    "                    if gw.policy[new_state]=='E':\n",
    "                        new_state = (new_state[0], new_state[1]+1)\n",
    "                    ra=gw.state_values[new_state]\n",
    "                    \n",
    "            subpolitica(r)\n",
    "            if ra!=0:\n",
    "                stvalues[5]=ra\n",
    "                ra=0\n",
    "            \n",
    "            new_state=state\n",
    "            for i in range(len(re)):\n",
    "                action = gw.policy[new_state]\n",
    "                # Dar un paso\n",
    "                new_state, reward, ac, done = gw.step(new_state, action, random=False)\n",
    "                re[i]=reward\n",
    "                stvaluesr[i]=gw.state_values[new_state]\n",
    "                # Actualizar valores\n",
    "                if new_state==(9,0):\n",
    "                        subpolitica(a)\n",
    "                if(new_state in gw.goals or new_state in gw.pits) and i<5:\n",
    "                    if new_state==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    if new_state==(9,0):\n",
    "                        subpolitica(a)\n",
    "                    # actualización para un estado terminal\n",
    "                    if gw.policy[new_state]=='N':\n",
    "                        new_state = (new_state[0]-1, new_state[1])\n",
    "                    if gw.policy[new_state]=='S':\n",
    "                        new_state = (new_state[0]+1, new_state[1])\n",
    "                    if gw.policy[new_state]=='W':\n",
    "                        new_state = (new_state[0], new_state[1]-1)\n",
    "                    if gw.policy[new_state]=='E':\n",
    "                        new_state = (new_state[0], new_state[1]+1)\n",
    "                    ra=gw.state_values[new_state]\n",
    "            \n",
    "            if ra==0:\n",
    "                av=sum(stvalues)\n",
    "                retro=sum(stvaluesr)\n",
    "                total=av+retro\n",
    "                new_values[state]=(gamma*retro-1.2)/12\n",
    "            else:\n",
    "                stvaluesr[5]=ra\n",
    "                av=sum(stvalues)\n",
    "                retro=sum(stvaluesr)\n",
    "                total=av+retro\n",
    "                new_values[state]=(gamma*retro-1.2)/12\n",
    "                ra=0\n",
    "    \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método permite aproximar los valores óptimos de los estados, de tal manera que se pueda encontrar, al final, una política $\\pi^{*}(s)$ que permita maximizar la recompensa recibida por el agente a lo largo del horizonte.\n",
    "\n",
    "$$V^{*}(s)=\\max_{\\pi} \\mathbb{E}\\left[ \\sum_{t=0}^{H} \\gamma^{t} R(s_t,a_t,s_{t+1}) | \\pi, s_0=s \\right]$$\n",
    "\n",
    "Cuando el horizonte es $H=0$, el valor de todos los estados es igual a su valor de inicialización, generalmente cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    ">Inicializar el valor de todos los estados como $V_{0}^{*}(s)=0$<br>\n",
    ">Para $k=1, \\cdots, H$:<br>\n",
    ">> Para todos los estados $s$:<br>\n",
    ">>> $\\displaystyle V_{k}^{*}(s)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{k-1}^{*}(s'))$<br>\n",
    ">>> $\\displaystyle \\pi_{k}^{*}(s)=arg\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{k-1}^{*}(s'))$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de value iteration funciona de manera análoga o similar al algoritmo de función de valor, con la diferencia de que al obtener los vectores de recompensa total de las acciones Avanzar y Retroceder (para sus 6 respectivas acciones reales) realiza una comparacion, actualiza el valor del estado con la suma de valor de estado para la acción que genera mayor retorno y actualiza la política óptima con la acción escogida para el estado.\n",
    "\n",
    "Entonces la diferencia fundamental en el algoritmo entre la función de valor y value iteration es que la función de valor (programación dinámica) actualiza el vector $v(s)$ mediante la suma descontada de recompensa todas las acciones posibles (para sus respectivas acciones reales), mientras que en value iteration se actualiza el valor del estado de manera greedy con respecto a la mayor suma de recompensa descontada de la acción que obtuvo mayor retorno, y actualiza la política escogiendo esa acción óptima, a lo largo del horizonte temporal. Por ende, la actualización de valor óptima en últimas se hace sobre 1/6 de probabilidad de acciones reales, ya que se escoge o la acción de Avanzar o Retroceder, en cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n",
    "\n",
    "# value_iteration: encontrar valores de estado óptimos para cada estado y politica óptima(acción óptima) para cada estado \n",
    "# params: gw=MDP gridworld, gamma=descuento\n",
    "def value_iteration(gw, gamma):\n",
    "    # Arreglos para los valores\n",
    "    \n",
    "    # vectores de valor de estados v(s), vs'()\n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)\n",
    "     \n",
    "    stvalues=[0,0,0,0,0,0] # valor de estados avanzar\n",
    "    stvaluesr=[0,0,0,0,0,0] # valor de estados retroceder\n",
    "    \n",
    "    re=[0,0,0,0,0,0] # recompensa estados avanzar \n",
    "    rer=[0,0,0,0,0,0] # recompensa estados retroceder \n",
    "    ra=[] # recompensa auxiliar\n",
    "    sa=0 # estado auxiliar\n",
    "    \n",
    "    \n",
    "    poli_op=[[]]\n",
    "    \n",
    "    for i in range (10):\n",
    "        # iterar sobre columnas\n",
    "        for j in range(10):\n",
    "            poli_op[i].append('')\n",
    "        poli_op.append([]) \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states:\n",
    "        \n",
    "        # definir politica avanzar \n",
    "        subpolitica(a)\n",
    "        \n",
    "        stvalues=[0,0,0,0,0,0] # inicializar valores de estados para avanzar\n",
    "        stvaluesr=[0,0,0,0,0,0] # inicializar valores de estados para retroceder\n",
    "        rer=[0,0,0,0,0,0] # inicializar recompensas de estados de retroceder\n",
    "        re=[0,0,0,0,0,0]# inicializar valores de estados de avanzar\n",
    "        \n",
    "        new_state=state\n",
    "        \n",
    "        # si estado es terminal\n",
    "        if state in gw.goals or state in gw.pits:\n",
    "            # Tomar acción de politica dada\n",
    "            action = gw.policy[state]\n",
    "            # Dar un paso\n",
    "            new_state, reward, _, done = gw.step(state, action, random=False)\n",
    "            \n",
    "            # Actualizar valores\n",
    "            new_values[state] = reward # valor de estado terminal es recompensa\n",
    "        \n",
    "        # si estado es serpiente\n",
    "        elif state in serp:\n",
    "            for s in serp:\n",
    "                if s==state:\n",
    "                    ind=serp.index(s)\n",
    "                    new_values[state]=gw.state_values[serpf[ind]]\n",
    "                    \n",
    "        # si estado es escalera \n",
    "        elif state in esca:\n",
    "            for s in esca:\n",
    "                if s==state:\n",
    "                    ind=esca.index(s)\n",
    "                    new_values[state]=gw.state_values[escaf[ind]]\n",
    "        else:\n",
    "            # si estado es no terminal\n",
    "                    \n",
    "            # iterar sobre recompensas de avanzar\n",
    "            for i in range(len(re)):\n",
    "                action = gw.policy[new_state]\n",
    "                # Dar un paso\n",
    "                new_state, reward, ac, done = gw.step(new_state, action, random=False)\n",
    "                re[i]=reward\n",
    "                stvalues[i]=gw.state_values[new_state]\n",
    "                # Actualizar valores\n",
    "                \n",
    "                # si estado siguiente es terminal Y quedan pasos\n",
    "                while(new_state in gw.goals or new_state in gw.pits) and i<5:\n",
    "                    if new_state==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    if new_state==(9,0):\n",
    "                        subpolitica(a)\n",
    "                  \n",
    "                    # actualización para un estado terminal\n",
    "                    if gw.policy[new_state]=='N':\n",
    "                        new_state = (new_state[0]-1, new_state[1])\n",
    "                    if gw.policy[new_state]=='S':\n",
    "                        new_state = (new_state[0]+1, new_state[1])\n",
    "                    if gw.policy[new_state]=='W':\n",
    "                        new_state = (new_state[0], new_state[1]-1)\n",
    "                    if gw.policy[new_state]=='E':\n",
    "                        new_state = (new_state[0], new_state[1]+1)\n",
    "                    ra.append(gw.state_values[new_state])\n",
    "                   \n",
    "            # definir politica retroceder\n",
    "            subpolitica(r)\n",
    "            if len(ra)!=0:\n",
    "                for i in range(len(ra)):\n",
    "                    stvalues[-i]=ra[i]\n",
    "                ra=[]\n",
    "            new_state=state\n",
    "            \n",
    "            # iterar sobre recompensas de retroceder\n",
    "            for i in range(len(rer)):\n",
    "                \n",
    "                action = gw.policy[new_state]\n",
    "                # Dar un paso\n",
    "                new_state, reward, ac, done = gw.step(new_state, action, random=False)\n",
    "                re[i]=reward\n",
    "                stvaluesr[i]=gw.state_values[new_state]\n",
    "                # Actualizar valores\n",
    "                if new_state==(9,0):\n",
    "                        subpolitica(a)\n",
    "                while(new_state in gw.goals or new_state in gw.pits) and i<5:\n",
    "                    if new_state==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    if new_state==(9,0):\n",
    "                        subpolitica(a)\n",
    "                    # actualización para un estado terminal\n",
    "                    if gw.policy[new_state]=='N':\n",
    "                        new_state = (new_state[0]-1, new_state[1])\n",
    "                    if gw.policy[new_state]=='S':\n",
    "                        new_state = (new_state[0]+1, new_state[1])\n",
    "                    if gw.policy[new_state]=='W':\n",
    "                        new_state = (new_state[0], new_state[1]-1)\n",
    "                    if gw.policy[new_state]=='E':\n",
    "                        new_state = (new_state[0], new_state[1]+1)\n",
    "                    ra.append(gw.state_values[new_state])\n",
    "            \n",
    "            # si ra=0, entonces estado siguiente es no terminal\n",
    "            \n",
    "            if len(ra)==0:\n",
    "                \n",
    "                av=sum(stvalues)\n",
    "                retro=sum(stvaluesr)\n",
    "                \n",
    "                if av>=retro:\n",
    "                    \n",
    "                    new_values[state]=(gamma*av-0.6)/6\n",
    "                    poli_op[state[0]][state[1]] = a[state[0]][state[1]]\n",
    "\n",
    "                else:\n",
    "                    new_values[state]=(gamma*retro-0.6)/6\n",
    "                    poli_op[state[0]][state[1]] = r[state[0]][state[1]]\n",
    "                    \n",
    "            else:\n",
    "                for i in range(len(ra)):\n",
    "                    stvaluesr[-i]=ra[i]    \n",
    "                av=sum(stvalues)\n",
    "                retro=sum(stvaluesr)\n",
    "                if av>=retro:\n",
    "                    new_values[state]=(gamma*av-0.6)/6\n",
    "                    poli_op[state[0]][state[1]] = a[state[0]][state[1]]\n",
    "\n",
    "                else:\n",
    "                    new_values[state]=(gamma*retro-0.6)/6\n",
    "                    poli_op[state[0]][state[1]] = r[state[0]][state[1]]\n",
    "                ra=[]\n",
    "            \n",
    "    if av>=retro:       \n",
    "        for s in serp:\n",
    "            poli_op[s[0]][s[1]]=a[s[0]][s[1]]\n",
    "        for e in esca:\n",
    "            poli_op[e[0]][e[1]]=a[e[0]][e[1]]\n",
    "        for g in gw.goals:\n",
    "            poli_op[g[0]][g[1]]=a[g[0]][g[1]]\n",
    "        for p in gw.pits:\n",
    "            poli_op[p[0]][p[1]]=a[p[0]][p[1]]\n",
    "        poli_op[0][0]='E'\n",
    "    else:\n",
    "        for s in serp:\n",
    "            poli_op[s[0]][s[1]]=a[s[0]][s[1]]\n",
    "        for e in esca:\n",
    "            poli_op[e[0]][e[1]]=a[e[0]][e[1]]\n",
    "        for g in gw.goals:\n",
    "            poli_op[g[0]][g[1]]=a[g[0]][g[1]]\n",
    "        for p in gw.pits:\n",
    "            poli_op[p[0]][p[1]]=a[p[0]][p[1]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    subpolitica(poli_op)\n",
    "    \n",
    "            \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escenarios\n",
    "\n",
    "A continuación se presentan los diferentes escenarios de Programación Dinámica y Value Iteration para evaluar los algoritmos que aproximan la función $V(s)$, para diferentes tasas de descuento $\\gamma$. Del mismo modo se presenta el valor del estado de la política evaluada y el diagrama que muestra para cada estado la función de valor aproximada.\n",
    "\n",
    "En los escenarios de value iteration se presenta gráficamente sobre el GridWorld (MDP) los valores óptimos de los estados así y se muestra para cada estado la acción óptima a la que converge el algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programación dinámica: Política siempre avanza\n",
    "\n",
    "## Tasa de descuento gamma=0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9 # tasa de descuento\n",
    "H = 100 # horizonte temporal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# resolver programación dinámica: función de valor de politica que siempre avanza  \n",
    "gw.update_values = MethodType(update_values, gw)\n",
    "gw.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = 10.000\n",
      "V(0, 1) = 2.210\n",
      "V(0, 2) = -0.598\n",
      "V(0, 3) = 2.216\n",
      "V(0, 4) = 2.216\n",
      "V(0, 5) = -0.644\n",
      "V(0, 6) = 2.210\n",
      "V(0, 7) = 1.041\n",
      "V(0, 8) = -0.124\n",
      "V(0, 9) = 0.937\n",
      "V(1, 0) = -0.147\n",
      "V(1, 1) = -0.067\n",
      "V(1, 2) = -0.659\n",
      "V(1, 3) = -0.005\n",
      "V(1, 4) = 0.118\n",
      "V(1, 5) = 0.086\n",
      "V(1, 6) = 0.211\n",
      "V(1, 7) = 0.471\n",
      "V(1, 8) = -1.000\n",
      "V(1, 9) = 0.745\n",
      "V(2, 0) = 10.000\n",
      "V(2, 1) = 1.321\n",
      "V(2, 2) = 1.506\n",
      "V(2, 3) = 1.714\n",
      "V(2, 4) = 1.972\n",
      "V(2, 5) = 2.367\n",
      "V(2, 6) = 2.732\n",
      "V(2, 7) = -0.751\n",
      "V(2, 8) = 1.331\n",
      "V(2, 9) = 1.305\n",
      "V(3, 0) = 0.007\n",
      "V(3, 1) = 2.216\n",
      "V(3, 2) = -0.258\n",
      "V(3, 3) = -0.644\n",
      "V(3, 4) = 0.192\n",
      "V(3, 5) = 0.211\n",
      "V(3, 6) = -1.000\n",
      "V(3, 7) = 0.702\n",
      "V(3, 8) = -0.515\n",
      "V(3, 9) = 1.243\n",
      "V(4, 0) = 0.158\n",
      "V(4, 1) = -0.664\n",
      "V(4, 2) = 0.022\n",
      "V(4, 3) = 0.122\n",
      "V(4, 4) = 0.179\n",
      "V(4, 5) = -0.712\n",
      "V(4, 6) = 1.041\n",
      "V(4, 7) = -0.102\n",
      "V(4, 8) = -0.688\n",
      "V(4, 9) = -0.124\n",
      "V(5, 0) = -0.292\n",
      "V(5, 1) = -0.346\n",
      "V(5, 2) = 1.714\n",
      "V(5, 3) = -0.719\n",
      "V(5, 4) = -1.000\n",
      "V(5, 5) = -0.721\n",
      "V(5, 6) = -0.209\n",
      "V(5, 7) = -0.703\n",
      "V(5, 8) = -0.047\n",
      "V(5, 9) = 0.937\n",
      "V(6, 0) = -0.305\n",
      "V(6, 1) = -0.242\n",
      "V(6, 2) = -0.128\n",
      "V(6, 3) = -1.000\n",
      "V(6, 4) = -0.447\n",
      "V(6, 5) = -0.462\n",
      "V(6, 6) = -0.488\n",
      "V(6, 7) = -0.515\n",
      "V(6, 8) = -0.556\n",
      "V(6, 9) = -0.620\n",
      "V(7, 0) = -0.067\n",
      "V(7, 1) = -0.719\n",
      "V(7, 2) = -1.000\n",
      "V(7, 3) = -0.644\n",
      "V(7, 4) = -0.641\n",
      "V(7, 5) = -0.630\n",
      "V(7, 6) = -0.615\n",
      "V(7, 7) = -0.598\n",
      "V(7, 8) = -0.581\n",
      "V(7, 9) = -0.563\n",
      "V(8, 0) = -0.655\n",
      "V(8, 1) = -0.659\n",
      "V(8, 2) = -0.662\n",
      "V(8, 3) = -0.664\n",
      "V(8, 4) = -0.614\n",
      "V(8, 5) = -0.598\n",
      "V(8, 6) = -0.678\n",
      "V(8, 7) = -0.681\n",
      "V(8, 8) = -0.685\n",
      "V(8, 9) = -0.688\n",
      "V(9, 0) = -0.751\n",
      "V(9, 1) = -0.736\n",
      "V(9, 2) = -0.731\n",
      "V(9, 3) = -0.726\n",
      "V(9, 4) = -0.721\n",
      "V(9, 5) = -0.716\n",
      "V(9, 6) = -0.712\n",
      "V(9, 7) = -0.630\n",
      "V(9, 8) = -0.703\n",
      "V(9, 9) = -0.692\n"
     ]
    }
   ],
   "source": [
    "# valor de estados de politica que siempre avanza  \n",
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/funcion_valor_avanzar1.jpeg\" alt=\"centered image\" width=600 height=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programación dinámica: Política siempre avanza\n",
    "\n",
    "## Tasa de descuento gamma=0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99 # tasa de descuento\n",
    "H = 100 # horizonte temporal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolver programación dinámica: función de valor de politica que siempre avanza  \n",
    "gw.update_values = MethodType(update_values, gw)\n",
    "gw.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = 10.000\n",
      "V(0, 1) = 2.525\n",
      "V(0, 2) = -0.794\n",
      "V(0, 3) = 2.540\n",
      "V(0, 4) = 2.540\n",
      "V(0, 5) = -0.901\n",
      "V(0, 6) = 2.525\n",
      "V(0, 7) = 1.292\n",
      "V(0, 8) = -0.150\n",
      "V(0, 9) = 1.195\n",
      "V(1, 0) = -0.044\n",
      "V(1, 1) = 0.061\n",
      "V(1, 2) = -0.849\n",
      "V(1, 3) = 0.169\n",
      "V(1, 4) = 0.314\n",
      "V(1, 5) = 0.248\n",
      "V(1, 6) = 0.396\n",
      "V(1, 7) = 0.698\n",
      "V(1, 8) = -1.000\n",
      "V(1, 9) = 0.973\n",
      "V(2, 0) = 10.000\n",
      "V(2, 1) = 1.541\n",
      "V(2, 2) = 1.754\n",
      "V(2, 3) = 1.991\n",
      "V(2, 4) = 2.292\n",
      "V(2, 5) = 2.810\n",
      "V(2, 6) = 3.264\n",
      "V(2, 7) = -1.172\n",
      "V(2, 8) = 1.705\n",
      "V(2, 9) = 1.697\n",
      "V(3, 0) = 0.101\n",
      "V(3, 1) = 2.540\n",
      "V(3, 2) = -0.226\n",
      "V(3, 3) = -0.901\n",
      "V(3, 4) = 0.408\n",
      "V(3, 5) = 0.396\n",
      "V(3, 6) = -1.000\n",
      "V(3, 7) = 0.973\n",
      "V(3, 8) = -0.638\n",
      "V(3, 9) = 1.648\n",
      "V(4, 0) = 0.283\n",
      "V(4, 1) = -0.834\n",
      "V(4, 2) = 0.059\n",
      "V(4, 3) = 0.217\n",
      "V(4, 4) = 0.290\n",
      "V(4, 5) = -1.038\n",
      "V(4, 6) = 1.292\n",
      "V(4, 7) = -0.102\n",
      "V(4, 8) = -0.961\n",
      "V(4, 9) = -0.150\n",
      "V(5, 0) = -0.400\n",
      "V(5, 1) = -0.486\n",
      "V(5, 2) = 1.991\n",
      "V(5, 3) = -0.968\n",
      "V(5, 4) = -1.000\n",
      "V(5, 5) = -1.074\n",
      "V(5, 6) = -0.280\n",
      "V(5, 7) = -1.010\n",
      "V(5, 8) = -0.061\n",
      "V(5, 9) = 1.195\n",
      "V(6, 0) = -0.419\n",
      "V(6, 1) = -0.311\n",
      "V(6, 2) = -0.198\n",
      "V(6, 3) = -1.000\n",
      "V(6, 4) = -0.564\n",
      "V(6, 5) = -0.577\n",
      "V(6, 6) = -0.607\n",
      "V(6, 7) = -0.638\n",
      "V(6, 8) = -0.691\n",
      "V(6, 9) = -0.773\n",
      "V(7, 0) = 0.061\n",
      "V(7, 1) = -0.968\n",
      "V(7, 2) = -1.000\n",
      "V(7, 3) = -0.901\n",
      "V(7, 4) = -0.883\n",
      "V(7, 5) = -0.856\n",
      "V(7, 6) = -0.825\n",
      "V(7, 7) = -0.794\n",
      "V(7, 8) = -0.763\n",
      "V(7, 9) = -0.735\n",
      "V(8, 0) = -0.850\n",
      "V(8, 1) = -0.849\n",
      "V(8, 2) = -0.844\n",
      "V(8, 3) = -0.834\n",
      "V(8, 4) = -0.807\n",
      "V(8, 5) = -0.780\n",
      "V(8, 6) = -0.919\n",
      "V(8, 7) = -0.931\n",
      "V(8, 8) = -0.944\n",
      "V(8, 9) = -0.961\n",
      "V(9, 0) = -1.172\n",
      "V(9, 1) = -1.127\n",
      "V(9, 2) = -1.111\n",
      "V(9, 3) = -1.092\n",
      "V(9, 4) = -1.074\n",
      "V(9, 5) = -1.055\n",
      "V(9, 6) = -1.038\n",
      "V(9, 7) = -0.856\n",
      "V(9, 8) = -1.010\n",
      "V(9, 9) = -0.981\n"
     ]
    }
   ],
   "source": [
    "# valor de estados de politica que siempre avanza  \n",
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/funcion_valor_avanzar2.jpeg\" alt=\"centered image\" width=600 height=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programación dinámica: Política aleatoria\n",
    "\n",
    "## Tasa de descuento gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.9 # tasa de descuento\n",
    "H= 100 # horizonte temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolver programación dinámica: función de valor de politica aleatoria\n",
    "gw.update_values = MethodType(update_values2, gw)\n",
    "gw.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = 10.000\n",
      "V(0, 1) = -0.184\n",
      "V(0, 2) = -0.237\n",
      "V(0, 3) = -0.174\n",
      "V(0, 4) = -0.161\n",
      "V(0, 5) = -0.177\n",
      "V(0, 6) = -0.199\n",
      "V(0, 7) = -0.174\n",
      "V(0, 8) = -0.242\n",
      "V(0, 9) = -0.033\n",
      "V(1, 0) = 0.579\n",
      "V(1, 1) = 0.636\n",
      "V(1, 2) = -0.182\n",
      "V(1, 3) = 0.699\n",
      "V(1, 4) = 0.766\n",
      "V(1, 5) = 0.837\n",
      "V(1, 6) = 0.150\n",
      "V(1, 7) = 0.118\n",
      "V(1, 8) = -1.000\n",
      "V(1, 9) = 0.018\n",
      "V(2, 0) = 10.000\n",
      "V(2, 1) = -0.186\n",
      "V(2, 2) = -0.189\n",
      "V(2, 3) = -0.191\n",
      "V(2, 4) = -0.193\n",
      "V(2, 5) = -0.193\n",
      "V(2, 6) = -0.195\n",
      "V(2, 7) = -0.182\n",
      "V(2, 8) = -0.228\n",
      "V(2, 9) = -0.224\n",
      "V(3, 0) = -0.181\n",
      "V(3, 1) = -0.161\n",
      "V(3, 2) = -0.180\n",
      "V(3, 3) = -0.177\n",
      "V(3, 4) = -0.180\n",
      "V(3, 5) = 0.150\n",
      "V(3, 6) = -1.000\n",
      "V(3, 7) = -0.216\n",
      "V(3, 8) = -0.192\n",
      "V(3, 9) = -0.221\n",
      "V(4, 0) = -0.181\n",
      "V(4, 1) = -0.182\n",
      "V(4, 2) = -0.181\n",
      "V(4, 3) = -0.185\n",
      "V(4, 4) = -0.175\n",
      "V(4, 5) = -0.182\n",
      "V(4, 6) = -0.174\n",
      "V(4, 7) = -0.185\n",
      "V(4, 8) = -0.181\n",
      "V(4, 9) = -0.242\n",
      "V(5, 0) = -0.260\n",
      "V(5, 1) = -0.265\n",
      "V(5, 2) = -0.191\n",
      "V(5, 3) = -0.120\n",
      "V(5, 4) = -1.000\n",
      "V(5, 5) = -0.182\n",
      "V(5, 6) = -0.251\n",
      "V(5, 7) = -0.181\n",
      "V(5, 8) = -0.244\n",
      "V(5, 9) = -0.033\n",
      "V(6, 0) = -0.255\n",
      "V(6, 1) = -0.251\n",
      "V(6, 2) = -0.246\n",
      "V(6, 3) = -1.000\n",
      "V(6, 4) = -0.186\n",
      "V(6, 5) = -0.190\n",
      "V(6, 6) = -0.193\n",
      "V(6, 7) = -0.192\n",
      "V(6, 8) = -0.191\n",
      "V(6, 9) = -0.190\n",
      "V(7, 0) = 0.636\n",
      "V(7, 1) = -0.120\n",
      "V(7, 2) = -1.000\n",
      "V(7, 3) = -0.177\n",
      "V(7, 4) = -0.177\n",
      "V(7, 5) = -0.176\n",
      "V(7, 6) = -0.176\n",
      "V(7, 7) = -0.237\n",
      "V(7, 8) = -0.246\n",
      "V(7, 9) = -0.189\n",
      "V(8, 0) = -0.182\n",
      "V(8, 1) = -0.182\n",
      "V(8, 2) = -0.182\n",
      "V(8, 3) = -0.182\n",
      "V(8, 4) = -0.182\n",
      "V(8, 5) = -0.182\n",
      "V(8, 6) = -0.181\n",
      "V(8, 7) = -0.181\n",
      "V(8, 8) = -0.181\n",
      "V(8, 9) = -0.181\n",
      "V(9, 0) = -0.182\n",
      "V(9, 1) = -0.182\n",
      "V(9, 2) = -0.182\n",
      "V(9, 3) = -0.182\n",
      "V(9, 4) = -0.182\n",
      "V(9, 5) = -0.182\n",
      "V(9, 6) = -0.182\n",
      "V(9, 7) = -0.176\n",
      "V(9, 8) = -0.181\n",
      "V(9, 9) = -0.181\n"
     ]
    }
   ],
   "source": [
    "# valor de estados de politica aleatoria \n",
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/funcion_valor_aleatorio1.jpeg\" alt=\"centered image\" width=600 height=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programación dinámica: Política aleatoria\n",
    "\n",
    "## Tasa de descuento gamma=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99 # tasa de descuento\n",
    "H= 100 # horizonte temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolver programación dinámica: función de valor de politica aleatoria\n",
    "gw.update_values = MethodType(update_values2, gw)\n",
    "gw.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = 10.000\n",
      "V(0, 1) = -0.194\n",
      "V(0, 2) = -0.252\n",
      "V(0, 3) = -0.179\n",
      "V(0, 4) = -0.161\n",
      "V(0, 5) = -0.183\n",
      "V(0, 6) = -0.197\n",
      "V(0, 7) = -0.166\n",
      "V(0, 8) = -0.259\n",
      "V(0, 9) = 0.005\n",
      "V(1, 0) = 0.640\n",
      "V(1, 1) = 0.710\n",
      "V(1, 2) = -0.197\n",
      "V(1, 3) = 0.786\n",
      "V(1, 4) = 0.868\n",
      "V(1, 5) = 0.957\n",
      "V(1, 6) = 0.210\n",
      "V(1, 7) = 0.175\n",
      "V(1, 8) = -1.000\n",
      "V(1, 9) = 0.065\n",
      "V(2, 0) = 10.000\n",
      "V(2, 1) = -0.202\n",
      "V(2, 2) = -0.205\n",
      "V(2, 3) = -0.207\n",
      "V(2, 4) = -0.209\n",
      "V(2, 5) = -0.209\n",
      "V(2, 6) = -0.210\n",
      "V(2, 7) = -0.198\n",
      "V(2, 8) = -0.239\n",
      "V(2, 9) = -0.236\n",
      "V(3, 0) = -0.196\n",
      "V(3, 1) = -0.161\n",
      "V(3, 2) = -0.194\n",
      "V(3, 3) = -0.183\n",
      "V(3, 4) = -0.193\n",
      "V(3, 5) = 0.210\n",
      "V(3, 6) = -1.000\n",
      "V(3, 7) = -0.225\n",
      "V(3, 8) = -0.208\n",
      "V(3, 9) = -0.232\n",
      "V(4, 0) = -0.194\n",
      "V(4, 1) = -0.197\n",
      "V(4, 2) = -0.194\n",
      "V(4, 3) = -0.199\n",
      "V(4, 4) = -0.184\n",
      "V(4, 5) = -0.198\n",
      "V(4, 6) = -0.166\n",
      "V(4, 7) = -0.198\n",
      "V(4, 8) = -0.196\n",
      "V(4, 9) = -0.259\n",
      "V(5, 0) = -0.284\n",
      "V(5, 1) = -0.290\n",
      "V(5, 2) = -0.207\n",
      "V(5, 3) = -0.123\n",
      "V(5, 4) = -1.000\n",
      "V(5, 5) = -0.198\n",
      "V(5, 6) = -0.273\n",
      "V(5, 7) = -0.197\n",
      "V(5, 8) = -0.265\n",
      "V(5, 9) = 0.005\n",
      "V(6, 0) = -0.278\n",
      "V(6, 1) = -0.273\n",
      "V(6, 2) = -0.268\n",
      "V(6, 3) = -1.000\n",
      "V(6, 4) = -0.202\n",
      "V(6, 5) = -0.207\n",
      "V(6, 6) = -0.210\n",
      "V(6, 7) = -0.208\n",
      "V(6, 8) = -0.206\n",
      "V(6, 9) = -0.204\n",
      "V(7, 0) = 0.710\n",
      "V(7, 1) = -0.123\n",
      "V(7, 2) = -1.000\n",
      "V(7, 3) = -0.183\n",
      "V(7, 4) = -0.182\n",
      "V(7, 5) = -0.180\n",
      "V(7, 6) = -0.179\n",
      "V(7, 7) = -0.252\n",
      "V(7, 8) = -0.263\n",
      "V(7, 9) = -0.202\n",
      "V(8, 0) = -0.198\n",
      "V(8, 1) = -0.197\n",
      "V(8, 2) = -0.197\n",
      "V(8, 3) = -0.197\n",
      "V(8, 4) = -0.197\n",
      "V(8, 5) = -0.197\n",
      "V(8, 6) = -0.196\n",
      "V(8, 7) = -0.196\n",
      "V(8, 8) = -0.196\n",
      "V(8, 9) = -0.196\n",
      "V(9, 0) = -0.198\n",
      "V(9, 1) = -0.198\n",
      "V(9, 2) = -0.198\n",
      "V(9, 3) = -0.198\n",
      "V(9, 4) = -0.198\n",
      "V(9, 5) = -0.198\n",
      "V(9, 6) = -0.198\n",
      "V(9, 7) = -0.180\n",
      "V(9, 8) = -0.197\n",
      "V(9, 9) = -0.196\n"
     ]
    }
   ],
   "source": [
    "# valor de estados de politica aleatoria\n",
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/funcion_valor_aleatorio2.jpeg\" alt=\"centered image\" width=600 height=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "## Tasa de descuento gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.9 # tasa de descuento\n",
    "H= 100 # horizonte temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# resolver value iteration: calcular valores de estados óptimos v*(s) y politica óptima \n",
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=(9,0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = E\n",
      "V(0, 1) = W\n",
      "V(0, 2) = W\n",
      "V(0, 3) = W\n",
      "V(0, 4) = W\n",
      "V(0, 5) = W\n",
      "V(0, 6) = W\n",
      "V(0, 7) = W\n",
      "V(0, 8) = W\n",
      "V(0, 9) = S\n",
      "V(1, 0) = S\n",
      "V(1, 1) = W\n",
      "V(1, 2) = E\n",
      "V(1, 3) = W\n",
      "V(1, 4) = W\n",
      "V(1, 5) = W\n",
      "V(1, 6) = W\n",
      "V(1, 7) = W\n",
      "V(1, 8) = E\n",
      "V(1, 9) = W\n",
      "V(2, 0) = N\n",
      "V(2, 1) = W\n",
      "V(2, 2) = W\n",
      "V(2, 3) = W\n",
      "V(2, 4) = W\n",
      "V(2, 5) = W\n",
      "V(2, 6) = W\n",
      "V(2, 7) = W\n",
      "V(2, 8) = W\n",
      "V(2, 9) = W\n",
      "V(3, 0) = E\n",
      "V(3, 1) = E\n",
      "V(3, 2) = W\n",
      "V(3, 3) = E\n",
      "V(3, 4) = E\n",
      "V(3, 5) = E\n",
      "V(3, 6) = E\n",
      "V(3, 7) = E\n",
      "V(3, 8) = E\n",
      "V(3, 9) = N\n",
      "V(4, 0) = N\n",
      "V(4, 1) = W\n",
      "V(4, 2) = W\n",
      "V(4, 3) = W\n",
      "V(4, 4) = W\n",
      "V(4, 5) = W\n",
      "V(4, 6) = W\n",
      "V(4, 7) = W\n",
      "V(4, 8) = W\n",
      "V(4, 9) = W\n",
      "V(5, 0) = E\n",
      "V(5, 1) = E\n",
      "V(5, 2) = E\n",
      "V(5, 3) = E\n",
      "V(5, 4) = E\n",
      "V(5, 5) = E\n",
      "V(5, 6) = E\n",
      "V(5, 7) = E\n",
      "V(5, 8) = E\n",
      "V(5, 9) = N\n",
      "V(6, 0) = N\n",
      "V(6, 1) = W\n",
      "V(6, 2) = W\n",
      "V(6, 3) = W\n",
      "V(6, 4) = W\n",
      "V(6, 5) = W\n",
      "V(6, 6) = E\n",
      "V(6, 7) = E\n",
      "V(6, 8) = E\n",
      "V(6, 9) = S\n",
      "V(7, 0) = E\n",
      "V(7, 1) = W\n",
      "V(7, 2) = E\n",
      "V(7, 3) = W\n",
      "V(7, 4) = W\n",
      "V(7, 5) = W\n",
      "V(7, 6) = W\n",
      "V(7, 7) = W\n",
      "V(7, 8) = W\n",
      "V(7, 9) = W\n",
      "V(8, 0) = N\n",
      "V(8, 1) = W\n",
      "V(8, 2) = W\n",
      "V(8, 3) = W\n",
      "V(8, 4) = W\n",
      "V(8, 5) = W\n",
      "V(8, 6) = W\n",
      "V(8, 7) = W\n",
      "V(8, 8) = W\n",
      "V(8, 9) = W\n",
      "V(9, 0) = E\n",
      "V(9, 1) = E\n",
      "V(9, 2) = E\n",
      "V(9, 3) = E\n",
      "V(9, 4) = E\n",
      "V(9, 5) = E\n",
      "V(9, 6) = E\n",
      "V(9, 7) = E\n",
      "V(9, 8) = E\n",
      "V(9, 9) = N\n"
     ]
    }
   ],
   "source": [
    "# valor de estados de politica aleatoria\n",
    "for state in gw.policy:\n",
    "    print(f'V{state} = {gw.policy[state]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/value1.jpg\" alt=\"centered image\" width=600 height=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "## Tasa de descuento gamma=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99 # tasa de descuento\n",
    "H= 100 # horizonte temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  3\n",
      "llegue a  (9, 3)\n",
      "estaba en el estado (9, 3)\n",
      "saque en el dado:  3\n",
      "llegue a  (9, 6)\n",
      "estaba en el estado (9, 6)\n",
      "saque en el dado:  4\n",
      "llegue a  (8, 9)\n",
      "estaba en el estado (8, 9)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 6)\n",
      "estaba en el estado (8, 6)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  5\n",
      "llegue a  (8, 8)\n",
      "estaba en el estado (8, 8)\n",
      "saque en el dado:  6\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 5)\n",
      "estaba en el estado (8, 5)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  5\n",
      "llegue a  (8, 8)\n",
      "estaba en el estado (8, 8)\n",
      "saque en el dado:  1\n",
      "llegue a  (8, 7)\n",
      "estaba en el estado (8, 7)\n",
      "saque en el dado:  1\n",
      "llegue a  (8, 6)\n",
      "estaba en el estado (8, 6)\n",
      "saque en el dado:  5\n",
      "llegue a  (8, 1)\n",
      "estaba en el estado (8, 1)\n",
      "saque en el dado:  5\n",
      "llegue a  (8, 6)\n",
      "estaba en el estado (8, 6)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 5)\n",
      "estaba en el estado (8, 5)\n",
      "saque en el dado:  4\n",
      "llegue a  (8, 1)\n",
      "estaba en el estado (8, 1)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 5)\n",
      "estaba en el estado (8, 5)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 6)\n",
      "estaba en el estado (8, 6)\n",
      "saque en el dado:  5\n",
      "llegue a  (8, 1)\n",
      "estaba en el estado (8, 1)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 5)\n",
      "estaba en el estado (8, 5)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 6)\n",
      "estaba en el estado (8, 6)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  4\n",
      "llegue a  (8, 7)\n",
      "estaba en el estado (8, 7)\n",
      "saque en el dado:  6\n",
      "llegue a  (8, 1)\n",
      "estaba en el estado (8, 1)\n",
      "saque en el dado:  1\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 4)\n",
      "estaba en el estado (8, 4)\n",
      "saque en el dado:  5\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  1\n",
      "llegue a  (1, 0)\n",
      "estaba en el estado (1, 0)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 4)\n",
      "estaba en el estado (2, 4)\n",
      "saque en el dado:  1\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  6\n",
      "llegue a  (1, 3)\n",
      "estaba en el estado (1, 3)\n",
      "saque en el dado:  1\n",
      "llegue a  (1, 2)\n",
      "estaba en el estado (1, 2)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  4\n",
      "llegue a  (8, 7)\n",
      "estaba en el estado (8, 7)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 4)\n",
      "estaba en el estado (8, 4)\n",
      "saque en el dado:  5\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  6\n",
      "llegue a  (1, 3)\n",
      "estaba en el estado (1, 3)\n",
      "saque en el dado:  2\n",
      "llegue a  (1, 1)\n",
      "estaba en el estado (1, 1)\n",
      "saque en el dado:  5\n",
      "llegue a  (2, 4)\n",
      "estaba en el estado (2, 4)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 0)\n",
      "estaba en el estado (2, 0)\n",
      "llegue a  Terminal Diamante\n"
     ]
    }
   ],
   "source": [
    "# resolver value iteration: calcular valores de estados óptimos v*(s) y politica óptima \n",
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=(9,0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = E\n",
      "V(0, 1) = W\n",
      "V(0, 2) = W\n",
      "V(0, 3) = W\n",
      "V(0, 4) = W\n",
      "V(0, 5) = W\n",
      "V(0, 6) = W\n",
      "V(0, 7) = W\n",
      "V(0, 8) = W\n",
      "V(0, 9) = S\n",
      "V(1, 0) = S\n",
      "V(1, 1) = W\n",
      "V(1, 2) = E\n",
      "V(1, 3) = W\n",
      "V(1, 4) = W\n",
      "V(1, 5) = W\n",
      "V(1, 6) = W\n",
      "V(1, 7) = W\n",
      "V(1, 8) = E\n",
      "V(1, 9) = W\n",
      "V(2, 0) = N\n",
      "V(2, 1) = W\n",
      "V(2, 2) = W\n",
      "V(2, 3) = W\n",
      "V(2, 4) = W\n",
      "V(2, 5) = W\n",
      "V(2, 6) = W\n",
      "V(2, 7) = W\n",
      "V(2, 8) = W\n",
      "V(2, 9) = W\n",
      "V(3, 0) = S\n",
      "V(3, 1) = E\n",
      "V(3, 2) = W\n",
      "V(3, 3) = E\n",
      "V(3, 4) = E\n",
      "V(3, 5) = E\n",
      "V(3, 6) = E\n",
      "V(3, 7) = E\n",
      "V(3, 8) = E\n",
      "V(3, 9) = N\n",
      "V(4, 0) = N\n",
      "V(4, 1) = W\n",
      "V(4, 2) = W\n",
      "V(4, 3) = W\n",
      "V(4, 4) = W\n",
      "V(4, 5) = W\n",
      "V(4, 6) = W\n",
      "V(4, 7) = E\n",
      "V(4, 8) = W\n",
      "V(4, 9) = W\n",
      "V(5, 0) = E\n",
      "V(5, 1) = E\n",
      "V(5, 2) = E\n",
      "V(5, 3) = E\n",
      "V(5, 4) = E\n",
      "V(5, 5) = E\n",
      "V(5, 6) = E\n",
      "V(5, 7) = E\n",
      "V(5, 8) = E\n",
      "V(5, 9) = N\n",
      "V(6, 0) = N\n",
      "V(6, 1) = W\n",
      "V(6, 2) = W\n",
      "V(6, 3) = W\n",
      "V(6, 4) = E\n",
      "V(6, 5) = E\n",
      "V(6, 6) = E\n",
      "V(6, 7) = E\n",
      "V(6, 8) = E\n",
      "V(6, 9) = S\n",
      "V(7, 0) = E\n",
      "V(7, 1) = W\n",
      "V(7, 2) = E\n",
      "V(7, 3) = W\n",
      "V(7, 4) = W\n",
      "V(7, 5) = W\n",
      "V(7, 6) = W\n",
      "V(7, 7) = E\n",
      "V(7, 8) = E\n",
      "V(7, 9) = W\n",
      "V(8, 0) = E\n",
      "V(8, 1) = E\n",
      "V(8, 2) = E\n",
      "V(8, 3) = E\n",
      "V(8, 4) = W\n",
      "V(8, 5) = W\n",
      "V(8, 6) = W\n",
      "V(8, 7) = W\n",
      "V(8, 8) = W\n",
      "V(8, 9) = W\n",
      "V(9, 0) = E\n",
      "V(9, 1) = E\n",
      "V(9, 2) = E\n",
      "V(9, 3) = E\n",
      "V(9, 4) = E\n",
      "V(9, 5) = E\n",
      "V(9, 6) = E\n",
      "V(9, 7) = E\n",
      "V(9, 8) = E\n",
      "V(9, 9) = N\n"
     ]
    }
   ],
   "source": [
    "# valor de estados de politica aleatoria\n",
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.policy[state]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/value2.jpg\" alt=\"centered image\" width=600 height=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FALTA VARIACIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variación 1: Value Iteration\n",
    "# Tasa de descuento  gamma=0.9\n",
    "# 2 casillas azules (meta) aleatorias en primeras 2 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "# crear MDP: Serpientes y Escaleras\n",
    "# params: rows=filas, cols=columnas, walls=celda bloqueada, pits=bombas, goals=meta, live_reward=0\n",
    "\n",
    "bombas = [(7,2),(6,3),(5,4),(3,6),(1,8)]\n",
    "\n",
    "estados_filas1_2 = [(0,0),(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7),(0,8),(0,9),\n",
    "                   (1,0),(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(1,7),(1,8),(1,9)]\n",
    "\n",
    "nuevas_metas = []\n",
    "\n",
    "nuevas_metas = random.sample(estados_filas1_2,k=2)\n",
    "\n",
    "print(nuevas_metas)\n",
    "\n",
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=bombas, goals=nuevas_metas, live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.9 # tasa de descuento\n",
    "H= 100 # horizonte temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  4\n",
      "llegue a  (9, 4)\n",
      "estaba en el estado (9, 4)\n",
      "saque en el dado:  1\n",
      "llegue a  (9, 5)\n",
      "estaba en el estado (9, 5)\n",
      "saque en el dado:  4\n",
      "llegue a  (9, 9)\n",
      "estaba en el estado (9, 9)\n",
      "saque en el dado:  6\n",
      "llegue a  (8, 3)\n",
      "estaba en el estado (8, 3)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 0)\n",
      "estaba en el estado (8, 0)\n",
      "saque en el dado:  4\n",
      "llegue a  (8, 4)\n",
      "estaba en el estado (8, 4)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  3\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  4\n",
      "llegue a  (1, 2)\n",
      "estaba en el estado (1, 2)\n",
      "saque en el dado:  1\n",
      "llegue a  (8, 0)\n",
      "estaba en el estado (8, 0)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  1\n",
      "llegue a  (8, 1)\n",
      "estaba en el estado (8, 1)\n",
      "saque en el dado:  3\n",
      "llegue a  (7, 2)\n",
      "estaba en el estado (7, 2)\n",
      "llegue a  Terminal Bomba\n"
     ]
    }
   ],
   "source": [
    "# resolver value iteration: calcular valores de estados óptimos v*(s) y politica óptima \n",
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=(9,0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = E\n",
      "V(0, 1) = E\n",
      "V(0, 2) = W\n",
      "V(0, 3) = W\n",
      "V(0, 4) = W\n",
      "V(0, 5) = W\n",
      "V(0, 6) = W\n",
      "V(0, 7) = E\n",
      "V(0, 8) = W\n",
      "V(0, 9) = S\n",
      "V(1, 0) = S\n",
      "V(1, 1) = W\n",
      "V(1, 2) = E\n",
      "V(1, 3) = W\n",
      "V(1, 4) = W\n",
      "V(1, 5) = W\n",
      "V(1, 6) = W\n",
      "V(1, 7) = W\n",
      "V(1, 8) = E\n",
      "V(1, 9) = W\n",
      "V(2, 0) = N\n",
      "V(2, 1) = E\n",
      "V(2, 2) = W\n",
      "V(2, 3) = W\n",
      "V(2, 4) = W\n",
      "V(2, 5) = W\n",
      "V(2, 6) = W\n",
      "V(2, 7) = W\n",
      "V(2, 8) = W\n",
      "V(2, 9) = W\n",
      "V(3, 0) = E\n",
      "V(3, 1) = E\n",
      "V(3, 2) = W\n",
      "V(3, 3) = E\n",
      "V(3, 4) = E\n",
      "V(3, 5) = E\n",
      "V(3, 6) = E\n",
      "V(3, 7) = E\n",
      "V(3, 8) = E\n",
      "V(3, 9) = N\n",
      "V(4, 0) = N\n",
      "V(4, 1) = W\n",
      "V(4, 2) = W\n",
      "V(4, 3) = W\n",
      "V(4, 4) = W\n",
      "V(4, 5) = W\n",
      "V(4, 6) = W\n",
      "V(4, 7) = W\n",
      "V(4, 8) = W\n",
      "V(4, 9) = W\n",
      "V(5, 0) = E\n",
      "V(5, 1) = E\n",
      "V(5, 2) = E\n",
      "V(5, 3) = E\n",
      "V(5, 4) = E\n",
      "V(5, 5) = E\n",
      "V(5, 6) = E\n",
      "V(5, 7) = E\n",
      "V(5, 8) = E\n",
      "V(5, 9) = N\n",
      "V(6, 0) = N\n",
      "V(6, 1) = W\n",
      "V(6, 2) = W\n",
      "V(6, 3) = W\n",
      "V(6, 4) = W\n",
      "V(6, 5) = E\n",
      "V(6, 6) = E\n",
      "V(6, 7) = E\n",
      "V(6, 8) = E\n",
      "V(6, 9) = S\n",
      "V(7, 0) = E\n",
      "V(7, 1) = W\n",
      "V(7, 2) = E\n",
      "V(7, 3) = W\n",
      "V(7, 4) = W\n",
      "V(7, 5) = W\n",
      "V(7, 6) = W\n",
      "V(7, 7) = W\n",
      "V(7, 8) = W\n",
      "V(7, 9) = W\n",
      "V(8, 0) = E\n",
      "V(8, 1) = W\n",
      "V(8, 2) = W\n",
      "V(8, 3) = W\n",
      "V(8, 4) = W\n",
      "V(8, 5) = W\n",
      "V(8, 6) = W\n",
      "V(8, 7) = W\n",
      "V(8, 8) = W\n",
      "V(8, 9) = W\n",
      "V(9, 0) = E\n",
      "V(9, 1) = E\n",
      "V(9, 2) = E\n",
      "V(9, 3) = E\n",
      "V(9, 4) = E\n",
      "V(9, 5) = E\n",
      "V(9, 6) = E\n",
      "V(9, 7) = E\n",
      "V(9, 8) = E\n",
      "V(9, 9) = N\n"
     ]
    }
   ],
   "source": [
    "# valor de estados de politica aleatoria\n",
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.policy[state]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/var1.jpg\" alt=\"centered image\" width=600 height=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variación 2: Value Iteration\n",
    "# Tasa de descuento  gamma=0.9\n",
    "# 7 casillas rojas (bombas) aleatorias en primeras 9 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear MDP: Serpientes y Escaleras\n",
    "# params: rows=filas, cols=columnas, walls=celda bloqueada, pits=bombas, goals=meta, live_reward=0\n",
    "\n",
    "metas = [(0,0),(2,0)]\n",
    "\n",
    "estados_filas10 = [(9,0),(9,1),(9,2),(9,3),(9,4),(9,5),(9,6),(9,7),(9,8),(9,9)]\n",
    "\n",
    "estados_posibles = []\n",
    "nuevas_bombas = []\n",
    "\n",
    "for state in gw.states:\n",
    "    if state not in estados_filas10 and state not in bombas and state not in metas and state not in serp and state not in serpf and state not in esca and state not in escaf:\n",
    "        estados_posibles.append(state)\n",
    "\n",
    "nuevas_bombas = random.sample(estados_posibles,k=7)\n",
    "\n",
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=nuevas_bombas, goals=metas, live_reward=0,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.9 # tasa de descuento\n",
    "H= 100 # horizonte temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  6\n",
      "llegue a  (9, 6)\n",
      "estaba en el estado (9, 6)\n",
      "saque en el dado:  1\n",
      "llegue a  (9, 5)\n",
      "estaba en el estado (9, 5)\n",
      "saque en el dado:  1\n",
      "llegue a  (9, 6)\n",
      "estaba en el estado (9, 6)\n",
      "saque en el dado:  3\n",
      "llegue a  (9, 3)\n",
      "estaba en el estado (9, 3)\n",
      "saque en el dado:  4\n",
      "llegue a  (9, 7)\n",
      "estaba en el estado (9, 7)\n",
      "saque en el dado:  5\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  5\n",
      "llegue a  (2, 4)\n",
      "estaba en el estado (2, 4)\n",
      "saque en el dado:  6\n",
      "llegue a  (1, 2)\n",
      "estaba en el estado (1, 2)\n",
      "saque en el dado:  4\n",
      "llegue a  (7, 3)\n",
      "estaba en el estado (7, 3)\n",
      "saque en el dado:  4\n",
      "llegue a  (8, 0)\n",
      "estaba en el estado (8, 0)\n",
      "saque en el dado:  1\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  6\n",
      "llegue a  (2, 5)\n",
      "estaba en el estado (2, 5)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  6\n",
      "llegue a  (1, 4)\n",
      "estaba en el estado (1, 4)\n",
      "saque en el dado:  2\n",
      "llegue a  (1, 2)\n",
      "estaba en el estado (1, 2)\n",
      "saque en el dado:  5\n",
      "llegue a  (7, 4)\n",
      "estaba en el estado (7, 4)\n",
      "saque en el dado:  1\n",
      "llegue a  (7, 3)\n",
      "estaba en el estado (7, 3)\n",
      "saque en el dado:  1\n",
      "llegue a  (7, 2)\n",
      "estaba en el estado (7, 2)\n",
      "saque en el dado:  1\n",
      "llegue a  (7, 1)\n",
      "estaba en el estado (7, 1)\n",
      "saque en el dado:  3\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  3\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  5\n",
      "llegue a  (2, 4)\n",
      "estaba en el estado (2, 4)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 1)\n",
      "estaba en el estado (2, 1)\n",
      "saque en el dado:  1\n",
      "llegue a  (2, 0)\n",
      "estaba en el estado (2, 0)\n",
      "llegue a  Terminal Diamante\n"
     ]
    }
   ],
   "source": [
    "# resolver value iteration: calcular valores de estados óptimos v*(s) y politica óptima \n",
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=(9,0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = 10.000\n",
      "V(0, 1) = 1.456\n",
      "V(0, 2) = 1.925\n",
      "V(0, 3) = -1.000\n",
      "V(0, 4) = 2.840\n",
      "V(0, 5) = 2.250\n",
      "V(0, 6) = 2.401\n",
      "V(0, 7) = 2.521\n",
      "V(0, 8) = 1.361\n",
      "V(0, 9) = 2.640\n",
      "V(1, 0) = 3.578\n",
      "V(1, 1) = 4.416\n",
      "V(1, 2) = 2.250\n",
      "V(1, 3) = 4.362\n",
      "V(1, 4) = 4.075\n",
      "V(1, 5) = 4.202\n",
      "V(1, 6) = 3.332\n",
      "V(1, 7) = -1.000\n",
      "V(1, 8) = 3.296\n",
      "V(1, 9) = 2.483\n",
      "V(2, 0) = 10.000\n",
      "V(2, 1) = 3.296\n",
      "V(2, 2) = 4.296\n",
      "V(2, 3) = 4.160\n",
      "V(2, 4) = 4.193\n",
      "V(2, 5) = 4.162\n",
      "V(2, 6) = 4.416\n",
      "V(2, 7) = 0.570\n",
      "V(2, 8) = 3.169\n",
      "V(2, 9) = 3.001\n",
      "V(3, 0) = 1.519\n",
      "V(3, 1) = 2.840\n",
      "V(3, 2) = -1.000\n",
      "V(3, 3) = 2.250\n",
      "V(3, 4) = 2.109\n",
      "V(3, 5) = 3.332\n",
      "V(3, 6) = 1.887\n",
      "V(3, 7) = 2.217\n",
      "V(3, 8) = 1.462\n",
      "V(3, 9) = 2.827\n",
      "V(4, 0) = 1.613\n",
      "V(4, 1) = 2.250\n",
      "V(4, 2) = 1.300\n",
      "V(4, 3) = 1.178\n",
      "V(4, 4) = 1.505\n",
      "V(4, 5) = 0.570\n",
      "V(4, 6) = 2.521\n",
      "V(4, 7) = -1.000\n",
      "V(4, 8) = 1.388\n",
      "V(4, 9) = 1.361\n",
      "V(5, 0) = 1.808\n",
      "V(5, 1) = 1.675\n",
      "V(5, 2) = 4.160\n",
      "V(5, 3) = 2.250\n",
      "V(5, 4) = 1.947\n",
      "V(5, 5) = 0.880\n",
      "V(5, 6) = 1.808\n",
      "V(5, 7) = 0.789\n",
      "V(5, 8) = 1.675\n",
      "V(5, 9) = 2.640\n",
      "V(6, 0) = 1.808\n",
      "V(6, 1) = 1.947\n",
      "V(6, 2) = 1.947\n",
      "V(6, 3) = 1.902\n",
      "V(6, 4) = 1.563\n",
      "V(6, 5) = 1.546\n",
      "V(6, 6) = 1.507\n",
      "V(6, 7) = 1.462\n",
      "V(6, 8) = 1.389\n",
      "V(6, 9) = 1.388\n",
      "V(7, 0) = 4.416\n",
      "V(7, 1) = 2.250\n",
      "V(7, 2) = 2.250\n",
      "V(7, 3) = 2.250\n",
      "V(7, 4) = 2.250\n",
      "V(7, 5) = 2.250\n",
      "V(7, 6) = 2.250\n",
      "V(7, 7) = 1.925\n",
      "V(7, 8) = -1.000\n",
      "V(7, 9) = 1.876\n",
      "V(8, 0) = 2.250\n",
      "V(8, 1) = 2.250\n",
      "V(8, 2) = 2.250\n",
      "V(8, 3) = 2.250\n",
      "V(8, 4) = 2.250\n",
      "V(8, 5) = 2.250\n",
      "V(8, 6) = -1.000\n",
      "V(8, 7) = 1.925\n",
      "V(8, 8) = -1.000\n",
      "V(8, 9) = 1.388\n",
      "V(9, 0) = 0.570\n",
      "V(9, 1) = 0.789\n",
      "V(9, 2) = 0.789\n",
      "V(9, 3) = 0.804\n",
      "V(9, 4) = 0.880\n",
      "V(9, 5) = 0.635\n",
      "V(9, 6) = 0.570\n",
      "V(9, 7) = 2.250\n",
      "V(9, 8) = 0.789\n",
      "V(9, 9) = 0.901\n"
     ]
    }
   ],
   "source": [
    "# valor de estados de politica aleatoria\n",
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/var2.jpg\" alt=\"centered image\" width=600 height=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "* La tasa de descuento $\\gamma$ controla la preferencia de tener recompensas a corto o a largo plazo del algoritmo. Con base a los resultados obtenidos en el método de Value Iteration, tasas de descuento mayore generaban valores de estado óptimos mayores para estados contiguos o cercanos a los estados terminales de mayor recompensa (meta), para los estados sobre las filas 0-3 más próximos a las metas en (0,0) y (2,0), mientras que para tassa de descuento inferiores el algoritmo se comportó de manera conservadora y prefirió estados cercanos a escaleras de tal forma que preferió recompensas a largo plazo.\n",
    "\n",
    "* La función de valor de la política de avanzar tuvo mayor magnitud (tanto positiva como negativa) para los valores de los estados, es decir los estados más favorables del MDP obtuvieron mayor valor de estado con la política de avanzar en relación con los valores de estado con la política aleatoria. Del mismo modo, los estaddos menos favorables del MDP obtuvieron menores valores de estado con la políitca de avanzar en relación con la política aleatoria. Por otro lado, sólamente 10 estados  y 12 estados obtuvieron un valor de estado positivo siguiendo la política aleatoria, mientras que la política de Avanzar obtuvo más estados con valor de estado positivo, de modo que se puede concluir que la función de valor de la política de avanzar genera más estados favorables sobre el MDP, a pesar que las recompensas en los estados desfavorables sea más negativa. \n",
    "\n",
    "\n",
    "* El método de value iteration con tasa de descuento $gamma$ mayor el agente prefirió estados contiguos a la meta donde la recompensa obtenida fue inmedianta, en relación con los valores de estado óptimos obtenidos por el método de value iteration con tasa de descuento menor, donde prefirió buscar  casillas de escaleras para buscar recompensas a largo plazo, a pesar de que le tomase más iteraciones en el horizonte temporal alcanzar la meta. \n",
    "\n",
    "* El método de función de valor nos permite comparar y contrastar el valor de políticas dadas, mientras que el método de value iteration nos permite encontrar los mejores valores de estados posibles dados un MDP, y la políitca óptima que se debe seguir.\n",
    "\n",
    "* El número de iteraciones en el horizonte temporal H que le tomó a la función de valor converger a los valores de estados dada una política es entre 30 y 40 iteraciones, mientras que los escenarios de value iteration  convergieron entre 50 y 60 iteraciones en el horizonte temporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
