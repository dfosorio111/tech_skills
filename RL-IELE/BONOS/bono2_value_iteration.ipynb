{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.2.0 (SDL 2.0.22, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Librerias que contienen la dinámica del entorno gridworld\n",
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El presente informe presenta la implementació del método de value iteration para la variación con viento hacia la derecha sobre las acciones posibles. Se presenta una explicación detallada en el cálculo de la función de valor v(s) realizada en cada iteración del algoritmo, así como la implementación por pasos intermedios del calculo del valor de estado v(s) hasta convergencia para el GridWorld del enunciado.\n",
    "\n",
    "Para esto se tuvo en cuenta un MDP basado en el GridWorld de 4x4 (16 estados), donde los estados terminales con valor de estado v(st)=0 para los estados (0,0),(3,3) es siempre 0, y la recompensa de todas las transiciones $r-trans=R(s,a,s')=-1$ en todas las transiciones. Las acciones posibles son Norte 'N', Sur 'S', East 'E', Weast 'W'. Del mismo modo, la dinámica del MDP $p(s'|s,a)$ es la siguiente: la probabilidad de llegar al estado (estado1) realizar la acción indicada  es 0.8, mientras que con probabilidad 0.2 llega al estado de la derecha del estado1. Si el estado de llegada en el segundo caso es por fuera del MDP, entonces se queda en el estado1.\n",
    "\n",
    "A partir de los resultados se observa que no hay cambios significativos después de un horizonte temporal H de 6 iteraciones(hay convergencia aproximada de $v(s)$ después de 6 iteraciones). Del mismo modo se presentan los resultados del cálculo de la función de valor de estado $v(s)$ para las primeras 7 iteraciones, junto con el cambio de la política hasta asegurar convergencia.\n",
    "\n",
    "Finalmente se presenta el cálculo de la función de valor de estado $v(s)$ mediante el método de value iteration y la política óptima encontrada para un horizonte temporal H de 30 iteraciones, para el Escenario 1 con tasa de descuento $\\gamma=0.9$ y  , para el Escenario 1 con tasa de descuento $\\gamma=1$. \n",
    "\n",
    "## Escenario 1: Tasa de descuento $\\gamma=0.9$\n",
    "\n",
    "\n",
    "El valor del estado hasta convergencia aproximada para todos los estados del MDP calculados son los siguientes:\n",
    "\n",
    "* Valor de estado v((0, 0))=  0.0\n",
    "* Valor de estado v((0, 1))=  -1.4353120000000001\n",
    "* Valor de estado v((0, 2))=  -2.4797861463414637\n",
    "* Valor de estado v((0, 3))=  -2.71\n",
    "* Valor de estado v((1, 0))=  -1.2583561600000002\n",
    "* Valor de estado v((1, 1))=  -2.3244102868292686\n",
    "* Valor de estado v((1, 2))=  -2.4184\n",
    "* Valor de estado v((1, 3))=  -1.9\n",
    "* Valor de estado v((2, 0))=  -2.3244102868292686\n",
    "* Valor de estado v((2, 1))=  -2.4184\n",
    "* Valor de estado v((2, 2))=  -1.72\n",
    "* Valor de estado v((2, 3))=  -1.0\n",
    "* Valor de estado v((3, 0))=  -2.4184\n",
    "* Valor de estado v((3, 1))=  -1.72\n",
    "* Valor de estado v((3, 2))=  -1.0\n",
    "* Valor de estado v((3, 3))=  0.0\n",
    "\n",
    "\n",
    "La política óptima para el MDP del GridWorld con variación de viento hacia la derecha es la siguiente:\n",
    "\n",
    "{(0, 0): 'N',\n",
    " (0, 1): 'W',\n",
    " (0, 2): 'W',\n",
    " (0, 3): 'S',\n",
    " (1, 0): 'N',\n",
    " (1, 1): 'W',\n",
    " (1, 2): 'S',\n",
    " (1, 3): 'S',\n",
    " (2, 0): 'N',\n",
    " (2, 1): 'S',\n",
    " (2, 2): 'S',\n",
    " (2, 3): 'S',\n",
    " (3, 0): 'E',\n",
    " (3, 1): 'E',\n",
    " (3, 2): 'E',\n",
    " (3, 3): 'N'}\n",
    "\n",
    "\n",
    "## Escenario 2: Tasa de descuento $\\gamma=1$\n",
    "\n",
    "El valor del estado hasta convergencia aproximada para todos los estados del MDP calculados son los siguientes:\n",
    "\n",
    "* Valor de estado v((0, 0))=  0.0\n",
    "* Valor de estado v((0, 1))=  -1.528\n",
    "* Valor de estado v((0, 2))=  -2.7780000000000005\n",
    "* Valor de estado v((0, 3))=  -3.0000000000000004\n",
    "* Valor de estado v((1, 0))=  -1.3056\n",
    "* Valor de estado v((1, 1))=  -2.5556\n",
    "* Valor de estado v((1, 2))=  -2.6399999999999997\n",
    "* Valor de estado v((1, 3))=  -2.0\n",
    "* Valor de estado v((2, 0))=  -2.5556\n",
    "* Valor de estado v((2, 1))=  -2.6399999999999997\n",
    "* Valor de estado v((2, 2))=  -1.8\n",
    "* Valor de estado v((2, 3))=  -1.0\n",
    "* Valor de estado v((3, 0))=  -2.6399999999999997\n",
    "* Valor de estado v((3, 1))=  -1.8\n",
    "* Valor de estado v((3, 2))=  -1.0\n",
    "* Valor de estado v((3, 3))=  0.0\n",
    "\n",
    "La política óptima para el MDP del GridWorld con variación de viento hacia la derecha es la siguiente:\n",
    "\n",
    "{(0, 0): 'N',\n",
    " (0, 1): 'W',\n",
    " (0, 2): 'W',\n",
    " (0, 3): 'S',\n",
    " (1, 0): 'N',\n",
    " (1, 1): 'W',\n",
    " (1, 2): 'S',\n",
    " (1, 3): 'S',\n",
    " (2, 0): 'N',\n",
    " (2, 1): 'S',\n",
    " (2, 2): 'S',\n",
    " (2, 3): 'S',\n",
    " (3, 0): 'E',\n",
    " (3, 1): 'E',\n",
    " (3, 2): 'E',\n",
    " (3, 3): 'N'}\n",
    "\n",
    "\n",
    "A partir de los resultados se observa que para los Escenarios 1 y 2 el algoritmo de value iteration alcanza convergencia (aproximada) después de la 6 iteración en $v_6(s)$ porque no hay diferencias apreciables en el cálculo de la función de valor de estado $v(s)$ ni de la política óptima en convergencia aproximada después de la 6 iteración en adelante. Finalmente la política óptima encontrada en ambos escenarios es la misma.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gridworld.GridWorld at 0x1d205a7ba00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crear MDP\n",
    "# parametros:\n",
    "gw = GridWorld(rows=4, cols=4, walls=[], pits=[], goals=[(0,0),(3,3)], live_reward=0.0)\n",
    "gw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration: Variación con viento hacia la derecha (East)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el horizonte es $H=2$, \n",
    "\n",
    "Para estados terminales $S_t$, el valor de estado $v(s)=0$ SIEMPRE\n",
    "\n",
    "Para estados no terminales:\n",
    "$$V_{2}^{*}(s)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s'))$$\n",
    "\n",
    "Por ejemplo, si el agente está en (1,1), que corresponde al estado 5 del enunciado del examen, y se tiene un descuento $\\gamma=0.90$:\n",
    "\n",
    "* Si pretende ejecutar 'N':\n",
    "    * Si efectivamente se ejecuta 'N':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(-1.0+\\gamma V_1(0,1))$\n",
    "    * Si, por el viento hacia la derecha, se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.2(-1.0+\\gamma V_1(0,2))$\n",
    "\n",
    "* Si pretende ejecutar 'S':\n",
    "    * Si efectivamente se ejecuta 'S':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(-1.0+\\gamma V_1(2,1))$\n",
    "    * Si, por el viento hacia la derecha, se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.2(-1.0+\\gamma V_1(2,2))$\n",
    "\n",
    "* Si pretende ejecutar 'E':\n",
    "    * Si efectivamente se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(-1.0+\\gamma V_1(1,2))$\n",
    "    * Si, por el viento hacia la derecha, se ejecuta 'EE':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.2(-1.0+\\gamma V_1(1,3))$\n",
    "\n",
    "* Si pretende ejecutar 'W':\n",
    "    * Si efectivamente se ejecuta 'W':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(-1.0+\\gamma V_1(1,0))$\n",
    "    * Si, por el viento hacia la derecha, se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.2(-1.0+\\gamma V_1(1,1))$\n",
    "\n",
    "Finalmente el valor de estado $v(1,1)$ es el valor de estado de la acción que es máxima, y ésta acción se escoge como óptima para la interación actual del algoritmo. \n",
    "\n",
    "Este proceso se realiza para todos los estados del MDP, para una secuencia de iteraciones definidas por el horizonte temporal H hasta asegurar convergencia.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación detallada del algoritmo implementado\n",
    "\n",
    "El algoritmo de value iteration es una aplicación de Programación Dinámica de Policy Iteration (Policy evaluation(predicción) y Policy Improvement), en el que la etapa de predicción  Policy evaluation es truncada a 1 iteración, para una secuencia de iteraciones hasta asegurar convergencia, mientras que la política se mejora a partir del cálculo de la función de valor de estados $v(s)$ de la iteración anterior. \n",
    "\n",
    "A partir de la dinámica (probabilidades de transición), la recompensa de la transición, la tasa de descuento ($\\gamma=0.9$) y la definición del MDP del Gridworld con variación de viento hacia la derecha, se procede a implementar el algoritmo de value iterarion. Lo anterior se desarrolló de la siguiente manera:\n",
    "\n",
    "En primer lugar se definió que para estados terminales (0,0),(3,3) el valor de estado $v(s)$ es SIEMPRE 0. \n",
    "\n",
    "En seguida se iteró sobre todos los estados del MDP (a través del horizonte temporal), en cada iteración se da un paso en la dirección de la acción que se pretende ejecutar. En seguida, se define el estado siguiente para la acción que pretende tomar (estado1) y el estado al que llega si hay viento (estado2) que es a la derecha del estado1. En caso de que el estado2 esté por fuera del Gridworld (ej: estado no existe en MDP), entonces el estado2 es igual al estado1 es decir se queda en el mismo estado si no hubiese sido afectado por el viento en la dinámica del MDP.\n",
    "\n",
    "Finalmente, para calcular la función de valor de estados, se aplican las ecuaciones detalladas anteriormente (en donde se realizó el ejemplo para el cálculo del valor de estado (1,1) en H=2 ), se escoge la acción con máximo valor de estado calculado y se asigna como valor de estado en la iteración actual, y se escoge esta política (acción) como óptima en la iteración actual.\n",
    "\n",
    "El algoritmo itera sobre estados a lo largo de una secuencia de iteraciones definidas por el horizonte temporal H, hasta asegurar convergencia, donde se tiene el valor de estado óptimo $v(s)_*$ y la política óptima $\\pi_*$ que resuelve el MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def: método de PD: value iteration para calcular v(s) y política óptima\n",
    "# parametros: gw=MDP GridWorld (Taller 2), gamma=tasa de descuento (0.9)\n",
    "\n",
    "def value_iteration(gw, gamma):\n",
    "    \n",
    "    new_values = dict.fromkeys(gw.states , 0.0) # inicializa vector de valores de estado v0(s)=0 \n",
    "    r_trans = -1 # recompensa de transicion\n",
    "    \n",
    "    # iterar sobre estados\n",
    "    for state in gw.states: \n",
    "        if state in gw.goals:\n",
    "            #new_values[state] = 0\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            v = dict.fromkeys(gw.get_allowed_actions(state), 0.0) # inicializa vector de valores de estado v0(s)=0\n",
    "            \n",
    "            # definir estados finales para todas las accciones posibles ['N','S','E','W'] \n",
    "            # accion Norte \n",
    "            N, reward, _, done = gw.step(state, 'N', random=False)\n",
    "            if N in gw.goals: # si estado final es terminal (goal)\n",
    "                NE=(N[0],N[1]+1) # estado con viento es estado final +1 a la derecha\n",
    "                if NE not in gw.states: # si estado con viento no esta en MDP\n",
    "                    NE=N # se queda en el mismo estado \n",
    "                else:\n",
    "                    pass\n",
    "            else:  # si estado final es no terminal\n",
    "                NE, reward, _, done = gw.step(N, 'E', random=False) # estado con viento es estado final dando un paso a la derecha\n",
    "                    \n",
    "            # accion Sur       \n",
    "            S, reward, _, done = gw.step(state, 'S', random=False)         \n",
    "            if S in gw.goals: # si estado final es terminal (goal)\n",
    "                SE=(S[0],S[1]+1)# estado con viento es estado final +1 a la derecha\n",
    "                if SE not in gw.states: # si estado con viento no esta en MDP\n",
    "                    SE=S # se queda en el mismo estado \n",
    "                else:\n",
    "                    pass\n",
    "            else: # si estado final es no terminal\n",
    "                SE, reward, _, done = gw.step(S, 'E', random=False) # estado con viento es estado final dando un paso a la derecha\n",
    "                \n",
    "            # accion East       \n",
    "            E, reward, _, done = gw.step(state, 'E', random=False)         \n",
    "            if E in gw.goals:# si estado final es terminal (goal)\n",
    "                EE=(E[0],E[1]+1)# estado con viento es estado final +1 a la derecha\n",
    "                if EE not in gw.states:# si estado con viento no esta en MDP\n",
    "                    EE=E# se queda en el mismo estado \n",
    "                else:\n",
    "                    pass\n",
    "            else: # si estado final es no terminal\n",
    "                EE, reward, _, done = gw.step(E, 'E', random=False) # estado con viento es estado final dando un paso a la derecha\n",
    "                \n",
    "            # accion Weast       \n",
    "            W, reward, _, done = gw.step(state, 'W', random=False)         \n",
    "            if W in gw.goals:# si estado final es terminal (goal)\n",
    "                WE=(S[0],S[1]+1)# estado con viento es estado final +1 a la derecha\n",
    "                if WE not in gw.states:# si estado con viento no esta en MDP\n",
    "                    WE=W# se queda en el mismo estado \n",
    "                else:\n",
    "                    pass\n",
    "            else: # si estado final es no terminal\n",
    "                WE, reward, _, done = gw.step(W, 'E', random=False) # estado con viento es estado final dando un paso a la derecha\n",
    "\n",
    "            # calcular v(s') para todas las acciones posibles  ['N','S','E','W'] \n",
    "            v['N']= gw.action_probabilities2[0]*(r_trans+gamma*gw.state_values[N])+gw.action_probabilities2[1]*(r_trans+gamma*gw.state_values[NE])\n",
    "            v['S']= gw.action_probabilities2[0]*(r_trans+gamma*gw.state_values[S])+gw.action_probabilities2[1]*(r_trans+gamma*gw.state_values[SE])\n",
    "            v['E']= gw.action_probabilities2[0]*(r_trans+gamma*gw.state_values[E])+gw.action_probabilities2[1]*(r_trans+gamma*gw.state_values[EE])\n",
    "            v['W']= gw.action_probabilities2[0]*(r_trans+gamma*gw.state_values[W])+gw.action_probabilities2[1]*(r_trans+gamma*gw.state_values[WE])\n",
    "            \n",
    "            # actualizar politica (accion) en estado state como la accion con el valor de estado v(s') máximo \n",
    "            # actualizar v(s) como el valor de estado v(s') máximo\n",
    "            gw.policy[state], new_values[state] = gw.key_max(v)\n",
    "              \n",
    "    gw.state_values = copy.deepcopy(new_values)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escenario 1: Tasa de descuento $\\gamma=0.9$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos intermedios: $V_1(s)-V_5(s)$\n",
    "\n",
    "A continuación se presenta el cálculo intermedio de la función de valor $v_i(s)$ de manera secuencia en cada una de las iteraciones hasta alcanzar convergencia (aproximada). Del mismo modo en cada iteración se presenta la política mejorada por el algoritmo para la i-ésima iteración. El algoritmo alcanza convergencia (aproximada) después de 6 iteraciones, en las que no hay diferencias apreciables en el cálculo de la función de valor $v(s)$ de todos los estados ni de la política óptima a partir de la iteracion 6 y en adelante.\n",
    "\n",
    "Finalmente se presenta la función de valor para todos los estados y la política (óptima) en un horizonte temporal H=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear MDP\n",
    "# parametros:\n",
    "gw = GridWorld(rows=4, cols=4, walls=[], pits=[], goals=[(0,0),(3,3)], live_reward=0.0)\n",
    "gw\n",
    "\n",
    "gamma=0.9\n",
    "H=30\n",
    "init_state=(3,0)\n",
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=init_state )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado0: $V0(s)$ y política\n",
    "<center>\n",
    "    <img src=\"images/v0_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado1: $V1(s)$ y política\n",
    "<center>\n",
    "    <img src=\"images/v1_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado2: $V2(s)$ y política\n",
    "<center>\n",
    "    <img src=\"images/v2_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado3: $V3(s)$ y política\n",
    "<center>\n",
    "    <img src=\"images/v3_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado4: $V4(s)$ y política\n",
    "<center>\n",
    "    <img src=\"images/v4_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado5: $V5(s)$ y política \n",
    "<center>\n",
    "    <img src=\"images/v5_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado 6: $V6(s)$ y política \n",
    "<center>\n",
    "    <img src=\"images/v6_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado 7:  $V7(s)$ y política \n",
    "<center>\n",
    "    <img src=\"images/v7_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado 30:  $V30(s)$ y política\n",
    "## Horizonte temporal H=30 \n",
    "<center>\n",
    "    <img src=\"images/vfin_gamma1.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado en convergencia (aproximada) $v(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de estado v((0, 0))=  0.0\n",
      "Valor de estado v((0, 1))=  -1.4353120000000001\n",
      "Valor de estado v((0, 2))=  -2.4797861463414637\n",
      "Valor de estado v((0, 3))=  -2.71\n",
      "Valor de estado v((1, 0))=  -1.2583561600000002\n",
      "Valor de estado v((1, 1))=  -2.3244102868292686\n",
      "Valor de estado v((1, 2))=  -2.4184\n",
      "Valor de estado v((1, 3))=  -1.9\n",
      "Valor de estado v((2, 0))=  -2.3244102868292686\n",
      "Valor de estado v((2, 1))=  -2.4184\n",
      "Valor de estado v((2, 2))=  -1.72\n",
      "Valor de estado v((2, 3))=  -1.0\n",
      "Valor de estado v((3, 0))=  -2.4184\n",
      "Valor de estado v((3, 1))=  -1.72\n",
      "Valor de estado v((3, 2))=  -1.0\n",
      "Valor de estado v((3, 3))=  0.0\n"
     ]
    }
   ],
   "source": [
    "for state in gw.states:\n",
    "    print(\"Valor de estado v(\"+str(state)+')= ', gw.state_values[state] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Política óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 'N',\n",
       " (0, 1): 'W',\n",
       " (0, 2): 'W',\n",
       " (0, 3): 'S',\n",
       " (1, 0): 'N',\n",
       " (1, 1): 'W',\n",
       " (1, 2): 'S',\n",
       " (1, 3): 'S',\n",
       " (2, 0): 'N',\n",
       " (2, 1): 'S',\n",
       " (2, 2): 'S',\n",
       " (2, 3): 'S',\n",
       " (3, 0): 'E',\n",
       " (3, 1): 'E',\n",
       " (3, 2): 'E',\n",
       " (3, 3): 'N'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw.policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escenario 2: Tasa de descuento $\\gamma=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear MDP\n",
    "# parametros:\n",
    "gw = GridWorld(rows=4, cols=4, walls=[], pits=[], goals=[(0,0),(3,3)], live_reward=0.0)\n",
    "gw\n",
    "\n",
    "gamma=1\n",
    "H=30\n",
    "init_state=(3,0)\n",
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=init_state )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado 30:  $V30(s)$ y política\n",
    "## Horizonte temporal H=30 \n",
    "<center>\n",
    "    <img src=\"images/vfin_gamma2.jpg\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de estado en convergencia (aproximada) $v(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de estado v((0, 0))=  0.0\n",
      "Valor de estado v((0, 1))=  -1.528\n",
      "Valor de estado v((0, 2))=  -2.7780000000000005\n",
      "Valor de estado v((0, 3))=  -3.0000000000000004\n",
      "Valor de estado v((1, 0))=  -1.3056\n",
      "Valor de estado v((1, 1))=  -2.5556\n",
      "Valor de estado v((1, 2))=  -2.6399999999999997\n",
      "Valor de estado v((1, 3))=  -2.0\n",
      "Valor de estado v((2, 0))=  -2.5556\n",
      "Valor de estado v((2, 1))=  -2.6399999999999997\n",
      "Valor de estado v((2, 2))=  -1.8\n",
      "Valor de estado v((2, 3))=  -1.0\n",
      "Valor de estado v((3, 0))=  -2.6399999999999997\n",
      "Valor de estado v((3, 1))=  -1.8\n",
      "Valor de estado v((3, 2))=  -1.0\n",
      "Valor de estado v((3, 3))=  0.0\n"
     ]
    }
   ],
   "source": [
    "for state in gw.states:\n",
    "    print(\"Valor de estado v(\"+str(state)+')= ', gw.state_values[state] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Política óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 'N',\n",
       " (0, 1): 'W',\n",
       " (0, 2): 'W',\n",
       " (0, 3): 'S',\n",
       " (1, 0): 'N',\n",
       " (1, 1): 'W',\n",
       " (1, 2): 'S',\n",
       " (1, 3): 'S',\n",
       " (2, 0): 'N',\n",
       " (2, 1): 'S',\n",
       " (2, 2): 'S',\n",
       " (2, 3): 'S',\n",
       " (3, 0): 'E',\n",
       " (3, 1): 'E',\n",
       " (3, 2): 'E',\n",
       " (3, 3): 'N'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw.policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
