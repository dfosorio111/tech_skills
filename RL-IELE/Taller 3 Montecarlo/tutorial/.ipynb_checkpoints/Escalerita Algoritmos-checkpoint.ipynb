{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escaleras y Serpientes: MC y TD(0) <img src=\"images/uniandes_logo.png\" width=\"250\"/>\n",
    "\n",
    "autores:\n",
    "\n",
    "Diego Fernando Osorio   201513417   - df.osorio11@uniandes.edu.co\n",
    "Emmanuel Gonzalez Gonzalez   201614679  - e.gonzalezg@uniandes.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este taller se trabajar谩 sobre la modificaci贸n del juego de escaleras y serpientes del taller anterior. Se utilizan funciones del taller pasado para el desarrollo de los nuevos algoritmos. En este taller se realiza la implementaci贸n de los algoritmos de MonteCarlo Off-Policy (MC) y  m茅todos TD(0): SARSA y Q-Learning para encontrar pol铆ticas 贸ptimas al estimar las funciones de valor.\n",
    "\n",
    "Los m茅todos de MC se basan en la generaci贸n de episodios y un importante componente de aletoriedad en las secuencias de estado, acci贸n y recompensa. La regla de actualizaci贸n para el algoritmo MC se basa en la estimaci贸n de la funci贸n de valor par estado-acci贸n $Q_(s,a)$ donde el TARGET es el retorno del episodio. La pol铆tica para un estado se define de manera GREEDY con respecto a $Q_(s,a)$.\n",
    "\n",
    "Por otro lado, los m茅todos TD se basan en la regla de actualizaci贸n dada por la siguiente ecuacio贸n:\n",
    "\n",
    "$V(s) =V(s)+伪[R_t+1 *纬 Vs_t+1 - V(s) $, donde se tiene que el TARGET es R_t+1 *纬 Vs_t+1 y el error es la diferencia del TARGET y $V(s) $ de la actualizaci贸n actual.\n",
    "\n",
    "El control en TD(0) se realiza la implementaci贸n de 2 m茅todos similares: SARSA y Q-Learning. Ambos m茅todos realizan la generaci贸n de episodios, mientras que SARSA realiza la actualizaci贸n teniendo en cuenta la funci贸n de valor par estado-acci贸n actual $Q_(s,a)$ y la siguiente $Q_(s',a')$, en las que la escogencia de las acciones $a, a'$ se realiza E-GREEDY con respecto a la funci贸n $Q_(s,a)$, en Q-Learning la actualizaci贸n se realiza con $Q_(s,a)$ escogiendo $a$ de manera E-GREEDY y $Q_(s',a')$ escogiendo $a'$ GREEDY (m谩xima) con respecto a la funci贸n de valor $Q_(s,a)$. Entonces SARSA con epsilon $系=0$ en donde siempre escoge la acci贸n en el paso siguiente GREEDY es SARSA.\n",
    "\n",
    "La diferencia es que MC realiza la generaci贸n de los episodios y espera a que todos los episodios, es decir la secuencia $S,A,Ri$ hayan sido generados, y luego recorre cada uno de los episodios generados para estimar $Q_(s,a)$ de manera off-policy (con dos pol铆ticas $, $, donde $$ es pol铆tca TARGET que se modifica GREEDY con respecto a la funci贸n $Q_(s,a)$ y $$ se utiliza para generar los episodios ) para luego as铆 estimar $Q_(s,a)$ mediante los pesos de correcci贸n $W/C(s,a)$ de la pol铆tica TARGET y de comportamiento. \n",
    "\n",
    "Por otro lado SARSA y Q-Learning a medida que generan los episodios actualizan la funci贸n $Q_(s,a)$ con la funci贸n que se tiene en el estado actual $Q_(s,a)$ y el paso siguiente $Q_(s',a')$, para todos los episodios. Es decir en TD(0) a medida que avanza sobre el episodio, mirando un paso adelante, realiza la actualizaci贸n de la funci贸n de valor par estado-acci贸n $Q_(s,a)$ y NO espera al final del episodio. Del mismo modo se generaron algortimos que permitan generar episodios aleatorios dada una pol铆tica arbitraria, permitiendo as铆 dada una acci贸n en un estado, retornar una recompensa y el estado resultante de esta acci贸n, hasta llegar al final del episodio para el MDP de Escaleras y Serpientes.\n",
    "\n",
    "Se realiz贸 la evaluaci贸n, comparaci贸n y contraste de los 3 algoritmos para encontrar pol铆ticas 贸ptimas y estimar la funci贸n $Q_(s,a)$ 贸ptima. Para esto se tuvo en cuenta los siguiente hiperpar谩metros de algoritmos: tasa de aprendizaje $伪=0.1$, epsilon (E-GREEDY) $系=0.05$, y tasa de descuento $纬=0.9$.\n",
    "\n",
    "Se observa que n煤mero de pasos promediados para 10,000 episodios generados se tienen un n煤mero de pasos promedio de 52.037, 12.779 y 12.730 para los algoritmos MC Off-policy, SARSA y Q-Learning respectivamente. Por otro lado el porcentaje de victorias en promedio fue de 4.9%, 3.31% y 3.17%, respectivamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/escalerita.png\" alt=\"centered image\" width=300/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones importantes:**\n",
    "\n",
    "* La meta del jugador es ganar la partida llegando a una de las casillas marcadas en azul.\n",
    "* El jugador pierde la partida si cae en una de las casillas marcadas en rojo.\n",
    "* En cada jugada, antes de lanzar el dado, el jugador decide si quiere avanzar o retroceder el n煤mero de casillas indicadas por el dado.\n",
    "* En las casillas 1 y 100 la ficha rebota (si se supera el extremo, se avanza en la otra direcci贸n la cantidad restante)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones utiles del taller anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Librerias que contienen la din谩mica del entorno gridworld\n",
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit\n",
    "gw = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subpolitica (x):\n",
    "    # iterar sobre estados\n",
    "    for i in gw.states:\n",
    "        # asignar politica en estado i como x[i[0],i[1]] \n",
    "        gw.policy[i]=x[i[0]][i[1]]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inipoli():\n",
    "    state = gw.states\n",
    "\n",
    "    # definir politica para avanzar\n",
    "    # iterar sobre estados\n",
    "    for i, s in enumerate(state):\n",
    "        if s[0]%2==0 and s[1]!=0 : # si fila es par y columna no es 0, acci贸n W\n",
    "            gw.policy[s] = 'W'\n",
    "        elif s[0]%2==0 and s[1]==0 : # si fila es par y columna es 0, acci贸n N\n",
    "            gw.policy[s] = 'N'\n",
    "        elif s[0]%2==1 and s[1]!=9 : # si fila es par y columna no es 9, acci贸n E  \n",
    "            gw.policy[s] = 'E'\n",
    "        elif s[0]%2==1 and s[1]==9 : # si fila es par y columna es 9, acci贸n N\n",
    "            gw.policy[s] = 'N'\n",
    "    a=[[]]\n",
    "    r=[[]]\n",
    "\n",
    "    # iterar sobre filas\n",
    "    for i in range (10):\n",
    "        # iterar sobre columnas\n",
    "        for j in range(10):\n",
    "            a[i].append('')\n",
    "            r[i].append('')\n",
    "        a.append([])\n",
    "        r.append([])\n",
    "\n",
    "    for i in gw.states:\n",
    "        a[i[0]][i[1]]=gw.policy[i]\n",
    "\n",
    "    for i, s in enumerate(state):\n",
    "        if s[0]%2==0 and s[1]!=9 :\n",
    "            gw.policy[s] = 'E'\n",
    "        elif s[0]%2==0 and s[1]==9 :\n",
    "            gw.policy[s] = 'S'\n",
    "        elif s[0]%2==1 and s[1]!=0 :\n",
    "            gw.policy[s] = 'W'\n",
    "        elif s[0]%2==1 and s[1]==0 :\n",
    "            gw.policy[s] = 'S'\n",
    "    for i in gw.states:\n",
    "        r[i[0]][i[1]]=gw.policy[i]\n",
    "    subpolitica(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=-0.1,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = gw.states\n",
    "\n",
    "# definir politica para avanzar\n",
    "# iterar sobre estados\n",
    "for i, s in enumerate(state):\n",
    "    if s[0]%2==0 and s[1]!=0 : # si fila es par y columna no es 0, acci贸n W\n",
    "        gw.policy[s] = 'W'\n",
    "    elif s[0]%2==0 and s[1]==0 : # si fila es par y columna es 0, acci贸n N\n",
    "        gw.policy[s] = 'N'\n",
    "    elif s[0]%2==1 and s[1]!=9 : # si fila es par y columna no es 9, acci贸n E  \n",
    "        gw.policy[s] = 'E'\n",
    "    elif s[0]%2==1 and s[1]==9 : # si fila es par y columna es 9, acci贸n N\n",
    "        gw.policy[s] = 'N'\n",
    "a=[[]]\n",
    "r=[[]]\n",
    "\n",
    "# iterar sobre filas\n",
    "for i in range (10):\n",
    "    # iterar sobre columnas\n",
    "    for j in range(10):\n",
    "        a[i].append('')\n",
    "        r[i].append('')\n",
    "    a.append([])\n",
    "    r.append([])\n",
    "\n",
    "for i in gw.states:\n",
    "    a[i[0]][i[1]]=gw.policy[i]\n",
    "\n",
    "for i, s in enumerate(state):\n",
    "    if s[0]%2==0 and s[1]!=9 :\n",
    "        gw.policy[s] = 'E'\n",
    "    elif s[0]%2==0 and s[1]==9 :\n",
    "        gw.policy[s] = 'S'\n",
    "    elif s[0]%2==1 and s[1]!=0 :\n",
    "        gw.policy[s] = 'W'\n",
    "    elif s[0]%2==1 and s[1]==0 :\n",
    "        gw.policy[s] = 'S'\n",
    "for i in gw.states:\n",
    "    r[i[0]][i[1]]=gw.policy[i]\n",
    "subpolitica(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[9][0]='E'\n",
    "a[0][0]='E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traductorP(p):\n",
    "    for i in gw.states:\n",
    "        if p[i]=='a':\n",
    "            gw.policy[i]=a[i[0]][i[1]]\n",
    "        else:\n",
    "            gw.policy[i]=r[i[0]][i[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de control de Montecarlo off-policy\n",
    "\n",
    "El algoritmo MC off-policy se explica a continuaci贸n. Se genera una serie de episodios. Cada episodio que es una secuencia de $S,A,Ri$ para cada timestep, en seguida para cada episodio se recorre desde el estado terminal $S_T$ hacia el principio del episodio y se aplica la regla de actualizaci贸n para estimar  $Q_(s,a)$ llevando la funci贸n hacia el retorno $G=R_t+1 +纬G$, teniendo en cuenta el peso $W/C(s,a)$ que inicia en $W=1$ al inicio del epidosio y se va modificando teniendo en cuenta la probabilidad de ocurrencia de la pol铆tica target $\\pi$ en relaci贸n a la pol铆tica de comportamiento $$ que genera los episodios. $W$ es 0 cuando la acci贸n m谩xima dada por la pol铆tica de cubrimiento es diferente a la acci贸n dada por la pol铆tica target.\n",
    "\n",
    "El error en MC off-policy es $G-Q_(s,a)$.\n",
    "\n",
    "Los par谩metros del algoritmo son el n煤mero de episodios y la tasa de descuento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_offpolicy (gamma,epi):\n",
    "    \n",
    "    state_q_values = gw.init_qvalues2()\n",
    "    new_state_q_values =  gw.init_qvalues()    \n",
    "    pi =dict.fromkeys(gw.states,'a')\n",
    "    c =  gw.init_qvalues2()\n",
    "    for i in range(epi):\n",
    "        r,ac,est,f=gw.generainador(init_state=(-1,-1), gamma=1.0)\n",
    "        aja=True\n",
    "        while aja:\n",
    "            if len(est)>2:\n",
    "                for j in range(len(est)-1):\n",
    "                    if est[j]==est[j+1]:\n",
    "                        est.pop(j)\n",
    "                        r.pop(j)\n",
    "                        ac.pop(j)\n",
    "                        break\n",
    "                    if j==len(est)-2:\n",
    "                        aja=False \n",
    "            else:\n",
    "                aja=False\n",
    "        g = 0\n",
    "        w=1\n",
    "        for j in range(len(est)-2,-1,-1): # iterar sobre estados de episodio\n",
    "            g = gamma*g+r[j+1]\n",
    "            c[est[j]][ac[j]] =  c[est[j]][ac[j]]+w\n",
    "            \n",
    "            state_q_values [est[j]][ac[j]] = state_q_values[est[j]][ac[j]] + (w/(c[est[j]][ac[j]]))*(g-state_q_values[est[j]][ac[j]]) \n",
    "            \n",
    "            if (state_q_values [est[j]]['a'] > state_q_values [est[j]]['r']):\n",
    "                pi[est[j]]='a'\n",
    "            else:\n",
    "                pi[est[j]]='r'\n",
    "          \n",
    "            if ac[j]!= pi[est[j]]:\n",
    "                break\n",
    "                \n",
    "            w *=2 \n",
    "            \n",
    "    return state_q_values, pi\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,p= MC_offpolicy (0.9,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "traductorP(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  1\n",
      "llegue a  (9, 1)\n",
      "estaba en el estado (9, 1)\n",
      "saque en el dado:  4\n",
      "llegue a  (9, 5)\n",
      "estaba en el estado (9, 5)\n",
      "saque en el dado:  4\n",
      "llegue a  (9, 9)\n",
      "estaba en el estado (9, 9)\n",
      "saque en el dado:  2\n",
      "llegue a  (9, 7)\n",
      "estaba en el estado (9, 7)\n",
      "saque en el dado:  6\n",
      "llegue a  (8, 0)\n",
      "estaba en el estado (8, 0)\n",
      "saque en el dado:  1\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  1\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  1\n",
      "llegue a  (2, 1)\n",
      "estaba en el estado (2, 1)\n",
      "saque en el dado:  4\n",
      "llegue a  (1, 3)\n",
      "estaba en el estado (1, 3)\n",
      "saque en el dado:  5\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  6\n",
      "llegue a  (1, 4)\n",
      "estaba en el estado (1, 4)\n",
      "saque en el dado:  1\n",
      "llegue a  (1, 3)\n",
      "estaba en el estado (1, 3)\n",
      "saque en el dado:  6\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 0)\n",
      "estaba en el estado (2, 0)\n",
      "llegue a  Terminal Diamante\n"
     ]
    }
   ],
   "source": [
    "gw.follow_policy(init_state=(9,0), flag_q=False, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/MC 10k.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica despu茅s de 10,000 episodios\n",
    "    <img src=\"images/MC 100k.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica despu茅s de 100,000 episodios\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de control de SARSA\n",
    "\n",
    "El algoritmo SARSA on-policy se explica a continuaci贸n. Se genera una serie de episodios. Cada episodio que es una secuencia de $S,A,Ri$ para cada timestep, se realiza la estimaci贸n de  $Q_(s,a)$ teniendo en cuenta el estado actual en el timestep  $Q_(s,a)$ y el paso siguiente  $Q_(s',a')$ llevando la funci贸n hacia el TARGET  $R + \\gamma Q_(s',a')$. Las acciones tanto $a, a'$ se escogen de manera E-GREEDY con respecto al estimativo de  $Q_(s,a)$ y  $Q_(s',a)$ respectivamente.\n",
    "El error en SARS es $[R + \\gamma Q_(s',a')- Q_(s',a')]$. El algortimo SARSA es on-policy porque siempre se est谩 mejorando la misma pol铆tica $$ en funci贸n del estimado de $Q_(s,a)$\n",
    "\n",
    "Los par谩metros del algoritmo son el n煤mero de episodios, la tasa de descuento, la tasa de aprendizaje y el epsilon E-GREEDY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA (epi,gamma,alpha,eps):\n",
    "    state_q_values = gw.init_qvalues2()\n",
    "    \n",
    "    for i in range(epi):\n",
    "        done=False\n",
    "        state = random.choice(gw.states)\n",
    "        if state_q_values[state]['a'] >= state_q_values[state]['r']:\n",
    "            ac=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "        else:\n",
    "            ac=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "        while True:\n",
    "            if done:\n",
    "                break\n",
    "            dado=random.randint(1, 6)\n",
    "            newst=state\n",
    "            if ac=='a':\n",
    "                subpolitica(a)\n",
    "                for j in range(dado):\n",
    "                    if newst==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    if newst in gw.goals or newst in gw.pits:\n",
    "                        if a[newst[0]][newst[1]]=='N':\n",
    "                            newst = (newst[0]-1, newst[1])\n",
    "                        if a[newst[0]][newst[1]]=='S':\n",
    "                            newst = (newst[0]+1, newst[1])\n",
    "                        if a[newst[0]][newst[1]]=='W':\n",
    "                            newst = (newst[0], newst[1]-1)\n",
    "                        if a[newst[0]][newst[1]]=='E':\n",
    "                            newst = (newst[0], newst[1]+1) \n",
    "                    else:\n",
    "                        action=gw.policy[newst]\n",
    "                        newst, reward, _, done = gw.step(newst, action, random=False)\n",
    "                    \n",
    "                if newst in gw.goals:\n",
    "                    reward=10\n",
    "                    done=True\n",
    "                if newst in gw.pits:\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                state2=newst\n",
    "                if (state_q_values [state2]['a'] >= state_q_values [state2]['r']):\n",
    "                        ac2=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "                else:\n",
    "                    ac2=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "\n",
    "                state_q_values[state][ac] = state_q_values[state][ac]+alpha*(reward+gamma*state_q_values[state2][ac2] - state_q_values[state][ac])\n",
    "                state = state2\n",
    "                ac=ac2\n",
    "                      \n",
    "                    \n",
    "                    \n",
    "            else: # action es retroceder\n",
    "                \n",
    "                subpolitica(r)\n",
    "                for j in range(dado):\n",
    "                    if newst==(9,0):\n",
    "                        subpolitica(a)\n",
    "                    if newst in gw.goals or newst in gw.pits:\n",
    "                        if r[newst[0]][newst[1]]=='N':\n",
    "                            newst = (newst[0]-1, newst[1])\n",
    "                        if r[newst[0]][newst[1]]=='S':\n",
    "                            newst = (newst[0]+1, newst[1])\n",
    "                        if r[newst[0]][newst[1]]=='W':\n",
    "                            newst = (newst[0], newst[1]-1)\n",
    "                        if r[newst[0]][newst[1]]=='E':\n",
    "                            newst = (newst[0], newst[1]+1) \n",
    "                    else:\n",
    "                        action=gw.policy[newst]\n",
    "                        newst, reward, _, done = gw.step(newst, action, random=False)\n",
    "\n",
    "                if newst in gw.goals:\n",
    "                    reward=10\n",
    "                    done=True\n",
    "                if newst in gw.pits:\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                state2=newst\n",
    "                if (state_q_values [state2]['a'] >= state_q_values [state2]['r']):\n",
    "                        ac2=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "                else:\n",
    "                    ac2=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "                \n",
    "                state_q_values[state][ac] = state_q_values[state][ac]+alpha*(reward+gamma*state_q_values[state2][ac2] - state_q_values[state][ac])\n",
    "                state = state2\n",
    "                ac=ac2\n",
    "    \n",
    "    pi =dict.fromkeys(gw.states,0)\n",
    "    for i in gw.states:\n",
    "        if (state_q_values [i]['a'] >= state_q_values [i]['r']):\n",
    "            pi[i]='a'\n",
    "        else:\n",
    "            pi[i]='r'\n",
    "    return state_q_values,pi\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,p2=SARSA(100000,0.9,0.1,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "traductorP(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  2\n",
      "llegue a  (9, 2)\n",
      "estaba en el estado (9, 2)\n",
      "saque en el dado:  6\n",
      "llegue a  (9, 8)\n",
      "estaba en el estado (9, 8)\n",
      "saque en el dado:  6\n",
      "llegue a  (8, 4)\n",
      "estaba en el estado (8, 4)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  4\n",
      "llegue a  (7, 2)\n",
      "estaba en el estado (7, 2)\n",
      "llegue a  Terminal Bomba\n"
     ]
    }
   ],
   "source": [
    "gw.follow_policy(init_state=(9,0), flag_q=False, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/SARSA e 0.1.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica despu茅s de 10,000 episodios y un 茅psilon de 0.1\n",
    "    <img src=\"images/SARSA e 0.05.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica despu茅s de 10,000 episodios y un 茅psilon de 0.05\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de control de Q-Learning\n",
    "\n",
    "El algoritmo Q-Learning off-policy se explica a continuaci贸n. Se genera una serie de episodios. Cada episodio que es una secuencia de $S,A,Ri$ para cada timestep, se realiza la estimaci贸n de  $Q_(s,a)$ teniendo en cuenta el estado actual en el timestep  $Q_(s,a)$ y el paso siguiente  $Q_(s',a')$ llevando la funci贸n hacia el TARGET  $R + \\gamma Q_(s',a')$. La diferencia con SARSA es que para Q-Learning la acci贸n $a,s$ se escoge de manera E-GREEDY con respecto al estimativo de  $Q_(s,a)$, mientras que la acci贸n $a',s'$ se escoge de manera GREEDY (m谩xima) con respecto al estimativo de  $Q_(s',a)$. Este peque帽o cambio de hacer GREEDY la escogencia de la acci贸n $a'$ para el siguiente estado genera que la pol铆tica mejorada sea DIFERENTE de la pol铆tica original, entonces es off-policy porque la pol铆tica mejorada es diferente en cada iteraci贸n.\n",
    "\n",
    "El error en Q-Learning es $[R + \\gamma max Q_(s',a)- Q_(s',a')]$. El algortimo SARSA es off-policy porque en cada iteraci贸n mejora una pol铆tica diferente que es GREEDY con respecto al valor de $ Q_(s',a')$ en cada iteraci贸n.\n",
    "\n",
    "Los par谩metros del algoritmo son el n煤mero de episodios, la tasa de descuento, la tasa de aprendizaje y el epsilon E-GREEDY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QL (epi,gamma,alpha,eps):\n",
    "    state_q_values = gw.init_qvalues2()\n",
    "    \n",
    "    for i in range(epi):\n",
    "        done=False\n",
    "        state = random.choice(gw.states)\n",
    "        if state_q_values[state]['a'] >= state_q_values[state]['r']:\n",
    "            ac=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "        else:\n",
    "            ac=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "        while True:\n",
    "            if done:\n",
    "                break\n",
    "            dado=random.randint(1, 6)\n",
    "            newst=state\n",
    "            if ac=='a':\n",
    "                subpolitica(a)\n",
    "                for j in range(dado):\n",
    "                    if newst==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    if newst in gw.goals or newst in gw.pits:\n",
    "                        if a[newst[0]][newst[1]]=='N':\n",
    "                            newst = (newst[0]-1, newst[1])\n",
    "                        if a[newst[0]][newst[1]]=='S':\n",
    "                            newst = (newst[0]+1, newst[1])\n",
    "                        if a[newst[0]][newst[1]]=='W':\n",
    "                            newst = (newst[0], newst[1]-1)\n",
    "                        if a[newst[0]][newst[1]]=='E':\n",
    "                            newst = (newst[0], newst[1]+1) \n",
    "                    else:\n",
    "                        action=gw.policy[newst]\n",
    "                        newst, reward, _, done = gw.step(newst, action, random=False)\n",
    "                    \n",
    "                if newst in gw.goals:\n",
    "                    reward=10\n",
    "                    done=True\n",
    "                if newst in gw.pits:\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                state2=newst\n",
    "                if (state_q_values [state2]['a'] >= state_q_values [state2]['r']):\n",
    "                        ac2=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "                        A='a'\n",
    "                else:\n",
    "                    ac2=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "                    A='r'\n",
    "                    \n",
    "                    \n",
    "\n",
    "                state_q_values[state][ac] = state_q_values[state][ac]+alpha*(reward+gamma*state_q_values[state2][A] - state_q_values[state][ac])\n",
    "                state = state2\n",
    "                ac=ac2\n",
    "                      \n",
    "                    \n",
    "                    \n",
    "            else: # action es retroceder\n",
    "                \n",
    "                subpolitica(r)\n",
    "                for j in range(dado):\n",
    "                    if newst==(9,0):\n",
    "                        subpolitica(a)\n",
    "                    if newst in gw.goals or newst in gw.pits:\n",
    "                        if r[newst[0]][newst[1]]=='N':\n",
    "                            newst = (newst[0]-1, newst[1])\n",
    "                        if r[newst[0]][newst[1]]=='S':\n",
    "                            newst = (newst[0]+1, newst[1])\n",
    "                        if r[newst[0]][newst[1]]=='W':\n",
    "                            newst = (newst[0], newst[1]-1)\n",
    "                        if r[newst[0]][newst[1]]=='E':\n",
    "                            newst = (newst[0], newst[1]+1) \n",
    "                    else:\n",
    "                        action=gw.policy[newst]\n",
    "                        newst, reward, _, done = gw.step(newst, action, random=False)\n",
    "\n",
    "                if newst in gw.goals:\n",
    "                    reward=10\n",
    "                    done=True\n",
    "                if newst in gw.pits:\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                state2=newst\n",
    "                if (state_q_values [state2]['a'] >= state_q_values [state2]['r']):\n",
    "                        ac2=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "                        A='a'\n",
    "                else:\n",
    "                    ac2=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "                    A='r'\n",
    "                \n",
    "                state_q_values[state][ac] = state_q_values[state][ac]+alpha*(reward+gamma*state_q_values[state2][A] - state_q_values[state][ac])\n",
    "                state = state2\n",
    "                ac=ac2\n",
    "    \n",
    "    pi =dict.fromkeys(gw.states,0)\n",
    "    for i in gw.states:\n",
    "        if (state_q_values [i]['a'] >= state_q_values [i]['r']):\n",
    "            pi[i]='a'\n",
    "        else:\n",
    "            pi[i]='r'\n",
    "    return state_q_values,pi\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,p3=QL(100000,0.9,0.1,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "traductorP(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  3\n",
      "llegue a  (9, 3)\n",
      "estaba en el estado (9, 3)\n",
      "saque en el dado:  6\n",
      "llegue a  (9, 9)\n",
      "estaba en el estado (9, 9)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 7)\n",
      "estaba en el estado (8, 7)\n",
      "saque en el dado:  5\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  3\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  2\n",
      "llegue a  (2, 0)\n",
      "estaba en el estado (2, 0)\n",
      "llegue a  Terminal Diamante\n"
     ]
    }
   ],
   "source": [
    "gw.follow_policy(init_state=(9,0), flag_q=False, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/QL 0.1.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica despu茅s de 10,000 episodios y un 茅psilon de 0.1\n",
    "    <img src=\"images/QL 0.05.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica despu茅s de 10,000 episodios y un 茅psilon de 0.05\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaci贸n de desempe帽o: Pasos promedio y tasa de victorias\n",
    "\n",
    "Finalmente se realiz贸 la evaluaci贸n de los m茅todos tanto MC como TD(0) realizando la generaci贸n de episodios con la funci贸n predeterminada sobre 1,000 partidas, para 10,0000 episodios de entrenamiento, con una tasa de descuento $纬=0.9$, epsilon E-GREEDY $系=0.05$ y una tasa de aprendizaje $伪=0.1$. Se generan las partidas y se promedian los pasos de los episodios as铆 como el porcentaje de partidas ganadas (ej: el episodio termina en el estado terminal meta/diamante). Se presentan los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluainador(partidas,gamma,epsilon,alpha,episodios):\n",
    "    q_mc,p_mc = MC_offpolicy(gamma,episodios)\n",
    "    q_sarsa,p_sarsa=SARSA(episodios,gamma,alpha,epsilon)\n",
    "    q_ql,p_ql=QL(episodios,gamma,alpha,epsilon)\n",
    "    \n",
    "    victorias_mc = 0\n",
    "    victorias_sarsa = 0\n",
    "    victorias_ql = 0\n",
    "\n",
    "    pasos_mc= []\n",
    "    pasos_sarsa=[]\n",
    "    pasos_ql=[]\n",
    "    for i in range(partidas):\n",
    "        traductorP(p_mc)\n",
    "        victoria_mc,pasos = gw.jugainador(init_state=(9,0), gamma=gamma)\n",
    "        victorias_mc +=victoria_mc\n",
    "        pasos_mc.append(pasos)\n",
    "        \n",
    "        traductorP(p_sarsa)\n",
    "        victoria_sarsa,pasos = gw.jugainador(init_state=(9,0), gamma=gamma)\n",
    "        victorias_sarsa +=victoria_sarsa\n",
    "        pasos_sarsa.append(pasos)\n",
    "        \n",
    "        traductorP(p_ql)\n",
    "        victoria_ql,pasos = gw.jugainador(init_state=(9,0), gamma=gamma)\n",
    "        victorias_ql +=victoria_ql\n",
    "        pasos_ql.append(pasos)\n",
    "        \n",
    "    porcentaje_win_mc = victorias_mc/partidas\n",
    "    porcentaje_win_sarsa = victorias_sarsa/partidas\n",
    "    porcentaje_win_ql = victorias_ql/partidas\n",
    "    \n",
    "    pasos_prom_mc =np.mean(pasos_mc)\n",
    "    pasos_prom_sarsa =np.mean(pasos_sarsa)\n",
    "    pasos_prom_ql =np.mean(pasos_ql)\n",
    "    \n",
    "    return porcentaje_win_mc,porcentaje_win_sarsa,porcentaje_win_ql, pasos_prom_mc, pasos_prom_sarsa, pasos_prom_ql\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_MC,V_sarsa,V_ql, pasos_mc, pasos_sarsa, pasos_ql=evaluainador(1000,0.9,0.05,0.1,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El porcentaje de victorias del algoritmo MC Off-Policy es: 38.6% y los pasos promedio que di贸 fueron: 23.088\n",
      "\n",
      "El porcentaje de victorias del algoritmo SARSA es: 31.2% y los pasos promedio que di贸 fueron: 12.259\n",
      " \n",
      "El porcentaje de victorias del algoritmo Q-Learning es: 28.599999999999998% y los pasos promedio que di贸 fueron: 12.082\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(f'El porcentaje de victorias del algoritmo MC Off-Policy es: {V_MC*100}% y los pasos promedio que di贸 fueron: {pasos_mc}\\n')\n",
    "print(f'El porcentaje de victorias del algoritmo SARSA es: {V_sarsa*100}% y los pasos promedio que di贸 fueron: {pasos_sarsa}\\n ')\n",
    "print(f'El porcentaje de victorias del algoritmo Q-Learning es: {V_ql*100}% y los pasos promedio que di贸 fueron: {pasos_ql}\\n ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pol铆ticas optimas encontradas por los algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp,mp2,mp3=[[]],[[]],[[]]\n",
    "for i in range(10):\n",
    "    for j in range (10):\n",
    "        mp[i].append(p[(i,j)])\n",
    "        mp2[i].append(p2[(i,j)])\n",
    "        mp3[i].append(p3[(i,j)])\n",
    "    mp.append([])\n",
    "    mp2.append([])\n",
    "    mp3.append([])  \n",
    "mp.pop(10)\n",
    "mp2.pop(10)\n",
    "mp3.pop(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica optima MC: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['a', 'a', 'r', 'a', 'a', 'r', 'a', 'a', 'a', 'a'],\n",
       " ['r', 'r', 'a', 'r', 'r', 'r', 'r', 'r', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'r', 'a', 'a'],\n",
       " ['a', 'a', 'r', 'r', 'r', 'r', 'a', 'a', 'r', 'a'],\n",
       " ['r', 'r', 'a', 'a', 'r', 'a', 'a', 'a', 'a', 'r'],\n",
       " ['a', 'a', 'a', 'r', 'a', 'r', 'r', 'r', 'a', 'r'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'r', 'a', 'a', 'r', 'r'],\n",
       " ['r', 'r', 'a', 'r', 'r', 'r', 'r', 'a', 'a', 'r'],\n",
       " ['a', 'r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'r'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'r', 'a', 'r']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Politica optima MC: \\n')\n",
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica optima SARSA: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'r', 'r'],\n",
       " ['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r'],\n",
       " ['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Politica optima SARSA: \\n')\n",
    "mp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica optima Q-Learning: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['a', 'r', 'r', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r'],\n",
       " ['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Politica optima Q-Learning: \\n')\n",
    "mp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "*   El algoritmo con el mejor desempe帽o en las mil partidas fue Montecarlo Off-Policy, esto se explica ya que el algoritmo espera al final del cap铆tulo para hacer la actualizaci贸n del par estado acci贸n, teniendo as铆 una mejor estimaci贸n que la que podr铆a lograr los algoritmos TD en el mismo n煤mero de episodios.\n",
    "*   El algoritmo que completaba los episodios de forma m谩s r谩pida fue Q-Learning, debido a que este toma como referencia el par estado acci贸n greedy, la funci贸n objetivo tender谩 ir de forma m谩s directa al estado terminal diamante y dada la naturaleza del juego y el sistema de recompensas, pasar谩 m谩s veces por terminales bomba terminando cayendo en estos de forma m谩s frecuente y acabando m谩s r谩pidamente los episodios. Corroborando que tambi茅n este sea el algoritmo con menor porcentaje de victorias.\n",
    "*   Cuando se aumentan los n煤meros de episodios al entrenar los algoritmos TD estos tienden a encontrar la misma pol铆tica optima, igualmente al disminuir el valor de 茅psilon del algoritmo SARSA.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
