{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escaleras y Serpientes: MC y TD(0) <img src=\"images/uniandes_logo.png\" width=\"250\"/>\n",
    "\n",
    "autores:\n",
    "\n",
    "Diego Fernando Osorio   201513417   - df.osorio11@uniandes.edu.co\n",
    "Emmanuel Gonzalez Gonzalez   201614679  - e.gonzalezg@uniandes.edu.co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este taller se trabajará sobre la modificación del juego de escaleras y serpientes del taller anterior. Se utilizan funciones del taller pasado para el desarrollo de los nuevos algoritmos. En este taller se realiza la implementación de los algoritmos de MonteCarlo Off-Policy (MC) y  métodos TD(0): SARSA y Q-Learning para encontrar políticas óptimas al estimar las funciones de valor.\n",
    "\n",
    "Los métodos de MC se basan en la generación de episodios y un importante componente de aletoriedad en las secuencias de estado, acción y recompensa. La regla de actualización para el algoritmo MC se basa en la estimación de la función de valor par estado-acción $Q_(s,a)$ donde el TARGET es el retorno del episodio. La política para un estado se define de manera GREEDY con respecto a $Q_(s,a)$.\n",
    "\n",
    "Por otro lado, los métodos TD se basan en la regla de actualización dada por la siguiente ecuacioón:\n",
    "\n",
    "$V(s) =V(s)+α[R_t+1 *γ Vs_t+1 - V(s) $, donde se tiene que el TARGET es R_t+1 *γ Vs_t+1 y el error es la diferencia del TARGET y $V(s) $ de la actualización actual.\n",
    "\n",
    "El control en TD(0) se realiza la implementación de 2 métodos similares: SARSA y Q-Learning. Ambos métodos realizan la generación de episodios, mientras que SARSA realiza la actualización teniendo en cuenta la función de valor par estado-acción actual $Q_(s,a)$ y la siguiente $Q_(s',a')$, en las que la escogencia de las acciones $a, a'$ se realiza E-GREEDY con respecto a la función $Q_(s,a)$, en Q-Learning la actualización se realiza con $Q_(s,a)$ escogiendo $a$ de manera E-GREEDY y $Q_(s',a')$ escogiendo $a'$ GREEDY (máxima) con respecto a la función de valor $Q_(s,a)$. Entonces SARSA con epsilon $ϵ=0$ en donde siempre escoge la acción en el paso siguiente GREEDY es SARSA.\n",
    "\n",
    "La diferencia es que MC realiza la generación de los episodios y espera a que todos los episodios, es decir la secuencia $S,A,Ri$ hayan sido generados, y luego recorre cada uno de los episodios generados para estimar $Q_(s,a)$ de manera off-policy (con dos políticas $π, 𝜇$, donde $π$ es polítca TARGET que se modifica GREEDY con respecto a la función $Q_(s,a)$ y $𝜇$ se utiliza para generar los episodios ) para luego así estimar $Q_(s,a)$ mediante los pesos de corrección $W/C(s,a)$ de la política TARGET y de comportamiento. \n",
    "\n",
    "Por otro lado SARSA y Q-Learning a medida que generan los episodios actualizan la función $Q_(s,a)$ con la función que se tiene en el estado actual $Q_(s,a)$ y el paso siguiente $Q_(s',a')$, para todos los episodios. Es decir en TD(0) a medida que avanza sobre el episodio, mirando un paso adelante, realiza la actualización de la función de valor par estado-acción $Q_(s,a)$ y NO espera al final del episodio. Del mismo modo se generaron algortimos que permitan generar episodios aleatorios dada una política arbitraria, permitiendo así dada una acción en un estado, retornar una recompensa y el estado resultante de esta acción, hasta llegar al final del episodio para el MDP de Escaleras y Serpientes.\n",
    "\n",
    "Se realizó la evaluación, comparación y contraste de los 3 algoritmos para encontrar políticas óptimas y estimar la función $Q_(s,a)$ óptima. Para esto se tuvo en cuenta los siguiente hiperparámetros de algoritmos: tasa de aprendizaje $α=0.1$, epsilon (E-GREEDY) $ϵ=0.05$, y tasa de descuento $γ=0.9$.\n",
    "\n",
    "Se observa que número de pasos promediados para 10,000 episodios generados se tienen un número de pasos promedio de 52.037, 12.779 y 12.730 para los algoritmos MC Off-policy, SARSA y Q-Learning respectivamente. Por otro lado el porcentaje de victorias en promedio fue de 4.9%, 3.31% y 3.17%, respectivamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/escalerita.png\" alt=\"centered image\" width=300/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones importantes:**\n",
    "\n",
    "* La meta del jugador es ganar la partida llegando a una de las casillas marcadas en azul.\n",
    "* El jugador pierde la partida si cae en una de las casillas marcadas en rojo.\n",
    "* En cada jugada, antes de lanzar el dado, el jugador decide si quiere avanzar o retroceder el número de casillas indicadas por el dado.\n",
    "* En las casillas 1 y 100 la ficha rebota (si se supera el extremo, se avanza en la otra dirección la cantidad restante)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones utiles del taller anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Librerias que contienen la dinámica del entorno gridworld\n",
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit\n",
    "gw = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subpolitica (x):\n",
    "    # iterar sobre estados\n",
    "    for i in gw.states:\n",
    "        # asignar politica en estado i como x[i[0],i[1]] \n",
    "        gw.policy[i]=x[i[0]][i[1]]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inipoli():\n",
    "    state = gw.states\n",
    "\n",
    "    # definir politica para avanzar\n",
    "    # iterar sobre estados\n",
    "    for i, s in enumerate(state):\n",
    "        if s[0]%2==0 and s[1]!=0 : # si fila es par y columna no es 0, acción W\n",
    "            gw.policy[s] = 'W'\n",
    "        elif s[0]%2==0 and s[1]==0 : # si fila es par y columna es 0, acción N\n",
    "            gw.policy[s] = 'N'\n",
    "        elif s[0]%2==1 and s[1]!=9 : # si fila es par y columna no es 9, acción E  \n",
    "            gw.policy[s] = 'E'\n",
    "        elif s[0]%2==1 and s[1]==9 : # si fila es par y columna es 9, acción N\n",
    "            gw.policy[s] = 'N'\n",
    "    a=[[]]\n",
    "    r=[[]]\n",
    "\n",
    "    # iterar sobre filas\n",
    "    for i in range (10):\n",
    "        # iterar sobre columnas\n",
    "        for j in range(10):\n",
    "            a[i].append('')\n",
    "            r[i].append('')\n",
    "        a.append([])\n",
    "        r.append([])\n",
    "\n",
    "    for i in gw.states:\n",
    "        a[i[0]][i[1]]=gw.policy[i]\n",
    "\n",
    "    for i, s in enumerate(state):\n",
    "        if s[0]%2==0 and s[1]!=9 :\n",
    "            gw.policy[s] = 'E'\n",
    "        elif s[0]%2==0 and s[1]==9 :\n",
    "            gw.policy[s] = 'S'\n",
    "        elif s[0]%2==1 and s[1]!=0 :\n",
    "            gw.policy[s] = 'W'\n",
    "        elif s[0]%2==1 and s[1]==0 :\n",
    "            gw.policy[s] = 'S'\n",
    "    for i in gw.states:\n",
    "        r[i[0]][i[1]]=gw.policy[i]\n",
    "    subpolitica(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp=[(0,2),(0,5),(0,8),(1,2),(2,7),(3,3),(3,8),(4,1),(4,5),(4,8),(5,3),(5,5),(5,7)]\n",
    "serpf=[(7,7),(7,3),(4,9),(8,1),(9,0),(7,3),(6,7),(8,3),(9,6),(8,9),(7,1),(9,4),(9,8)]\n",
    "esca=[(9,7),(7,0),(5,9),(5,2),(4,6),(3,5),(3,1),(2,0)]\n",
    "escaf=[(7,5),(1,1),(0,9),(2,3),(0,7),(1,6),(0,4),(0,0)]\n",
    "gw = GridWorld(rows=10, cols=10, walls=[], pits=[(7,2),(6,3),(5,4),(3,6),(1,8)], goals=[(0,0),(2,0)], live_reward=-0.1,esc=esca, escf=escaf, serp=serp,serpf=serpf)\n",
    "inipoli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = gw.states\n",
    "\n",
    "# definir politica para avanzar\n",
    "# iterar sobre estados\n",
    "for i, s in enumerate(state):\n",
    "    if s[0]%2==0 and s[1]!=0 : # si fila es par y columna no es 0, acción W\n",
    "        gw.policy[s] = 'W'\n",
    "    elif s[0]%2==0 and s[1]==0 : # si fila es par y columna es 0, acción N\n",
    "        gw.policy[s] = 'N'\n",
    "    elif s[0]%2==1 and s[1]!=9 : # si fila es par y columna no es 9, acción E  \n",
    "        gw.policy[s] = 'E'\n",
    "    elif s[0]%2==1 and s[1]==9 : # si fila es par y columna es 9, acción N\n",
    "        gw.policy[s] = 'N'\n",
    "a=[[]]\n",
    "r=[[]]\n",
    "\n",
    "# iterar sobre filas\n",
    "for i in range (10):\n",
    "    # iterar sobre columnas\n",
    "    for j in range(10):\n",
    "        a[i].append('')\n",
    "        r[i].append('')\n",
    "    a.append([])\n",
    "    r.append([])\n",
    "\n",
    "for i in gw.states:\n",
    "    a[i[0]][i[1]]=gw.policy[i]\n",
    "\n",
    "for i, s in enumerate(state):\n",
    "    if s[0]%2==0 and s[1]!=9 :\n",
    "        gw.policy[s] = 'E'\n",
    "    elif s[0]%2==0 and s[1]==9 :\n",
    "        gw.policy[s] = 'S'\n",
    "    elif s[0]%2==1 and s[1]!=0 :\n",
    "        gw.policy[s] = 'W'\n",
    "    elif s[0]%2==1 and s[1]==0 :\n",
    "        gw.policy[s] = 'S'\n",
    "for i in gw.states:\n",
    "    r[i[0]][i[1]]=gw.policy[i]\n",
    "subpolitica(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[9][0]='E'\n",
    "a[0][0]='E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traductorP(p):\n",
    "    for i in gw.states:\n",
    "        if p[i]=='a':\n",
    "            gw.policy[i]=a[i[0]][i[1]]\n",
    "        else:\n",
    "            gw.policy[i]=r[i[0]][i[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de control de Montecarlo off-policy\n",
    "\n",
    "El algoritmo MC off-policy se explica a continuación. Se genera una serie de episodios. Cada episodio que es una secuencia de $S,A,Ri$ para cada timestep, en seguida para cada episodio se recorre desde el estado terminal $S_T$ hacia el principio del episodio y se aplica la regla de actualización para estimar  $Q_(s,a)$ llevando la función hacia el retorno $G=R_t+1 +γG$, teniendo en cuenta el peso $W/C(s,a)$ que inicia en $W=1$ al inicio del epidosio y se va modificando teniendo en cuenta la probabilidad de ocurrencia de la política target $\\pi$ en relación a la política de comportamiento $𝜇$ que genera los episodios. $W$ es 0 cuando la acción máxima dada por la política de cubrimiento es diferente a la acción dada por la política target.\n",
    "\n",
    "El error en MC off-policy es $G-Q_(s,a)$.\n",
    "\n",
    "Los parámetros del algoritmo son el número de episodios y la tasa de descuento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_offpolicy (gamma,epi):\n",
    "    \n",
    "    state_q_values = gw.init_qvalues2()\n",
    "    new_state_q_values =  gw.init_qvalues()    \n",
    "    pi =dict.fromkeys(gw.states,'a')\n",
    "    c =  gw.init_qvalues2()\n",
    "    for i in range(epi):\n",
    "        r,ac,est,f=gw.generainador(init_state=(-1,-1), gamma=1.0)\n",
    "        aja=True\n",
    "        while aja:\n",
    "            if len(est)>2:\n",
    "                for j in range(len(est)-1):\n",
    "                    if est[j]==est[j+1]:\n",
    "                        est.pop(j)\n",
    "                        r.pop(j)\n",
    "                        ac.pop(j)\n",
    "                        break\n",
    "                    if j==len(est)-2:\n",
    "                        aja=False \n",
    "            else:\n",
    "                aja=False\n",
    "        g = 0\n",
    "        w=1\n",
    "        for j in range(len(est)-2,-1,-1): # iterar sobre estados de episodio\n",
    "            g = gamma*g+r[j+1]\n",
    "            c[est[j]][ac[j]] =  c[est[j]][ac[j]]+w\n",
    "            \n",
    "            state_q_values [est[j]][ac[j]] = state_q_values[est[j]][ac[j]] + (w/(c[est[j]][ac[j]]))*(g-state_q_values[est[j]][ac[j]]) \n",
    "            \n",
    "            if (state_q_values [est[j]]['a'] > state_q_values [est[j]]['r']):\n",
    "                pi[est[j]]='a'\n",
    "            else:\n",
    "                pi[est[j]]='r'\n",
    "          \n",
    "            if ac[j]!= pi[est[j]]:\n",
    "                break\n",
    "                \n",
    "            w *=2 \n",
    "            \n",
    "    return state_q_values, pi\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,p= MC_offpolicy (0.9,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "traductorP(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  1\n",
      "llegue a  (9, 1)\n",
      "estaba en el estado (9, 1)\n",
      "saque en el dado:  4\n",
      "llegue a  (9, 5)\n",
      "estaba en el estado (9, 5)\n",
      "saque en el dado:  4\n",
      "llegue a  (9, 9)\n",
      "estaba en el estado (9, 9)\n",
      "saque en el dado:  2\n",
      "llegue a  (9, 7)\n",
      "estaba en el estado (9, 7)\n",
      "saque en el dado:  6\n",
      "llegue a  (8, 0)\n",
      "estaba en el estado (8, 0)\n",
      "saque en el dado:  1\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  4\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  1\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  1\n",
      "llegue a  (2, 1)\n",
      "estaba en el estado (2, 1)\n",
      "saque en el dado:  4\n",
      "llegue a  (1, 3)\n",
      "estaba en el estado (1, 3)\n",
      "saque en el dado:  5\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  6\n",
      "llegue a  (1, 4)\n",
      "estaba en el estado (1, 4)\n",
      "saque en el dado:  1\n",
      "llegue a  (1, 3)\n",
      "estaba en el estado (1, 3)\n",
      "saque en el dado:  6\n",
      "llegue a  (2, 3)\n",
      "estaba en el estado (2, 3)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 0)\n",
      "estaba en el estado (2, 0)\n",
      "llegue a  Terminal Diamante\n"
     ]
    }
   ],
   "source": [
    "gw.follow_policy(init_state=(9,0), flag_q=False, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/MC 10k.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica después de 10,000 episodios\n",
    "    <img src=\"images/MC 100k.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica después de 100,000 episodios\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de control de SARSA\n",
    "\n",
    "El algoritmo SARSA on-policy se explica a continuación. Se genera una serie de episodios. Cada episodio que es una secuencia de $S,A,Ri$ para cada timestep, se realiza la estimación de  $Q_(s,a)$ teniendo en cuenta el estado actual en el timestep  $Q_(s,a)$ y el paso siguiente  $Q_(s',a')$ llevando la función hacia el TARGET  $R + \\gamma Q_(s',a')$. Las acciones tanto $a, a'$ se escogen de manera E-GREEDY con respecto al estimativo de  $Q_(s,a)$ y  $Q_(s',a)$ respectivamente.\n",
    "El error en SARS es $[R + \\gamma Q_(s',a')- Q_(s',a')]$. El algortimo SARSA es on-policy porque siempre se está mejorando la misma política $π$ en función del estimado de $Q_(s,a)$\n",
    "\n",
    "Los parámetros del algoritmo son el número de episodios, la tasa de descuento, la tasa de aprendizaje y el epsilon E-GREEDY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA (epi,gamma,alpha,eps):\n",
    "    state_q_values = gw.init_qvalues2()\n",
    "    \n",
    "    for i in range(epi):\n",
    "        done=False\n",
    "        state = random.choice(gw.states)\n",
    "        if state_q_values[state]['a'] >= state_q_values[state]['r']:\n",
    "            ac=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "        else:\n",
    "            ac=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "        while True:\n",
    "            if done:\n",
    "                break\n",
    "            dado=random.randint(1, 6)\n",
    "            newst=state\n",
    "            if ac=='a':\n",
    "                subpolitica(a)\n",
    "                for j in range(dado):\n",
    "                    if newst==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    if newst in gw.goals or newst in gw.pits:\n",
    "                        if a[newst[0]][newst[1]]=='N':\n",
    "                            newst = (newst[0]-1, newst[1])\n",
    "                        if a[newst[0]][newst[1]]=='S':\n",
    "                            newst = (newst[0]+1, newst[1])\n",
    "                        if a[newst[0]][newst[1]]=='W':\n",
    "                            newst = (newst[0], newst[1]-1)\n",
    "                        if a[newst[0]][newst[1]]=='E':\n",
    "                            newst = (newst[0], newst[1]+1) \n",
    "                    else:\n",
    "                        action=gw.policy[newst]\n",
    "                        newst, reward, _, done = gw.step(newst, action, random=False)\n",
    "                    \n",
    "                if newst in gw.goals:\n",
    "                    reward=10\n",
    "                    done=True\n",
    "                if newst in gw.pits:\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                state2=newst\n",
    "                if (state_q_values [state2]['a'] >= state_q_values [state2]['r']):\n",
    "                        ac2=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "                else:\n",
    "                    ac2=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "\n",
    "                state_q_values[state][ac] = state_q_values[state][ac]+alpha*(reward+gamma*state_q_values[state2][ac2] - state_q_values[state][ac])\n",
    "                state = state2\n",
    "                ac=ac2\n",
    "                      \n",
    "                    \n",
    "                    \n",
    "            else: # action es retroceder\n",
    "                \n",
    "                subpolitica(r)\n",
    "                for j in range(dado):\n",
    "                    if newst==(9,0):\n",
    "                        subpolitica(a)\n",
    "                    if newst in gw.goals or newst in gw.pits:\n",
    "                        if r[newst[0]][newst[1]]=='N':\n",
    "                            newst = (newst[0]-1, newst[1])\n",
    "                        if r[newst[0]][newst[1]]=='S':\n",
    "                            newst = (newst[0]+1, newst[1])\n",
    "                        if r[newst[0]][newst[1]]=='W':\n",
    "                            newst = (newst[0], newst[1]-1)\n",
    "                        if r[newst[0]][newst[1]]=='E':\n",
    "                            newst = (newst[0], newst[1]+1) \n",
    "                    else:\n",
    "                        action=gw.policy[newst]\n",
    "                        newst, reward, _, done = gw.step(newst, action, random=False)\n",
    "\n",
    "                if newst in gw.goals:\n",
    "                    reward=10\n",
    "                    done=True\n",
    "                if newst in gw.pits:\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                state2=newst\n",
    "                if (state_q_values [state2]['a'] >= state_q_values [state2]['r']):\n",
    "                        ac2=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "                else:\n",
    "                    ac2=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "                \n",
    "                state_q_values[state][ac] = state_q_values[state][ac]+alpha*(reward+gamma*state_q_values[state2][ac2] - state_q_values[state][ac])\n",
    "                state = state2\n",
    "                ac=ac2\n",
    "    \n",
    "    pi =dict.fromkeys(gw.states,0)\n",
    "    for i in gw.states:\n",
    "        if (state_q_values [i]['a'] >= state_q_values [i]['r']):\n",
    "            pi[i]='a'\n",
    "        else:\n",
    "            pi[i]='r'\n",
    "    return state_q_values,pi\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,p2=SARSA(100000,0.9,0.1,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "traductorP(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  2\n",
      "llegue a  (9, 2)\n",
      "estaba en el estado (9, 2)\n",
      "saque en el dado:  6\n",
      "llegue a  (9, 8)\n",
      "estaba en el estado (9, 8)\n",
      "saque en el dado:  6\n",
      "llegue a  (8, 4)\n",
      "estaba en el estado (8, 4)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  4\n",
      "llegue a  (7, 2)\n",
      "estaba en el estado (7, 2)\n",
      "llegue a  Terminal Bomba\n"
     ]
    }
   ],
   "source": [
    "gw.follow_policy(init_state=(9,0), flag_q=False, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/SARSA e 0.1.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica después de 10,000 episodios y un épsilon de 0.1\n",
    "    <img src=\"images/SARSA e 0.05.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica después de 10,000 episodios y un épsilon de 0.05\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de control de Q-Learning\n",
    "\n",
    "El algoritmo Q-Learning off-policy se explica a continuación. Se genera una serie de episodios. Cada episodio que es una secuencia de $S,A,Ri$ para cada timestep, se realiza la estimación de  $Q_(s,a)$ teniendo en cuenta el estado actual en el timestep  $Q_(s,a)$ y el paso siguiente  $Q_(s',a')$ llevando la función hacia el TARGET  $R + \\gamma Q_(s',a')$. La diferencia con SARSA es que para Q-Learning la acción $a,s$ se escoge de manera E-GREEDY con respecto al estimativo de  $Q_(s,a)$, mientras que la acción $a',s'$ se escoge de manera GREEDY (máxima) con respecto al estimativo de  $Q_(s',a)$. Este pequeño cambio de hacer GREEDY la escogencia de la acción $a'$ para el siguiente estado genera que la política mejorada sea DIFERENTE de la política original, entonces es off-policy porque la política mejorada es diferente en cada iteración.\n",
    "\n",
    "El error en Q-Learning es $[R + \\gamma max Q_(s',a)- Q_(s',a')]$. El algortimo SARSA es off-policy porque en cada iteración mejora una política diferente que es GREEDY con respecto al valor de $ Q_(s',a')$ en cada iteración.\n",
    "\n",
    "Los parámetros del algoritmo son el número de episodios, la tasa de descuento, la tasa de aprendizaje y el epsilon E-GREEDY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QL (epi,gamma,alpha,eps):\n",
    "    state_q_values = gw.init_qvalues2()\n",
    "    \n",
    "    for i in range(epi):\n",
    "        done=False\n",
    "        state = random.choice(gw.states)\n",
    "        if state_q_values[state]['a'] >= state_q_values[state]['r']:\n",
    "            ac=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "        else:\n",
    "            ac=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "        while True:\n",
    "            if done:\n",
    "                break\n",
    "            dado=random.randint(1, 6)\n",
    "            newst=state\n",
    "            if ac=='a':\n",
    "                subpolitica(a)\n",
    "                for j in range(dado):\n",
    "                    if newst==(0,0):\n",
    "                        subpolitica(r)\n",
    "                    if newst in gw.goals or newst in gw.pits:\n",
    "                        if a[newst[0]][newst[1]]=='N':\n",
    "                            newst = (newst[0]-1, newst[1])\n",
    "                        if a[newst[0]][newst[1]]=='S':\n",
    "                            newst = (newst[0]+1, newst[1])\n",
    "                        if a[newst[0]][newst[1]]=='W':\n",
    "                            newst = (newst[0], newst[1]-1)\n",
    "                        if a[newst[0]][newst[1]]=='E':\n",
    "                            newst = (newst[0], newst[1]+1) \n",
    "                    else:\n",
    "                        action=gw.policy[newst]\n",
    "                        newst, reward, _, done = gw.step(newst, action, random=False)\n",
    "                    \n",
    "                if newst in gw.goals:\n",
    "                    reward=10\n",
    "                    done=True\n",
    "                if newst in gw.pits:\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                state2=newst\n",
    "                if (state_q_values [state2]['a'] >= state_q_values [state2]['r']):\n",
    "                        ac2=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "                        A='a'\n",
    "                else:\n",
    "                    ac2=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "                    A='r'\n",
    "                    \n",
    "                    \n",
    "\n",
    "                state_q_values[state][ac] = state_q_values[state][ac]+alpha*(reward+gamma*state_q_values[state2][A] - state_q_values[state][ac])\n",
    "                state = state2\n",
    "                ac=ac2\n",
    "                      \n",
    "                    \n",
    "                    \n",
    "            else: # action es retroceder\n",
    "                \n",
    "                subpolitica(r)\n",
    "                for j in range(dado):\n",
    "                    if newst==(9,0):\n",
    "                        subpolitica(a)\n",
    "                    if newst in gw.goals or newst in gw.pits:\n",
    "                        if r[newst[0]][newst[1]]=='N':\n",
    "                            newst = (newst[0]-1, newst[1])\n",
    "                        if r[newst[0]][newst[1]]=='S':\n",
    "                            newst = (newst[0]+1, newst[1])\n",
    "                        if r[newst[0]][newst[1]]=='W':\n",
    "                            newst = (newst[0], newst[1]-1)\n",
    "                        if r[newst[0]][newst[1]]=='E':\n",
    "                            newst = (newst[0], newst[1]+1) \n",
    "                    else:\n",
    "                        action=gw.policy[newst]\n",
    "                        newst, reward, _, done = gw.step(newst, action, random=False)\n",
    "\n",
    "                if newst in gw.goals:\n",
    "                    reward=10\n",
    "                    done=True\n",
    "                if newst in gw.pits:\n",
    "                    reward=-1\n",
    "                    done=True\n",
    "                state2=newst\n",
    "                if (state_q_values [state2]['a'] >= state_q_values [state2]['r']):\n",
    "                        ac2=np.random.choice(['a','r'],p=[1-eps,eps])\n",
    "                        A='a'\n",
    "                else:\n",
    "                    ac2=np.random.choice(['r','a'],p=[1-eps,eps])\n",
    "                    A='r'\n",
    "                \n",
    "                state_q_values[state][ac] = state_q_values[state][ac]+alpha*(reward+gamma*state_q_values[state2][A] - state_q_values[state][ac])\n",
    "                state = state2\n",
    "                ac=ac2\n",
    "    \n",
    "    pi =dict.fromkeys(gw.states,0)\n",
    "    for i in gw.states:\n",
    "        if (state_q_values [i]['a'] >= state_q_values [i]['r']):\n",
    "            pi[i]='a'\n",
    "        else:\n",
    "            pi[i]='r'\n",
    "    return state_q_values,pi\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,p3=QL(100000,0.9,0.1,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "traductorP(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estaba en el estado (9, 0)\n",
      "saque en el dado:  3\n",
      "llegue a  (9, 3)\n",
      "estaba en el estado (9, 3)\n",
      "saque en el dado:  6\n",
      "llegue a  (9, 9)\n",
      "estaba en el estado (9, 9)\n",
      "saque en el dado:  2\n",
      "llegue a  (8, 7)\n",
      "estaba en el estado (8, 7)\n",
      "saque en el dado:  5\n",
      "llegue a  (8, 2)\n",
      "estaba en el estado (8, 2)\n",
      "saque en el dado:  3\n",
      "llegue a  (7, 0)\n",
      "estaba en el estado (7, 0)\n",
      "saque en el dado:  3\n",
      "llegue a  (2, 2)\n",
      "estaba en el estado (2, 2)\n",
      "saque en el dado:  2\n",
      "llegue a  (2, 0)\n",
      "estaba en el estado (2, 0)\n",
      "llegue a  Terminal Diamante\n"
     ]
    }
   ],
   "source": [
    "gw.follow_policy(init_state=(9,0), flag_q=False, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/QL 0.1.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica después de 10,000 episodios y un épsilon de 0.1\n",
    "    <img src=\"images/QL 0.05.png\" alt=\"centered image\" width=400/>\n",
    "    Gridworld de la politica después de 10,000 episodios y un épsilon de 0.05\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de desempeño: Pasos promedio y tasa de victorias\n",
    "\n",
    "Finalmente se realizó la evaluación de los métodos tanto MC como TD(0) realizando la generación de episodios con la función predeterminada sobre 1,000 partidas, para 10,0000 episodios de entrenamiento, con una tasa de descuento $γ=0.9$, epsilon E-GREEDY $ϵ=0.05$ y una tasa de aprendizaje $α=0.1$. Se generan las partidas y se promedian los pasos de los episodios así como el porcentaje de partidas ganadas (ej: el episodio termina en el estado terminal meta/diamante). Se presentan los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluainador(partidas,gamma,epsilon,alpha,episodios):\n",
    "    q_mc,p_mc = MC_offpolicy(gamma,episodios)\n",
    "    q_sarsa,p_sarsa=SARSA(episodios,gamma,alpha,epsilon)\n",
    "    q_ql,p_ql=QL(episodios,gamma,alpha,epsilon)\n",
    "    \n",
    "    victorias_mc = 0\n",
    "    victorias_sarsa = 0\n",
    "    victorias_ql = 0\n",
    "\n",
    "    pasos_mc= []\n",
    "    pasos_sarsa=[]\n",
    "    pasos_ql=[]\n",
    "    for i in range(partidas):\n",
    "        traductorP(p_mc)\n",
    "        victoria_mc,pasos = gw.jugainador(init_state=(9,0), gamma=gamma)\n",
    "        victorias_mc +=victoria_mc\n",
    "        pasos_mc.append(pasos)\n",
    "        \n",
    "        traductorP(p_sarsa)\n",
    "        victoria_sarsa,pasos = gw.jugainador(init_state=(9,0), gamma=gamma)\n",
    "        victorias_sarsa +=victoria_sarsa\n",
    "        pasos_sarsa.append(pasos)\n",
    "        \n",
    "        traductorP(p_ql)\n",
    "        victoria_ql,pasos = gw.jugainador(init_state=(9,0), gamma=gamma)\n",
    "        victorias_ql +=victoria_ql\n",
    "        pasos_ql.append(pasos)\n",
    "        \n",
    "    porcentaje_win_mc = victorias_mc/partidas\n",
    "    porcentaje_win_sarsa = victorias_sarsa/partidas\n",
    "    porcentaje_win_ql = victorias_ql/partidas\n",
    "    \n",
    "    pasos_prom_mc =np.mean(pasos_mc)\n",
    "    pasos_prom_sarsa =np.mean(pasos_sarsa)\n",
    "    pasos_prom_ql =np.mean(pasos_ql)\n",
    "    \n",
    "    return porcentaje_win_mc,porcentaje_win_sarsa,porcentaje_win_ql, pasos_prom_mc, pasos_prom_sarsa, pasos_prom_ql\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_MC,V_sarsa,V_ql, pasos_mc, pasos_sarsa, pasos_ql=evaluainador(1000,0.9,0.05,0.1,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El porcentaje de victorias del algoritmo MC Off-Policy es: 38.6% y los pasos promedio que dió fueron: 23.088\n",
      "\n",
      "El porcentaje de victorias del algoritmo SARSA es: 31.2% y los pasos promedio que dió fueron: 12.259\n",
      " \n",
      "El porcentaje de victorias del algoritmo Q-Learning es: 28.599999999999998% y los pasos promedio que dió fueron: 12.082\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(f'El porcentaje de victorias del algoritmo MC Off-Policy es: {V_MC*100}% y los pasos promedio que dió fueron: {pasos_mc}\\n')\n",
    "print(f'El porcentaje de victorias del algoritmo SARSA es: {V_sarsa*100}% y los pasos promedio que dió fueron: {pasos_sarsa}\\n ')\n",
    "print(f'El porcentaje de victorias del algoritmo Q-Learning es: {V_ql*100}% y los pasos promedio que dió fueron: {pasos_ql}\\n ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Políticas optimas encontradas por los algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp,mp2,mp3=[[]],[[]],[[]]\n",
    "for i in range(10):\n",
    "    for j in range (10):\n",
    "        mp[i].append(p[(i,j)])\n",
    "        mp2[i].append(p2[(i,j)])\n",
    "        mp3[i].append(p3[(i,j)])\n",
    "    mp.append([])\n",
    "    mp2.append([])\n",
    "    mp3.append([])  \n",
    "mp.pop(10)\n",
    "mp2.pop(10)\n",
    "mp3.pop(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica optima MC: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['a', 'a', 'r', 'a', 'a', 'r', 'a', 'a', 'a', 'a'],\n",
       " ['r', 'r', 'a', 'r', 'r', 'r', 'r', 'r', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'r', 'a', 'a'],\n",
       " ['a', 'a', 'r', 'r', 'r', 'r', 'a', 'a', 'r', 'a'],\n",
       " ['r', 'r', 'a', 'a', 'r', 'a', 'a', 'a', 'a', 'r'],\n",
       " ['a', 'a', 'a', 'r', 'a', 'r', 'r', 'r', 'a', 'r'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'r', 'a', 'a', 'r', 'r'],\n",
       " ['r', 'r', 'a', 'r', 'r', 'r', 'r', 'a', 'a', 'r'],\n",
       " ['a', 'r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'r'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'r', 'a', 'r']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Politica optima MC: \\n')\n",
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica optima SARSA: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'r', 'r'],\n",
       " ['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r'],\n",
       " ['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Politica optima SARSA: \\n')\n",
    "mp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica optima Q-Learning: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['a', 'r', 'r', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r'],\n",
       " ['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'],\n",
       " ['r', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Politica optima Q-Learning: \\n')\n",
    "mp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "*   El algoritmo con el mejor desempeño en las mil partidas fue Montecarlo Off-Policy, esto se explica ya que el algoritmo espera al final del capítulo para hacer la actualización del par estado acción, teniendo así una mejor estimación que la que podría lograr los algoritmos TD en el mismo número de episodios.\n",
    "*   El algoritmo que completaba los episodios de forma más rápida fue Q-Learning, debido a que este toma como referencia el par estado acción greedy, la función objetivo tenderá ir de forma más directa al estado terminal diamante y dada la naturaleza del juego y el sistema de recompensas, pasará más veces por terminales bomba terminando cayendo en estos de forma más frecuente y acabando más rápidamente los episodios. Corroborando que también este sea el algoritmo con menor porcentaje de victorias.\n",
    "*   Cuando se aumentan los números de episodios al entrenar los algoritmos TD estos tienden a encontrar la misma política optima, igualmente al disminuir el valor de épsilon del algoritmo SARSA.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
